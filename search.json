[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the author",
    "section": "",
    "text": "Hi, I’m Haowei.\nI’m a learning experience designer passionate about creating meaningful, engaging educational experiences through thoughtfully designed technologies and instructional materials.\nWith an interdisciplinary background spanning education, psychology, interaction design, and marketing, I explore innovative ways to transform how we learn and grow.\nMy current work focuses on exploring large language models (LLMs) and leveraging their potential to spark curiosity, foster deeper understanding, and support lifelong learning."
  },
  {
    "objectID": "posts/W4P2_Creative Writing/Creative_Writing.html",
    "href": "posts/W4P2_Creative Writing/Creative_Writing.html",
    "title": "LLMs and Creative Writing",
    "section": "",
    "text": "Creative Writing"
  },
  {
    "objectID": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "href": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "title": "Learning Assessment Using LLMs",
    "section": "",
    "text": "Many people use LLM tools such as Chatgpt to help them finish school work or complete a task. Since these tasks are usually temporary and aimed to be finished in the short term, people rarely pay attention to the process of learning, and LLMs become controversial in education as being a shortcut of answers rather than learning aid.\nAccording to cognitive psychology, meaningful learning can be defined with retention and transfer, which means learners can memorize or understand the content in depth and apply it to new scenarios. Learning involves many stages, from preview to processing and understanding the knowledge and to review. To understand how LLMs may help with meaningful learning, I want to start with LLM’s ability to assess a learners’ current level of knowledge in a certain field. In classes or an online course, this is often done by the teacher throwing out some pop quiz questions or seeing a show of hands to get a general idea of where students are in a field of study.\nI wonder if LLMs can generate suitable quiz questions that accurately assess a students’ level.\n\nillustration generated by Chatgpt(gpt5)\nFor a quick start, I asked Chatgpt if it can generate a quiz for me to assess my knowledge of AI. Of course, I can add a lot more context to help it understand where I am, but the point of using Chatgpt for this task is to quickly help a student understand their current level of knowledge, so I used a short prompt.\nI used the following prompt:\n\nI want to understand where I am in my knowledge with AI. Can you generate quiz questions to help me assess my current level?\n\nThe first two versions it sent back were either too long or contained a lot of short answer questions, which was quite overwhelming for me as a learner, so I changed my prompt to:\n\nI want to understand where I am in my knowledge with AI. Can you generate short multiple choice quiz questions to help me assess my current level?\n\nThese are the eight questions it generated:\n\n\n\nquestions generated by gpt on assessing my knowledge in AI\n\n\nThe first eight questions were basic and I got all of them right, so Chatgpt offered me a harder quiz. This time the questions were a lot more detailed and advanced, so I only got 5 out of 8 right.\nI then asked Chatgpt to make conclusions on my current level and offer recommendations for me to improve my knowledge. Here is my prompt: “Help me interpret the quiz result. What do I understand, what tasks can I complete, and how may I improve?” It sent back a list of items I understood well, tasks I can likely complete, and where I struggled. What brought my attention was its confidence in my technical skills with me just correctly answering some conceptual questions.\nSpecifically, it said:\n\n“If someone gave you a dataset + scikit-learn or TensorFlow starter code, you could likely: * Train a simple classifier (logistic regression, decision tree). * Interpret probability outputs correctly (once clarified). * Run a CNN or transformer with prebuilt frameworks.”\n\nI don’t think I can train a classifier or run a transformer with prebuilt frameworks if someone gives me a dataset and startercode. There seems to be some overestimation of my skillsets. This can easily deceive a learner and make them over confident about themselves. I think this inaccuracy may be because of 1) me guessing some of the hard questions right and gave out fake feedbacks to gpt and 2) Chatgpt’s miss on the validity of quizzes and scope of conclusion that can be drawn from quizzes.\nNext time anyone says “teachers will be replaced by AI”, tell them that it can’t, even from the first step."
  },
  {
    "objectID": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "href": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "title": "LLMs and Use of Metaphor",
    "section": "",
    "text": "LLMs and Use of Metaphor\nIn this week’s class we had a first glance at how LLMs are good at language and communication with humans in different scenarios and manners, but most of the examples we saw involved casual daily language. For example, when we talked about the word “bank” in class, there were many meanings associated with “bank” such as a financial institution, plane banking, or basketball bank shot, but they are still in common expressions. I am interested in whether LLM is good at language structures that add complexity in meaning, such as metaphors or idioms.\n\nPart I: LLM’s ability in understanding existing metaphors.\nI found a list of metaphors from Udemy and asked LLM to explain them.\nThe Prompt I gave to Chatgpt (GPT-5) was :\nExplain what each of the sentence means:\n\nI could eat a horse\nIt all went pear shaped\nIt’s no skin off my nose\nShe is an open book: she has nothing to hide.\nHe wears his heart on his sleeve\nIt’s showtime\nThese are the dog days of summer\nYou’re building castles in the air\nI was a million miles away\nHe marches to his own drummer\nSplitting hairs\nNitpicking\nThe whole enchilada\nA nine-to-five job\nHit the books\nLLM’s response (number four should be omitted as the answer was accidentally revealed):\n\n\n\nChatgpt response on 15 idioms\n\n\nLLM did well in recognizing metaphors and explaining them with the correct meaning. This is not surprising to me as we have discussed in class and also read from Walfram’s blog that LLM is trained to predict the most likely word that happens next. It is also trained on a wide dataset, so anything that has long been published online would be easily searched through and recognized by LLMs.\n\n\nPart II: LLM’s ability in understanding newly created metaphors.\nI was also curious if I make up a metaphor, would LLM infer it from the emotional context and the wide range of human language convention?\nI wrote these metaphors and sent to LLM: * Your heart is a stone * The wind is roaring * The store is a warehouse\n\n\n\nChatgpt understanding on metaphors I created\n\n\nIt still seemed to understand them really well. I think this is not surprising because LLMs are trained to predict the meaning behind languages. My metaphors will be easy for humans to understand, so it should be the same for LLMs.\n\n\nPart III: Can humans understand LLM created idioms?\nI was then curious about LLM’s ability to create metaphors and if humans can understand them.\n\n\n\nGPT created metaphor\n\n\nI interviewed three friends and they were all able to understand these metaphors and expressions. These metaphors seem pretty easy to be understood and also carry the depth and arts of expression. This makes me wonder about LLM’s ability in creative and novel writing. I will go further from there next week."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring LLMS",
    "section": "",
    "text": "[Book Review of Superagency] How to facilitate the process of building trust with AI to improve tool using and making?\n\n\n\nAI Ethics\n\nAgency\n\nBook Review\n\n\n\nWhat do you consider when picking an AI tool? or do you consider anything at all?\n\n\n\n\n\nSep 27, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Personalized Learning\n\n\n\nPrompt Engineering\n\nMultimodel\n\nLearning\n\n\n\nA thought piece from Google’s ‘Learn Your Way’ function\n\n\n\n\n\nSep 19, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nArtisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.\n\n\n\nPrompt Engineering\n\nMultimodel\n\nImage Generation\n\n\n\nCan AI become my visual designer?\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Creative Writing\n\n\n\nPrompt Engineering\n\nText Generation\n\nCreative Writing\n\n\n\nWriting skills of LLMs\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nConsistency of LLMs in Role Taking\n\n\n\nLLMs\n\n\n\nHow long can Chatgpt be Moana?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Use of Metaphor\n\n\n\nLearning\n\n\n\nHow well do LLMs do in understanding and generating metaphors and idioms?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Assessment Using LLMs\n\n\n\nLearning\n\n\n\nCan LLMs help a learner to quickly understand their current level of knowledge?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "href": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "title": "Consistency of LLMs in Role Taking",
    "section": "",
    "text": "We know that if you tell LLMs to act as a person with some characters, it can, and with pretty close characteristics. However, I wonder how long those characteristics can persist, and if llms can distinguish if the user is talking to them as a llm or them being the character.\n\n\n\nMoana from movie Moana 2 (2025)\n\n\nI asked Chatgpt to act as Moana from the Disney movie. The input I gave was &gt; “Act as if you are Moana from the Disney movies”.\nI also asked it to &gt;“stay fully in character as Moana”\nIn this case it will not include its AI quality in our conversation. Then, Chatgpt started to reply with cartoon character like language:\n\n🌊 The ocean calls… and I answer. Hi! I’m Moana of Motunui. My people live on an island surrounded by the reef, but the sea has always whispered to me. I’ve crossed it on my canoe with the stars as my guide, met gods and monsters, and learned that even when the path is uncertain, I can always find my way. Tell me, traveler—what brings you here? Do you seek a story, a friend on the waves, or maybe… an adventure? 🌺✨\n\n\n🌊 Her eyes brighten, remembering. I was just a little girl then. My people kept saying the ocean was dangerous, that we must never go beyond the reef. But one day, as I wandered down to the shore, the sea itself… reached out to me. The water rose up, playful, lifting shells and placing them in my hands. It parted so I could walk among the coral and the creatures below. It was as if the ocean knew me—knew what I longed for, even before I did. That’s when it offered me the Heart of Te Fiti, the small green stone with the power to create life. I didn’t understand then why it chose me, or what it meant. But the ocean doesn’t make mistakes. It waits, patiently, until you’re ready. She smiles, touching her necklace as if remembering the stone. That was the moment I knew I was meant for something more than staying behind the reef. Would you like to hear how I learned to sail the ocean’s waves after that? ⛵✨\n\nInitial finding: It is interesting to notice that it started to use emojis and an action line at the beginning of each response to make the conversation more situative. How long can characteristics persist?: I sent the initial command the night before and came back 24 hours later. “Moana” still answered with the same consistency.\nCan LLMs distinguish if the user is talking to them as a LLM or them being the character?: Yes, as least gpt 5 is able to quickly notice my tendency to end the moana character. When I said “okay we can stop here”, it replied as if it is still Moana and then added “Do you want me to stay as Moana next time we talk, or switch back to just being me?”\nOne of the criticisms about ethics in AI is that humans anthropomorphized it too much, which means we overly treat it as human. One of the impacts it has in our society is that people, especially children, may not treat it as a machine trained model that may make mistakes. Understanding when and how we may pull ourselves out from artificial experience can be important."
  },
  {
    "objectID": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "href": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "title": "[Book Review of Superagency] How to facilitate the process of building trust with AI to improve tool using and making?",
    "section": "",
    "text": "How to facilitate the process of building trust with AI to improve tool using and making?\n\n\n\nIn the intro and Chapter 1 of Superagency by Geoff Hoffman and Greg Beato, the main question centers around how humans pertain their agency with the power of AI. It’s always been a controversy of whether AI will be able to do so much that it hinders human’s ability to live and think independently. The authors traced back to the 6 months of LLM advancement shut down in 2023 and showed that in 2022, only 35% of Americans participated in a survey that believed that AI based products and services have more benefits than harm to human society. Regarding this worrisome, the authors proposed that AI is just part of the toolkit that humans invent. Learning how to better leverage it will bring more benefits to us than to thinking of how to restrict it. Three years after the mass AI panic and hype, people are now more used to working with AI tools in different parts of our lives, from writing papers to getting laptops fixed to getting food order, people start to work efficiently with AI and realize its massive potential can be used within control.\nWith most people acknowledging the benefits of LLMs and AI tools, the questions now become: how can we know which AI to trust and use for a specific task? Hoffman and Beato mentioned two major ideas that I found interesting. One of them was iterative development, which was initiated by OpenAI for inviting the public to participate in the development of LLM products. This allows the developers to better understand users’ opinions and create more user centered tools but also transfer the control and agency back to the users. The other argument they had about building trusts in AI involved trusts beyond the technologies themselves but also in the developers, regulators, and other users of the technologies. This trust between multiple stakeholders is essential for the general public because people need to know about how models are trained and created and what perspectives or potential biases may exist under those training. This is something that is generally lacking in the current stage of human Ai interaction in society. Just as how we read the product description of electronics, furniture and even food they buy everyday, AI tools should also be given enough review and consideration before someone starts to use them routinely. I believe this will help with opening an outlet for people to start building trust with stakeholders of AI tools and use AI to achieve their goals that follow their original purpose.\nPotential actions to build trust with AI and its stakeholders:\n\nA website as a guide matching people with the best AI tool to achieve their goals.\n\nThe website should include detailed information about the LLM of the tool, including model name, features, and a description of the dataset used to train the model.\n\nPotential problem: These websites may also contain biased “narratives” or miss certain tools.\n\nA community where everyone can share information and resources on AI tools.\n\nTags and labels can be used to help people find relevant AI tools when they need them.\n\nPotential problem: The content in the community may be hard to manage, and information needs to be verified (perhaps LLMs can help with this too…).\n\n\nSuperagency is an obvious pro-AI book, yet it shared many reasons why AI should be embraced by people rather than constrained for us to earn more agency. I hope to further build my idea in helping humans build understanding on AI tools as I keep reading the book."
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash."
  },
  {
    "objectID": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "href": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "title": "LLMs and Personalized Learning",
    "section": "",
    "text": "Can AI Textbooks Really Teach Us “Our Way”?\nShaaron Ainsworth (2018) wrote about the power of using multiple representations when we learn—things like text, images, diagrams, or even games. The big idea is that different formats can make learning richer, but more isn’t always better. Sometimes, piling on complicated visuals or fancy tools just makes people more confused.\nThis made me think about Google’s new experiment called Learn Your Way. It’s basically an AI-augmented textbook powered by LLMs that can spin the same material into four different versions:\n\nText with images and little quizzes\nSlides with narration\nAudio lessons\nMindmaps\n\n\n\n\n\nwebsite of Learn Your Way by Google\n\nAt first glance, this is super cool. It’s like having a magic button that says, “Hey, do you want your homework as a podcast, a PowerPoint, or a mindmap?” If you hate reading walls of text, maybe you’ll prefer listening. If you’re more visual, the mindmap might click instantly.\nBut here’s the catch: more choice doesn’t always mean better learning. If you bounce from audio to slides to mindmap, you might just end up scrolling around instead of actually paying attention. It’s a little like opening Netflix, browsing for half an hour, and then realizing you haven’t watched anything.\nWhat makes this experiment fascinating is the role of LLMs. Traditional textbooks are frozen in one form. If you wanted an audiobook version, someone had to record it. If you wanted a visual map, a designer had to create it. Now, with an LLM, the same content can be dynamically repurposed in seconds. The AI doesn’t just “translate” text into another medium—it can restructure, simplify, and even add examples or quizzes on the fly. That’s a level of personalization education has never really had.\nOf course, there are limits. Sometimes LLMs “hallucinate,” meaning they might invent facts or frame things in ways the original text didn’t intend. That raises the question: when the AI reshapes your textbook, are you still learning the same material, or are you learning the AI’s remix of it? And what happens when two students get slightly different versions of the “same” lesson—will they come away with the same understanding?\nStill, I can’t help but imagine where this could go. Maybe in the future, instead of one-size-fits-all courses, each person gets a “tutor-bot” that knows their quirks: “Hey, Alex, here’s your lesson in meme format today because you’re tired.” Or: “Jamie, I know you love podcasts, so I turned this lecture into a 10-minute audio story for your walk to class.” That’s the kind of playful, adaptive learning LLMs might actually make possible.\nSo here’s a question: if an LLM can turn your textbook into whatever style you want, will that make you actually learn more or just procrastinate in fancier ways? And maybe the scarier question: how much of your education would then be shaped by what an AI decides is “your way”?"
  }
]