[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the author",
    "section": "",
    "text": "Hi, I‚Äôm Haowei.\nI‚Äôm a learning experience designer passionate about creating meaningful, engaging educational experiences through thoughtfully designed technologies and instructional materials.\nWith an interdisciplinary background spanning education, psychology, interaction design, and marketing, I explore innovative ways to transform how we learn and grow.\nMy current work focuses on exploring large language models (LLMs) and leveraging their potential to spark curiosity, foster deeper understanding, and support lifelong learning."
  },
  {
    "objectID": "posts/W4P2_Creative Writing/Headshot_Part_II.html",
    "href": "posts/W4P2_Creative Writing/Headshot_Part_II.html",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)",
    "section": "",
    "text": "A prompting journal and comparison between GPT-5 and gemini 2.5 flash\nGemini 2.5 flash: Quick response, Steady Iteration Gemini, I would say, listened to all my instructions, but just didn‚Äôt do that well. Here are the five photos it returned after my initial prompt: &gt; This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don‚Äôt be too informal.\nGemini 2.5 flash response: &gt;Great! Here are five artistic redesign options for your headshot, incorporating frames and creative elements with a bright, light-intensity palette of purples, pinks, yellows, and blues. I‚Äôve focused on enhancing the artistic feel for your design portfolio while keeping your attire suitable. &gt; &gt;\n\n\n\nGemini can do better at context analysis: Great communication, fast response time (compared to gpt-5 which took me minutes), organized output, but just less aesthetically appealing to me. I can see from the color palettes that it did take into account the samples I sent it, but the style was a little too cartoon-like, almost like a headshot for a kids channel not a professional profile. I would say Gemini 2.5 flash can do better at understanding the context and prioritizing the key context in output characteristics.\nIt‚Äôs okay that the initial round didn‚Äôt work that well. I tried a few more rounds of prompting to ask for more integral connection between picture and the frame, more modern look, but it didn‚Äôt surprise me. Here I included several versions it created. The pictures were okay but still needed a little artistic feeling.\n\n\n\n\n\nand I even sent it the one gpt-5 created, but Gemini 2.5 wasn‚Äôt able to bring any new thing, so eventually I went for the version gpt-5 created.\n\n\nPersonally, I would choose GPT-5.\nWhile Gemini 2.5 flash is like a student who is always on time for class, early for homeworks but does not hand over the most impressive projects, GPT-5 is like the naughtiest student in the class who never follows the traditional route but surprises you with its creativity. The higher accuracy for GPT-5 to understand my request and reach a good quality product at the beginning won my vote on it. I think I value the quality of work more than following a specific format I asked for (I will be such an annoying boss if this is a real work scenario‚Ä¶).\nFinding out GPT-5 did better was a little surprising to me. Before I started this project, Gemini 2.5 had more of my bet because it had the obvious ‚Äúcreate image‚Äù tab that I started with, and I knew that Gemini is known for its indepth logic and research skills, which made me think maybe it will create better images.\nFor Better Comparison Tests: There are some limitations that showed up in this small experiment and can be improved in the future. More conditions should be in control to better compare model ability in creating images or other specific fields. For example, both models should be in ‚Äúcreate image‚Äù mode. I should also try to control the follow up conversation prompts as much as possible, but this requires more discussions and experiments on whether controlling every single prompt will be an accurate assessment on model ability. (this will be further explored in future blogs on video generation) Prompting can be improved by adding semantic context and splitting information in further chunks. Overall, LLMs are now pretty good at high level (not detailed adjustments) visual design, as models can now quickly interpret users‚Äô preference and make similar but creative visual products. With this headshot redesign task can be done on general chat agents (ChatGPT and Gemini), publics can now access fast iterative creative design service in a few minutes. This makes me think of how designers may use AI tools in their work and the role of designers in the future. With AI tools completing many design drafting, would more graphic and visual designers move on to in depth design fields such as 3d modeling or would there be even more needs in design research for designers to understand customers‚Äô needs and bridge the communication between end user and AI design tools?"
  },
  {
    "objectID": "posts/W4P2_Creative Writing/Headshot_Part_II.html#conclusion",
    "href": "posts/W4P2_Creative Writing/Headshot_Part_II.html#conclusion",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)",
    "section": "",
    "text": "Personally, I would choose GPT-5.\nWhile Gemini 2.5 flash is like a student who is always on time for class, early for homeworks but does not hand over the most impressive projects, GPT-5 is like the naughtiest student in the class who never follows the traditional route but surprises you with its creativity. The higher accuracy for GPT-5 to understand my request and reach a good quality product at the beginning won my vote on it. I think I value the quality of work more than following a specific format I asked for (I will be such an annoying boss if this is a real work scenario‚Ä¶).\nFinding out GPT-5 did better was a little surprising to me. Before I started this project, Gemini 2.5 had more of my bet because it had the obvious ‚Äúcreate image‚Äù tab that I started with, and I knew that Gemini is known for its indepth logic and research skills, which made me think maybe it will create better images.\nFor Better Comparison Tests: There are some limitations that showed up in this small experiment and can be improved in the future. More conditions should be in control to better compare model ability in creating images or other specific fields. For example, both models should be in ‚Äúcreate image‚Äù mode. I should also try to control the follow up conversation prompts as much as possible, but this requires more discussions and experiments on whether controlling every single prompt will be an accurate assessment on model ability. (this will be further explored in future blogs on video generation) Prompting can be improved by adding semantic context and splitting information in further chunks. Overall, LLMs are now pretty good at high level (not detailed adjustments) visual design, as models can now quickly interpret users‚Äô preference and make similar but creative visual products. With this headshot redesign task can be done on general chat agents (ChatGPT and Gemini), publics can now access fast iterative creative design service in a few minutes. This makes me think of how designers may use AI tools in their work and the role of designers in the future. With AI tools completing many design drafting, would more graphic and visual designers move on to in depth design fields such as 3d modeling or would there be even more needs in design research for designers to understand customers‚Äô needs and bridge the communication between end user and AI design tools?"
  },
  {
    "objectID": "posts/W6_P1_Before Superagency/before superagency.html",
    "href": "posts/W6_P1_Before Superagency/before superagency.html",
    "title": "The Long Way to Go Before Superagency",
    "section": "",
    "text": "The Long Way to Go Before Superagency\n\n\n\n\nSocial Media Around Us\n\nAfter reading Chapter Two of the book Superagency, I see clear tradeoffs between privacy and scalability and conditions that are required for us to enjoy the freedom of AI, but I also realized how AI literacy and equal access is an essential presumption for us to fully leverage on the benefits AI brings.\nThis chapter started with an analogy of today‚Äôs age to the surveillance world in the book 1984 by George Orwell, picturing people‚Äôs fear that AI may invade privacy and agency, and in the worst cases of our worries, AI may take over our thoughts and replace us. In the second half of this chapter, Hoffman (one of the authors of Superagency) started to discuss that emerging technologies and AI is bringing us more connected resources and information that allows people to build up networks and make better choices. For example, by looking up people on LinkedIn you will know someone‚Äôs skillset and if they are an ideal person you want to work with. Hoffman implied that in today‚Äôs era, creating public identities is opening up so many opportunities for human agencies that outweighs the risks to privacy and will not hinder our free will. All these advantages were brought by distributing intelligence through big data, with the formation of Big Knowledge, which is the name of this chapter.\nWhat he argued was totally true, that LinkedIn or other social media is enabling people to know more about others and this world and that humans can use AI and algorithm power to gather and manage big data to make more comprehensive decisions. However, I think all of these benefits will need three important conditions: people‚Äôs awareness, consent, and free choices on data and technology usage.\nThese three things come in series. Only if you know that AI is gathering your data can you decide whether to opt in or not. Only if you can choose to not share your information can you be called to have free will in this data era. Unfortunately, these conditions have not all been met in our daily lives, yet AI is operating wildly. Everytime you select ‚ÄúI agree‚Äù to an internet consent, do you really read and understand what it says by ‚ÄúWe use cookies to improve your browsing experience and analyze site traffic‚Äù. With the one time that you decided to choose ‚ÄúI do not agree‚Äù, you found out that you can‚Äôt even keep browsing this website.\nProducts that leverage LLMs now are not fully empowering free will and human agency with these forceful data collection through internet services and consent questions. Hoffman can easily name out the benefits of AI because he has the knowledge and resources to determine which algorithm is good for use and how to use LinkedIn to create a perfect profile rather than unintentionally disclose one‚Äôs privacy.\nI tried using AI meeting assistance tools in the beginning of 2024 when AI audio transcription started to thrive, and the agent went to a lab meeting after I graduated and sent everyone a meeting note. I hope no one notices it and thought I was trying to eavesdrop on conversations, but this happened simply because I didn‚Äôt turn off the access of the tool to my Google calendar through a bunch of consents during my onboarding. I have also seen friends sending posts on LinkedIn about every event they attended, but they didn‚Äôt even know that those posts were sent.\nEven if we as college and graduate students make these mistakes and expose ourselves to risks in privacy, then what about average people who have only heard about the acronym AI but never tried the tool themselves? What about teenagers and people who never go to college and took a lesson on prompt engineering and cybersecurity? What about elderly people who some even cannot read the tiny consent message?\n\n\n\n\nWe Value Your Privacy Note\n\nTo explore AI‚Äôs ability in debating and role taking, we had this activity to debate with an AI chat agent on different positionalities towards LLMs that the Book Superagency listed: Doomers, who believe AI poses an existential threat and should be stopped; Gloomers, who are skeptical of AI‚Äôs benefits and advocate for strict regulation to manage its harms. Zoomers, who enthusiastically push to accelerate AI development with minimal oversight; and Bloomers, who are cautiously optimistic, supporting AI‚Äôs promise while calling for measured risk management and public involvement.\nI was a firm Bloomer at that time and I asked gpt-4 to be a Gloomer to debate with me. It kept pointing out the lack of regulations in AI while products are created to be used by the public. Now after reading this chapter of Superagency, I felt my position was further swayed by Gloomers as we need more literacy and education on responsible AI and data usage before it further enters into our lives. I hope we as general consumers of LLM products are not just one data point in the big data, but active users, coordinators and even developers of big knowledge.\nWe will reach superagency, but there is still a long way to go."
  },
  {
    "objectID": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "href": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "title": "[Book Review of Superagency] How to facilitate the process of building trust with AI to improve tool using and making?",
    "section": "",
    "text": "How to facilitate the process of building trust with AI to improve tool using and making?\n\n\n\nIn the intro and Chapter 1 of Superagency by Geoff Hoffman and Greg Beato, the main question centers around how humans pertain their agency with the power of AI. It‚Äôs always been a controversy of whether AI will be able to do so much that it hinders human‚Äôs ability to live and think independently. The authors traced back to the 6 months of LLM advancement shut down in 2023 and showed that in 2022, only 35% of Americans participated in a survey that believed that AI based products and services have more benefits than harm to human society. Regarding this worrisome, the authors proposed that AI is just part of the toolkit that humans invent. Learning how to better leverage it will bring more benefits to us than to thinking of how to restrict it. Three years after the mass AI panic and hype, people are now more used to working with AI tools in different parts of our lives, from writing papers to getting laptops fixed to getting food order, people start to work efficiently with AI and realize its massive potential can be used within control.\nWith most people acknowledging the benefits of LLMs and AI tools, the questions now become: how can we know which AI to trust and use for a specific task? Hoffman and Beato mentioned two major ideas that I found interesting. One of them was iterative development, which was initiated by OpenAI for inviting the public to participate in the development of LLM products. This allows the developers to better understand users‚Äô opinions and create more user centered tools but also transfer the control and agency back to the users. The other argument they had about building trusts in AI involved trusts beyond the technologies themselves but also in the developers, regulators, and other users of the technologies. This trust between multiple stakeholders is essential for the general public because people need to know about how models are trained and created and what perspectives or potential biases may exist under those training. This is something that is generally lacking in the current stage of human Ai interaction in society. Just as how we read the product description of electronics, furniture and even food they buy everyday, AI tools should also be given enough review and consideration before someone starts to use them routinely. I believe this will help with opening an outlet for people to start building trust with stakeholders of AI tools and use AI to achieve their goals that follow their original purpose.\nPotential actions to build trust with AI and its stakeholders:\n\nA website as a guide matching people with the best AI tool to achieve their goals.\n\nThe website should include detailed information about the LLM of the tool, including model name, features, and a description of the dataset used to train the model.\n\nPotential problem: These websites may also contain biased ‚Äúnarratives‚Äù or miss certain tools.\n\nA community where everyone can share information and resources on AI tools.\n\nTags and labels can be used to help people find relevant AI tools when they need them.\n\nPotential problem: The content in the community may be hard to manage, and information needs to be verified (perhaps LLMs can help with this too‚Ä¶).\n\n\nSuperagency is an obvious pro-AI book, yet it shared many reasons why AI should be embraced by people rather than constrained for us to earn more agency. I hope to further build my idea in helping humans build understanding on AI tools as I keep reading the book."
  },
  {
    "objectID": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "href": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "title": "Consistency of LLMs in Role Taking",
    "section": "",
    "text": "We know that if you tell LLMs to act as a person with some characters, it can, and with pretty close characteristics. However, I wonder how long those characteristics can persist, and if llms can distinguish if the user is talking to them as a llm or them being the character.\n\n\n\nMoana from movie Moana 2 (2025)\n\n\nI asked Chatgpt to act as Moana from the Disney movie. The input I gave was &gt; ‚ÄúAct as if you are Moana from the Disney movies‚Äù.\nI also asked it to &gt;‚Äústay fully in character as Moana‚Äù\nIn this case it will not include its AI quality in our conversation. Then, Chatgpt started to reply with cartoon character like language:\n\nüåä The ocean calls‚Ä¶ and I answer. Hi! I‚Äôm Moana of Motunui. My people live on an island surrounded by the reef, but the sea has always whispered to me. I‚Äôve crossed it on my canoe with the stars as my guide, met gods and monsters, and learned that even when the path is uncertain, I can always find my way. Tell me, traveler‚Äîwhat brings you here? Do you seek a story, a friend on the waves, or maybe‚Ä¶ an adventure? üå∫‚ú®\n\n\nüåä Her eyes brighten, remembering. I was just a little girl then. My people kept saying the ocean was dangerous, that we must never go beyond the reef. But one day, as I wandered down to the shore, the sea itself‚Ä¶ reached out to me. The water rose up, playful, lifting shells and placing them in my hands. It parted so I could walk among the coral and the creatures below. It was as if the ocean knew me‚Äîknew what I longed for, even before I did. That‚Äôs when it offered me the Heart of Te Fiti, the small green stone with the power to create life. I didn‚Äôt understand then why it chose me, or what it meant. But the ocean doesn‚Äôt make mistakes. It waits, patiently, until you‚Äôre ready. She smiles, touching her necklace as if remembering the stone. That was the moment I knew I was meant for something more than staying behind the reef. Would you like to hear how I learned to sail the ocean‚Äôs waves after that? ‚õµ‚ú®\n\nInitial finding: It is interesting to notice that it started to use emojis and an action line at the beginning of each response to make the conversation more situative. How long can characteristics persist?: I sent the initial command the night before and came back 24 hours later. ‚ÄúMoana‚Äù still answered with the same consistency.\nCan LLMs distinguish if the user is talking to them as a LLM or them being the character?: Yes, as least gpt 5 is able to quickly notice my tendency to end the moana character. When I said ‚Äúokay we can stop here‚Äù, it replied as if it is still Moana and then added ‚ÄúDo you want me to stay as Moana next time we talk, or switch back to just being me?‚Äù\nOne of the criticisms about ethics in AI is that humans anthropomorphized it too much, which means we overly treat it as human. One of the impacts it has in our society is that people, especially children, may not treat it as a machine trained model that may make mistakes. Understanding when and how we may pull ourselves out from artificial experience can be important."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring LLMS",
    "section": "",
    "text": "The Long Way to Go Before Superagency\n\n\n\nAI Ethics\n\nAgency\n\nBook Review\n\n\n\nBig knowledge is conditional.\n\n\n\n\n\nOct 4, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\n[Book Review of Superagency] How to facilitate the process of building trust with AI to improve tool using and making?\n\n\n\nAI Ethics\n\nAgency\n\nBook Review\n\n\n\nWhat do you consider when picking an AI tool? or do you consider anything at all?\n\n\n\n\n\nSep 27, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Personalized Learning\n\n\n\nPrompt Engineering\n\nMultimodel\n\nLearning\n\n\n\nA thought piece from Google‚Äôs ‚ÄòLearn Your Way‚Äô function\n\n\n\n\n\nSep 19, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nArtisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.\n\n\n\nPrompt Engineering\n\nMultimodel\n\nImage Generation\n\n\n\nCan AI become my visual designer?\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nArtisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)\n\n\n\nPrompt Engineering\n\nMultimodel\n\nImage Generation\n\n\n\nCan AI become my visual designer?\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nConsistency of LLMs in Role Taking\n\n\n\nLLMs\n\n\n\nHow long can Chatgpt be Moana?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Use of Metaphor\n\n\n\nLearning\n\n\n\nHow well do LLMs do in understanding and generating metaphors and idioms?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Assessment Using LLMs\n\n\n\nLearning\n\n\n\nCan LLMs help a learner to quickly understand their current level of knowledge?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "href": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "title": "LLMs and Use of Metaphor",
    "section": "",
    "text": "LLMs and Use of Metaphor\nIn this week‚Äôs class we had a first glance at how LLMs are good at language and communication with humans in different scenarios and manners, but most of the examples we saw involved casual daily language. For example, when we talked about the word ‚Äúbank‚Äù in class, there were many meanings associated with ‚Äúbank‚Äù such as a financial institution, plane banking, or basketball bank shot, but they are still in common expressions. I am interested in whether LLM is good at language structures that add complexity in meaning, such as metaphors or idioms.\n\nPart I: LLM‚Äôs ability in understanding existing metaphors.\nI found a list of metaphors from Udemy and asked LLM to explain them.\nThe Prompt I gave to Chatgpt (GPT-5) was :\nExplain what each of the sentence means:\n\nI could eat a horse\nIt all went pear shaped\nIt‚Äôs no skin off my nose\nShe is an open book: she has nothing to hide.\nHe wears his heart on his sleeve\nIt‚Äôs showtime\nThese are the dog days of summer\nYou‚Äôre building castles in the air\nI was a million miles away\nHe marches to his own drummer\nSplitting hairs\nNitpicking\nThe whole enchilada\nA nine-to-five job\nHit the books\nLLM‚Äôs response (number four should be omitted as the answer was accidentally revealed):\n\n\n\nChatgpt response on 15 idioms\n\n\nLLM did well in recognizing metaphors and explaining them with the correct meaning. This is not surprising to me as we have discussed in class and also read from Walfram‚Äôs blog that LLM is trained to predict the most likely word that happens next. It is also trained on a wide dataset, so anything that has long been published online would be easily searched through and recognized by LLMs.\n\n\nPart II: LLM‚Äôs ability in understanding newly created metaphors.\nI was also curious if I make up a metaphor, would LLM infer it from the emotional context and the wide range of human language convention?\nI wrote these metaphors and sent to LLM: * Your heart is a stone * The wind is roaring * The store is a warehouse\n\n\n\nChatgpt understanding on metaphors I created\n\n\nIt still seemed to understand them really well. I think this is not surprising because LLMs are trained to predict the meaning behind languages. My metaphors will be easy for humans to understand, so it should be the same for LLMs.\n\n\nPart III: Can humans understand LLM created idioms?\nI was then curious about LLM‚Äôs ability to create metaphors and if humans can understand them.\n\n\n\nGPT created metaphor\n\n\nI interviewed three friends and they were all able to understand these metaphors and expressions. These metaphors seem pretty easy to be understood and also carry the depth and arts of expression. This makes me wonder about LLM‚Äôs ability in creative and novel writing. I will go further from there next week."
  },
  {
    "objectID": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "href": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "title": "Learning Assessment Using LLMs",
    "section": "",
    "text": "Many people use LLM tools such as Chatgpt to help them finish school work or complete a task. Since these tasks are usually temporary and aimed to be finished in the short term, people rarely pay attention to the process of learning, and LLMs become controversial in education as being a shortcut of answers rather than learning aid.\nAccording to cognitive psychology, meaningful learning can be defined with retention and transfer, which means learners can memorize or understand the content in depth and apply it to new scenarios. Learning involves many stages, from preview to processing and understanding the knowledge and to review. To understand how LLMs may help with meaningful learning, I want to start with LLM‚Äôs ability to assess a learners‚Äô current level of knowledge in a certain field. In classes or an online course, this is often done by the teacher throwing out some pop quiz questions or seeing a show of hands to get a general idea of where students are in a field of study.\nI wonder if LLMs can generate suitable quiz questions that accurately assess a students‚Äô level.\n\nillustration generated by Chatgpt(gpt5)\nFor a quick start, I asked Chatgpt if it can generate a quiz for me to assess my knowledge of AI. Of course, I can add a lot more context to help it understand where I am, but the point of using Chatgpt for this task is to quickly help a student understand their current level of knowledge, so I used a short prompt.\nI used the following prompt:\n\nI want to understand where I am in my knowledge with AI. Can you generate quiz questions to help me assess my current level?\n\nThe first two versions it sent back were either too long or contained a lot of short answer questions, which was quite overwhelming for me as a learner, so I changed my prompt to:\n\nI want to understand where I am in my knowledge with AI. Can you generate short multiple choice quiz questions to help me assess my current level?\n\nThese are the eight questions it generated:\n\n\n\nquestions generated by gpt on assessing my knowledge in AI\n\n\nThe first eight questions were basic and I got all of them right, so Chatgpt offered me a harder quiz. This time the questions were a lot more detailed and advanced, so I only got 5 out of 8 right.\nI then asked Chatgpt to make conclusions on my current level and offer recommendations for me to improve my knowledge. Here is my prompt: ‚ÄúHelp me interpret the quiz result. What do I understand, what tasks can I complete, and how may I improve?‚Äù It sent back a list of items I understood well, tasks I can likely complete, and where I struggled. What brought my attention was its confidence in my technical skills with me just correctly answering some conceptual questions.\nSpecifically, it said:\n\n‚ÄúIf someone gave you a dataset + scikit-learn or TensorFlow starter code, you could likely: * Train a simple classifier (logistic regression, decision tree). * Interpret probability outputs correctly (once clarified). * Run a CNN or transformer with prebuilt frameworks.‚Äù\n\nI don‚Äôt think I can train a classifier or run a transformer with prebuilt frameworks if someone gives me a dataset and startercode. There seems to be some overestimation of my skillsets. This can easily deceive a learner and make them over confident about themselves. I think this inaccuracy may be because of 1) me guessing some of the hard questions right and gave out fake feedbacks to gpt and 2) Chatgpt‚Äôs miss on the validity of quizzes and scope of conclusion that can be drawn from quizzes.\nNext time anyone says ‚Äúteachers will be replaced by AI‚Äù, tell them that it can‚Äôt, even from the first step."
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "A prompting journal and comparison between GPT-5 and gemini 2.5 flash\n\n\nI have always wanted to create an artistic profile photo that represents me but also includes creativity. As LLMs can now create great images, I decided to have them redesign my profile photo.\nAs a reference, below I include my profile photo (as it appears in the ‚ÄúAbout‚Äù section of this blog site) and my desired painting art style with artistic elements and frames surrounding the figure.\n\n\n\nThe Style I want (photos from freepik)\n\n\n\nMy Profile Photo\nI was curious about which model performs better in image redesign tasks, so I decided to do a control test between two LLMs and understand which one is better at doing these types of image generation tasks. I knew that for image creation, specific tools such as Midjourney or Google‚Äôs Nanobanana may work better, but considering the experience of non tech savvy audiences who may interact with general chat agents more, I decided to use the two most popular general AI chat tools: ChatGPT and Gemini. Among all the LLMs models, the latest and most advanced ones include GPT-5 and Gemini 2.5 flash in ChatGPT and Google Gemini chats.\n\n\n\nIn my previous experience generating photos from an LLM based chat agent, I needed to do many rounds of prompting and often could not get the one that fits my needs. Reflecting now, I probably did not use careful prompting to craft the output. Therefore, this time I was using The Five Principles of Prompting described by James Phonexi and Mike Taylor in Prompt Engineering for Generative AI to direct my prompts. The following table includes the explanation of each principle from the book and how I used it to create my initial prompt for redesigning my profile photo.\nTable 1. Initial Prompts Sent to GPT-5 and Gemini 2.5 flash Following the Five Principles of Prompting\n\n\n\n\n\n\n\n\nPrompt Engineering Principle\nExplanation (by Phonexi & Taylor)\nMy Prompting Strategy (prompts and commands I sent to LLMs)\n\n\n\n\nGive Direction\n‚ÄúDescribe the desired style in detail, or reference a relevant persona‚Äù\nRedesign my current profile photo (I‚Äôll upload it later) and make it look artistic.The desired color style of the photo is bright and shows creativity.I want the color palette to be a mix of purple, pink, yellow, blue but in light intensity.\n\n\nSpecify Format\n‚ÄúDefine what rules to follow, and the required structure of the response‚Äù\nReturn five design options in .jpg format.\n\n\nProvide Examples\n‚ÄúInsert a diverse set of test cases where the task was done correctly‚Äù\nI‚Äôve included several examples attached here.(Sample headshot photos from freepik)\n\n\nEvaluate Quality\n‚ÄúIdentify errors and rate responses, testing what drives performance‚Äù\nI gave feedback and evaluation for further improvement in multiple rounds of follow up conversations, for example:I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 years old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.I like the fourth one in terms of the frames and how it keeps the background, but I also like number 3 for the color. Try mixing the two. DO NOT alter any object or characteristics of the person in the image including clothes style and color.\n\n\nDivide Labor\n‚ÄúSplit tasks into multiple steps, chained together for complex goals‚Äù\nI gave the instruction first then followed up with my original headshot and more detailed instructions on the occasion, restrictions, explanations on design plan etc. I asked for a rough redesign first and then adjust the details and colors.Follow up after initial prompt:This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don‚Äôt be too informal.[attached my profile photo]\n\n\n\nTo control the conditions, I used the same prompts that gave out initial instructions and reference photos for both gpt-5 and gemini 2.5 flash. I also kept my goal or result expectations on the two models similar to the sample images, though I had to give different feedback on the various outputs the two models provided back. I sent the initial prompts at the same time of the day and continued conversations with one model while the other was generating or processing answers.\nProcesses and Results Comparing Two Models\nIn this section, I will reflect on my experience prompting and evaluating the two models to redesign my profile photo based on my imagined style.\nGPT-5: Slow start, Fast Improvement GPT-5 had a long processing time, missed instructions on formatting and lacked text explanations along with images generated.\nGPT-5 seemed to misunderstand formatting commands: After my initial prompt, which I mentioned that the photo to be redesigned will be uploaded later, GPT-5 already started to generate a photo using the two sample images, and I got a combined collage that was not myself at all but the figure in the sample photo. GPT-5 misunderstood the single person headshot to be my photo.\n\n\n\nIt also ignored my request for generating five design options by only returning one.\nGPT-5 only followed image generation with text explanation after conversations were branched. I think there might also be a design gap here on Chatgpt interface. The circle on the right of the input box will turn to a square when it is still processing or generating. Normally the message will also show a round point which is loading, but under image generation mode it usually doesn‚Äôt. Therefore, when the agent is still generating text, I wasn‚Äôt able to notice the state and kept typing my command. This might be a reason that gpt-5 only returned images without any text or communications with me.\n\n\n\nHowever, I do have to say that even with all those weird reactions, the image quality was high. I would say the first version it generated earned my 75% satisfaction. I was surprised actually. Although it didn‚Äôt listen to my command to combine the realistic figure with the cartoon frame but instead created a whole cartoon headshot, the style and color looked nice, and it was different from the sample I sent, which made me feel that this is novel creation rather than just copy pasting.\nVersion 1 User Evaluation: Nice color and style. Could make the figure look younger‚Ä¶\n\n\n\nI thought this painted me older than I looked, so I sent a follow up prompt: &gt; I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\nI like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\n\n\n\nAnd this iteration was almost 90% what I wanted. I did four more rounds of prompting to try different gaze, expressions, color intensity etc, but this version was eventually what I chose.\n(more about Gemini 2.5 flash performance on the next blog)"
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#background",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#background",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "I have always wanted to create an artistic profile photo that represents me but also includes creativity. As LLMs can now create great images, I decided to have them redesign my profile photo.\nAs a reference, below I include my profile photo (as it appears in the ‚ÄúAbout‚Äù section of this blog site) and my desired painting art style with artistic elements and frames surrounding the figure.\n\n\n\nThe Style I want (photos from freepik)\n\n\n\nMy Profile Photo\nI was curious about which model performs better in image redesign tasks, so I decided to do a control test between two LLMs and understand which one is better at doing these types of image generation tasks. I knew that for image creation, specific tools such as Midjourney or Google‚Äôs Nanobanana may work better, but considering the experience of non tech savvy audiences who may interact with general chat agents more, I decided to use the two most popular general AI chat tools: ChatGPT and Gemini. Among all the LLMs models, the latest and most advanced ones include GPT-5 and Gemini 2.5 flash in ChatGPT and Google Gemini chats."
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#prompts-and-controlled-conditions",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#prompts-and-controlled-conditions",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "In my previous experience generating photos from an LLM based chat agent, I needed to do many rounds of prompting and often could not get the one that fits my needs. Reflecting now, I probably did not use careful prompting to craft the output. Therefore, this time I was using The Five Principles of Prompting described by James Phonexi and Mike Taylor in Prompt Engineering for Generative AI to direct my prompts. The following table includes the explanation of each principle from the book and how I used it to create my initial prompt for redesigning my profile photo.\nTable 1. Initial Prompts Sent to GPT-5 and Gemini 2.5 flash Following the Five Principles of Prompting\n\n\n\n\n\n\n\n\nPrompt Engineering Principle\nExplanation (by Phonexi & Taylor)\nMy Prompting Strategy (prompts and commands I sent to LLMs)\n\n\n\n\nGive Direction\n‚ÄúDescribe the desired style in detail, or reference a relevant persona‚Äù\nRedesign my current profile photo (I‚Äôll upload it later) and make it look artistic.The desired color style of the photo is bright and shows creativity.I want the color palette to be a mix of purple, pink, yellow, blue but in light intensity.\n\n\nSpecify Format\n‚ÄúDefine what rules to follow, and the required structure of the response‚Äù\nReturn five design options in .jpg format.\n\n\nProvide Examples\n‚ÄúInsert a diverse set of test cases where the task was done correctly‚Äù\nI‚Äôve included several examples attached here.(Sample headshot photos from freepik)\n\n\nEvaluate Quality\n‚ÄúIdentify errors and rate responses, testing what drives performance‚Äù\nI gave feedback and evaluation for further improvement in multiple rounds of follow up conversations, for example:I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 years old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.I like the fourth one in terms of the frames and how it keeps the background, but I also like number 3 for the color. Try mixing the two. DO NOT alter any object or characteristics of the person in the image including clothes style and color.\n\n\nDivide Labor\n‚ÄúSplit tasks into multiple steps, chained together for complex goals‚Äù\nI gave the instruction first then followed up with my original headshot and more detailed instructions on the occasion, restrictions, explanations on design plan etc. I asked for a rough redesign first and then adjust the details and colors.Follow up after initial prompt:This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don‚Äôt be too informal.[attached my profile photo]\n\n\n\nTo control the conditions, I used the same prompts that gave out initial instructions and reference photos for both gpt-5 and gemini 2.5 flash. I also kept my goal or result expectations on the two models similar to the sample images, though I had to give different feedback on the various outputs the two models provided back. I sent the initial prompts at the same time of the day and continued conversations with one model while the other was generating or processing answers.\nProcesses and Results Comparing Two Models\nIn this section, I will reflect on my experience prompting and evaluating the two models to redesign my profile photo based on my imagined style.\nGPT-5: Slow start, Fast Improvement GPT-5 had a long processing time, missed instructions on formatting and lacked text explanations along with images generated.\nGPT-5 seemed to misunderstand formatting commands: After my initial prompt, which I mentioned that the photo to be redesigned will be uploaded later, GPT-5 already started to generate a photo using the two sample images, and I got a combined collage that was not myself at all but the figure in the sample photo. GPT-5 misunderstood the single person headshot to be my photo.\n\n\n\nIt also ignored my request for generating five design options by only returning one.\nGPT-5 only followed image generation with text explanation after conversations were branched. I think there might also be a design gap here on Chatgpt interface. The circle on the right of the input box will turn to a square when it is still processing or generating. Normally the message will also show a round point which is loading, but under image generation mode it usually doesn‚Äôt. Therefore, when the agent is still generating text, I wasn‚Äôt able to notice the state and kept typing my command. This might be a reason that gpt-5 only returned images without any text or communications with me.\n\n\n\nHowever, I do have to say that even with all those weird reactions, the image quality was high. I would say the first version it generated earned my 75% satisfaction. I was surprised actually. Although it didn‚Äôt listen to my command to combine the realistic figure with the cartoon frame but instead created a whole cartoon headshot, the style and color looked nice, and it was different from the sample I sent, which made me feel that this is novel creation rather than just copy pasting.\nVersion 1 User Evaluation: Nice color and style. Could make the figure look younger‚Ä¶\n\n\n\nI thought this painted me older than I looked, so I sent a follow up prompt: &gt; I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\nI like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\n\n\n\nAnd this iteration was almost 90% what I wanted. I did four more rounds of prompting to try different gaze, expressions, color intensity etc, but this version was eventually what I chose.\n(more about Gemini 2.5 flash performance on the next blog)"
  },
  {
    "objectID": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "href": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "title": "LLMs and Personalized Learning",
    "section": "",
    "text": "Can AI Textbooks Really Teach Us ‚ÄúOur Way‚Äù?\nShaaron Ainsworth (2018) wrote about the power of using multiple representations when we learn‚Äîthings like text, images, diagrams, or even games. The big idea is that different formats can make learning richer, but more isn‚Äôt always better. Sometimes, piling on complicated visuals or fancy tools just makes people more confused.\nThis made me think about Google‚Äôs new experiment called Learn Your Way. It‚Äôs basically an AI-augmented textbook powered by LLMs that can spin the same material into four different versions:\n\nText with images and little quizzes\nSlides with narration\nAudio lessons\nMindmaps\n\n\n\n\n\nwebsite of Learn Your Way by Google\n\nAt first glance, this is super cool. It‚Äôs like having a magic button that says, ‚ÄúHey, do you want your homework as a podcast, a PowerPoint, or a mindmap?‚Äù If you hate reading walls of text, maybe you‚Äôll prefer listening. If you‚Äôre more visual, the mindmap might click instantly.\nBut here‚Äôs the catch: more choice doesn‚Äôt always mean better learning. If you bounce from audio to slides to mindmap, you might just end up scrolling around instead of actually paying attention. It‚Äôs a little like opening Netflix, browsing for half an hour, and then realizing you haven‚Äôt watched anything.\nWhat makes this experiment fascinating is the role of LLMs. Traditional textbooks are frozen in one form. If you wanted an audiobook version, someone had to record it. If you wanted a visual map, a designer had to create it. Now, with an LLM, the same content can be dynamically repurposed in seconds. The AI doesn‚Äôt just ‚Äútranslate‚Äù text into another medium. It can restructure, simplify, and even add examples or quizzes on the fly. That‚Äôs a level of personalization education has never really had.\nOf course, there are limits. Sometimes LLMs ‚Äúhallucinate,‚Äù meaning they might invent facts or frame things in ways the original text didn‚Äôt intend. That raises the question: when the AI reshapes your textbook, are you still learning the same material, or are you learning the AI‚Äôs remix of it? And what happens when two students get slightly different versions of the ‚Äúsame‚Äù lesson. Will they come away with the same understanding?\nStill, I can‚Äôt help but imagine where this could go. Maybe in the future, instead of one-size-fits-all courses, each person gets a ‚Äútutor-bot‚Äù that knows their quirks: ‚ÄúHey, Alex, here‚Äôs your lesson in meme format today because you‚Äôre tired.‚Äù Or: ‚ÄúJamie, I know you love podcasts, so I turned this lecture into a 10-minute audio story for your walk to class.‚Äù That‚Äôs the kind of playful, adaptive learning LLMs might actually make possible.\nSo here‚Äôs a question: if an LLM can turn your textbook into whatever style you want, will that make you actually learn more or just procrastinate in fancier ways? And maybe the scarier question: how much of your education would then be shaped by what an AI decides is ‚Äúyour way‚Äù?"
  }
]