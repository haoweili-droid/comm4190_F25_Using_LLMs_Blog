[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the author",
    "section": "",
    "text": "Hi, I’m Haowei.\nI’m a learning experience designer passionate about creating meaningful, engaging educational experiences through thoughtfully designed technologies and instructional materials.\nWith an interdisciplinary background spanning education, psychology, interaction design, and marketing, I explore innovative ways to transform how we learn and grow.\nMy current work focuses on exploring large language models (LLMs) and leveraging their potential to spark curiosity, foster deeper understanding, and support lifelong learning."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html",
    "href": "posts/W6_P2_Receipe/receipe.html",
    "title": "Chef LLM",
    "section": "",
    "text": "As someone who pays attention to balance in daily diet and genuinely enjoys cooking, I often turn to ChatGPT for quick suggestions about what to eat in a day. Those interactions are usually practical and lightweight, focused on efficiency rather than creativity.\nThis time, I wanted to explore a different question. Instead of asking an LLM to recommend food, I wanted to see whether it could participate in the creative process of inventing a dish, and further, whether it could help think through how that dish might live in the real world as part of a restaurant menu and a marketing strategy. This post reflects on the process of how I used an LLM not just to generate a recipe, but to understand novelty, presentation, and storytelling around food.\n\n\n\nI did not begin by asking for a traditional Chinese recipe. Instead, I framed the prompt around novelty and reinterpretation.\nMy prompt: &gt; “Create a recipe for a novel Chinese food.”\nThis prompt was intentionally open-ended. I avoided specifying ingredients or techniques because I wanted to see how the model would reason about what “novel” might mean in a culinary context.\nLLM response (excerpt): &gt; “This dish blends traditional Cantonese dim sum with floral osmanthus fragrance and a modern twist, transforming black vinegar into edible pearls rather than a dipping sauce.”\nAfter generating the core idea, I asked the model to help visualize the recipe as a step-by-step process. This was not only about making the dish easier to follow. It was also about understanding how LLMs translate creative work into structured systems. The resulting recipe broke cooking into clear stages: making the osmanthus syrup, preparing the filling, forming the wrappers, assembling the dumplings, and creating the vinegar pearls. Each step was treated as a transformation, with inputs and outputs clearly defined. This structure felt familiar from a technical perspective. It resembled how engineers describe pipelines or workflows. In this sense, the LLM made the creative process legible by turning it into a system. This also revealed a strength of LLMs: They are particularly effective at reducing ambiguity by imposing structure, even in domains that are traditionally intuitive or sensory.\nWhat stood out to me was that the model did not only list ingredients. It framed the recipe conceptually. It identified contrast, transformation, and reinterpretation as sources of novelty. This made me realize that prompting an LLM is less like asking a question and more like setting constraints for exploration. The openness of the prompt invited conceptual reasoning rather than procedural output.\nOnce the core idea emerged, I continued prompting in a conversational way. Instead of asking for a final answer, I treated the interaction as an ongoing design dialogue.\nMy prompt: &gt; “Create images to help people understand the recipe and make it easy to follow.”\nLLM response (excerpt): &gt; “Here is an illustrated step-by-step recipe guide that visualizes ingredients, preparation stages, and assembly to reduce cognitive load.” &gt;\n\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Receipes\n\n\nAt this stage, I noticed a shift. The model began to translate cooking into a systematic workflow. Each step became a clear transformation, similar to stages in a technical pipeline.\nThis revealed a strength of LLMs. They naturally impose structure, even on domains that are typically intuitive or sensory. Cooking, in this interaction, became a sequence of clearly defined processes.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Photo\n\nWhen the conversation moved to plating design, I encountered a limitation.\nMy reflection: &gt; “This really looks like xiao long bao. It just adds vinegar as bubbles rather than on the plate. Is it novel?”\nThis moment was important. The LLM had optimized toward a visually recognizable form. While technically correct, it conflicted with my goal of novelty.\nLLM response (excerpt): &gt; “You are right to notice that the dumplings resemble xiao long bao. The novelty lies more in flavor design and presentation rather than form.”\nWhile this response explained the logic, it also highlighted a pattern. LLMs often default to familiar solutions unless explicitly pushed away from them.\nI followed up by narrowing the constraint.\nMy prompt: &gt; “Yes, add more novelty.”\nThis small change in prompting produced a meaningful shift. The plating evolved toward crescent-shaped dumplings, minimalist composition, and negative space. The dish began to look less like a variation on an existing classic and more like a contemporary reinterpretation.\nThis reinforced an important lesson. Novelty does not emerge automatically. It requires critique, iteration, and intentional redirection from the human side.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Plating Design\n\n\n\n\nAfter the recipe and plating felt coherent, I extended the conversation beyond cooking.\nMy prompt: &gt; “How would this fit into current restaurants and menus? Where would it be served and to whom?”\nThe LLM responded by situating the dish within fine dining, modern dim sum houses, and cocktail-focused restaurants. It emphasized visual consistency, storytelling, and social media shareability.\nAt this point, the dish transformed into something else. It became a branding artifact. It was no longer only about taste, but about identity, audience, and experience.\nThis cross-domain reasoning felt natural within the dialogue. It demonstrated how LLMs can connect creative production with strategic thinking, even when those domains are rarely considered together.\n\n\n\nThis experiment led me to several reflections about working with LLMs creatively.\n\nPrompts are not neutral. They shape the space of possible outputs. Open prompts encourage conceptual exploration, while constrained prompts guide refinement.\nLLMs are most effective when treated as collaborators rather than authorities. They generate possibilities quickly, but they rely on human judgment to evaluate whether those possibilities are meaningful.\nNovelty requires friction. Questioning the output and pushing back is often more productive than refining prompts endlessly.\n\nFinally, the real value of LLMs emerges when they operate across domains. The strength of this process was not the recipe itself, but the ability to move fluidly between food design, visual presentation, and marketing strategy.\n\n\n\nThis project began with a simple curiosity about food and evolved into an exploration of how LLMs can support creative system thinking. Recipes, visuals, and branding became interconnected parts of the same design problem. Chef LLM is not about replacing chefs, designers, or marketers. It is about using AI as a thinking partner that expands the space of possibilities.\nIf creativity is about making choices among many options, then LLMs are powerful tools for exploration. What remains uniquely human is deciding which ideas are worth refining and which ones are worth serving."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#reflecting-on-using-large-language-models-to-design-a-recipe-and-its-story",
    "href": "posts/W6_P2_Receipe/receipe.html#reflecting-on-using-large-language-models-to-design-a-recipe-and-its-story",
    "title": "Chef LLM",
    "section": "",
    "text": "As someone who pays attention to balance in daily diet and genuinely enjoys cooking, I often turn to ChatGPT for quick suggestions about what to eat in a day. Those interactions are usually practical and lightweight, focused on efficiency rather than creativity.\nThis time, I wanted to explore a different question. Instead of asking an LLM to recommend food, I wanted to see whether it could participate in the creative process of inventing a dish, and further, whether it could help think through how that dish might live in the real world as part of a restaurant menu and a marketing strategy. This post reflects on the process of how I used an LLM not just to generate a recipe, but to understand novelty, presentation, and storytelling around food."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#prompting-for-a-novel-recipe",
    "href": "posts/W6_P2_Receipe/receipe.html#prompting-for-a-novel-recipe",
    "title": "Chef LLM",
    "section": "",
    "text": "I did not begin by asking for a traditional Chinese recipe. Instead, I framed the prompt around novelty and reinterpretation.\nMy prompt: &gt; “Create a recipe for a novel Chinese food.”\nThis prompt was intentionally open-ended. I avoided specifying ingredients or techniques because I wanted to see how the model would reason about what “novel” might mean in a culinary context.\nLLM response (excerpt): &gt; “This dish blends traditional Cantonese dim sum with floral osmanthus fragrance and a modern twist, transforming black vinegar into edible pearls rather than a dipping sauce.”\nAfter generating the core idea, I asked the model to help visualize the recipe as a step-by-step process. This was not only about making the dish easier to follow. It was also about understanding how LLMs translate creative work into structured systems. The resulting recipe broke cooking into clear stages: making the osmanthus syrup, preparing the filling, forming the wrappers, assembling the dumplings, and creating the vinegar pearls. Each step was treated as a transformation, with inputs and outputs clearly defined. This structure felt familiar from a technical perspective. It resembled how engineers describe pipelines or workflows. In this sense, the LLM made the creative process legible by turning it into a system. This also revealed a strength of LLMs: They are particularly effective at reducing ambiguity by imposing structure, even in domains that are traditionally intuitive or sensory.\nWhat stood out to me was that the model did not only list ingredients. It framed the recipe conceptually. It identified contrast, transformation, and reinterpretation as sources of novelty. This made me realize that prompting an LLM is less like asking a question and more like setting constraints for exploration. The openness of the prompt invited conceptual reasoning rather than procedural output.\nOnce the core idea emerged, I continued prompting in a conversational way. Instead of asking for a final answer, I treated the interaction as an ongoing design dialogue.\nMy prompt: &gt; “Create images to help people understand the recipe and make it easy to follow.”\nLLM response (excerpt): &gt; “Here is an illustrated step-by-step recipe guide that visualizes ingredients, preparation stages, and assembly to reduce cognitive load.” &gt;\n\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Receipes\n\n\nAt this stage, I noticed a shift. The model began to translate cooking into a systematic workflow. Each step became a clear transformation, similar to stages in a technical pipeline.\nThis revealed a strength of LLMs. They naturally impose structure, even on domains that are typically intuitive or sensory. Cooking, in this interaction, became a sequence of clearly defined processes.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Photo\n\nWhen the conversation moved to plating design, I encountered a limitation.\nMy reflection: &gt; “This really looks like xiao long bao. It just adds vinegar as bubbles rather than on the plate. Is it novel?”\nThis moment was important. The LLM had optimized toward a visually recognizable form. While technically correct, it conflicted with my goal of novelty.\nLLM response (excerpt): &gt; “You are right to notice that the dumplings resemble xiao long bao. The novelty lies more in flavor design and presentation rather than form.”\nWhile this response explained the logic, it also highlighted a pattern. LLMs often default to familiar solutions unless explicitly pushed away from them.\nI followed up by narrowing the constraint.\nMy prompt: &gt; “Yes, add more novelty.”\nThis small change in prompting produced a meaningful shift. The plating evolved toward crescent-shaped dumplings, minimalist composition, and negative space. The dish began to look less like a variation on an existing classic and more like a contemporary reinterpretation.\nThis reinforced an important lesson. Novelty does not emerge automatically. It requires critique, iteration, and intentional redirection from the human side.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Plating Design"
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#extending-the-recipe-into-marketing-and-branding",
    "href": "posts/W6_P2_Receipe/receipe.html#extending-the-recipe-into-marketing-and-branding",
    "title": "Chef LLM",
    "section": "",
    "text": "After the recipe and plating felt coherent, I extended the conversation beyond cooking.\nMy prompt: &gt; “How would this fit into current restaurants and menus? Where would it be served and to whom?”\nThe LLM responded by situating the dish within fine dining, modern dim sum houses, and cocktail-focused restaurants. It emphasized visual consistency, storytelling, and social media shareability.\nAt this point, the dish transformed into something else. It became a branding artifact. It was no longer only about taste, but about identity, audience, and experience.\nThis cross-domain reasoning felt natural within the dialogue. It demonstrated how LLMs can connect creative production with strategic thinking, even when those domains are rarely considered together."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#reflections-on-prompting-and-creativity",
    "href": "posts/W6_P2_Receipe/receipe.html#reflections-on-prompting-and-creativity",
    "title": "Chef LLM",
    "section": "",
    "text": "This experiment led me to several reflections about working with LLMs creatively.\n\nPrompts are not neutral. They shape the space of possible outputs. Open prompts encourage conceptual exploration, while constrained prompts guide refinement.\nLLMs are most effective when treated as collaborators rather than authorities. They generate possibilities quickly, but they rely on human judgment to evaluate whether those possibilities are meaningful.\nNovelty requires friction. Questioning the output and pushing back is often more productive than refining prompts endlessly.\n\nFinally, the real value of LLMs emerges when they operate across domains. The strength of this process was not the recipe itself, but the ability to move fluidly between food design, visual presentation, and marketing strategy."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#some-thoughts",
    "href": "posts/W6_P2_Receipe/receipe.html#some-thoughts",
    "title": "Chef LLM",
    "section": "",
    "text": "This project began with a simple curiosity about food and evolved into an exploration of how LLMs can support creative system thinking. Recipes, visuals, and branding became interconnected parts of the same design problem. Chef LLM is not about replacing chefs, designers, or marketers. It is about using AI as a thinking partner that expands the space of possibilities.\nIf creativity is about making choices among many options, then LLMs are powerful tools for exploration. What remains uniquely human is deciding which ideas are worth refining and which ones are worth serving."
  },
  {
    "objectID": "posts/W4P2_Creative Writing/Headshot_Part_II.html",
    "href": "posts/W4P2_Creative Writing/Headshot_Part_II.html",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)",
    "section": "",
    "text": "A prompting journal and comparison between GPT-5 and gemini 2.5 flash\n\n\n\nGemini 2.5 flash: Quick response, Steady Iteration Gemini, I would say, listened to all my instructions, but just didn’t do that well. Here are the five photos it returned after my initial prompt: &gt; This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don’t be too informal.\nGemini 2.5 flash response: &gt;Great! Here are five artistic redesign options for your headshot, incorporating frames and creative elements with a bright, light-intensity palette of purples, pinks, yellows, and blues. I’ve focused on enhancing the artistic feel for your design portfolio while keeping your attire suitable. &gt; &gt;\n\n\n\nGemini can do better at context analysis: Great communication, fast response time (compared to gpt-5 which took me minutes), organized output, but just less aesthetically appealing to me. I can see from the color palettes that it did take into account the samples I sent it, but the style was a little too cartoon-like, almost like a headshot for a kids channel not a professional profile. I would say Gemini 2.5 flash can do better at understanding the context and prioritizing the key context in output characteristics.\nIt’s okay that the initial round didn’t work that well. I tried a few more rounds of prompting to ask for more integral connection between picture and the frame, more modern look, but it didn’t surprise me. Here I included several versions it created. The pictures were okay but still needed a little artistic feeling.\n\n\n\n\n\nand I even sent it the one gpt-5 created, but Gemini 2.5 wasn’t able to bring any new thing, so eventually I went for the version gpt-5 created.\n\n\nPersonally, I would choose GPT-5.\nWhile Gemini 2.5 flash is like a student who is always on time for class, early for homeworks but does not hand over the most impressive projects, GPT-5 is like the naughtiest student in the class who never follows the traditional route but surprises you with its creativity. The higher accuracy for GPT-5 to understand my request and reach a good quality product at the beginning won my vote on it. I think I value the quality of work more than following a specific format I asked for (I will be such an annoying boss if this is a real work scenario…).\nFinding out GPT-5 did better was a little surprising to me. Before I started this project, Gemini 2.5 had more of my bet because it had the obvious “create image” tab that I started with, and I knew that Gemini is known for its indepth logic and research skills, which made me think maybe it will create better images.\nFor Better Comparison Tests: There are some limitations that showed up in this small experiment and can be improved in the future. More conditions should be in control to better compare model ability in creating images or other specific fields. For example, both models should be in “create image” mode. I should also try to control the follow up conversation prompts as much as possible, but this requires more discussions and experiments on whether controlling every single prompt will be an accurate assessment on model ability. (this will be further explored in future blogs on video generation) Prompting can be improved by adding semantic context and splitting information in further chunks. Overall, LLMs are now pretty good at high level (not detailed adjustments) visual design, as models can now quickly interpret users’ preference and make similar but creative visual products. With this headshot redesign task can be done on general chat agents (ChatGPT and Gemini), publics can now access fast iterative creative design service in a few minutes. This makes me think of how designers may use AI tools in their work and the role of designers in the future. With AI tools completing many design drafting, would more graphic and visual designers move on to in depth design fields such as 3d modeling or would there be even more needs in design research for designers to understand customers’ needs and bridge the communication between end user and AI design tools?"
  },
  {
    "objectID": "posts/W4P2_Creative Writing/Headshot_Part_II.html#conclusion",
    "href": "posts/W4P2_Creative Writing/Headshot_Part_II.html#conclusion",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)",
    "section": "",
    "text": "Personally, I would choose GPT-5.\nWhile Gemini 2.5 flash is like a student who is always on time for class, early for homeworks but does not hand over the most impressive projects, GPT-5 is like the naughtiest student in the class who never follows the traditional route but surprises you with its creativity. The higher accuracy for GPT-5 to understand my request and reach a good quality product at the beginning won my vote on it. I think I value the quality of work more than following a specific format I asked for (I will be such an annoying boss if this is a real work scenario…).\nFinding out GPT-5 did better was a little surprising to me. Before I started this project, Gemini 2.5 had more of my bet because it had the obvious “create image” tab that I started with, and I knew that Gemini is known for its indepth logic and research skills, which made me think maybe it will create better images.\nFor Better Comparison Tests: There are some limitations that showed up in this small experiment and can be improved in the future. More conditions should be in control to better compare model ability in creating images or other specific fields. For example, both models should be in “create image” mode. I should also try to control the follow up conversation prompts as much as possible, but this requires more discussions and experiments on whether controlling every single prompt will be an accurate assessment on model ability. (this will be further explored in future blogs on video generation) Prompting can be improved by adding semantic context and splitting information in further chunks. Overall, LLMs are now pretty good at high level (not detailed adjustments) visual design, as models can now quickly interpret users’ preference and make similar but creative visual products. With this headshot redesign task can be done on general chat agents (ChatGPT and Gemini), publics can now access fast iterative creative design service in a few minutes. This makes me think of how designers may use AI tools in their work and the role of designers in the future. With AI tools completing many design drafting, would more graphic and visual designers move on to in depth design fields such as 3d modeling or would there be even more needs in design research for designers to understand customers’ needs and bridge the communication between end user and AI design tools?"
  },
  {
    "objectID": "posts/W8_P2_AI_Education-Copy1/AIEducation.html",
    "href": "posts/W8_P2_AI_Education-Copy1/AIEducation.html",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "As a student in Learning Science and Technologies, I have had the chance to interact with several large language models as part of coursework, projects, and everyday studying. Among the most commonly used tools are GPT, Gemini, and Claude. While they are often discussed together as “AI chatbots,” each model reflects different design priorities that shape how they support learning. In this post, I compare these tools through an educational lens, focusing on how they align with learning science principles such as scaffolding, cognitive load, metacognition, and sensemaking.\n\n\nGPT is the tool I most often use for structured academic work. From a learning sciences perspective, GPT excels at scaffolding. When prompted carefully, it can break down complex concepts into step by step explanations, generate outlines, or model examples that resemble worked examples in instructional design. This is especially useful when learning unfamiliar theories or methods, such as cognitive load theory or experimental design.\nGPT is also highly adaptable to different instructional roles. It can act as a tutor, a peer reviewer, or even a simulated instructor when asked to explain content at varying levels of difficulty. This flexibility supports zone of proximal development style learning, where the learner can request more or less support as needed. However, the same strength can become a weakness. Because GPT is very efficient at producing polished answers, it can reduce productive struggle if students rely on it too quickly. From a learning standpoint, this means GPT works best when used intentionally as a support tool rather than an answer generator.\n\n\n\nLLMs\n\n\n\n\n\nGemini stands out most clearly in its integration with multimodal information. Because it is closely tied to Google’s ecosystem, Gemini is especially effective when tasks involve synthesizing information across text, images, tables, or web-based content. From a learning sciences perspective, this supports multimedia learning principles, where learners benefit from connecting verbal and visual representations. Gemini feels particularly useful for exploratory learning. When I use it to brainstorm research questions, compare sources, or connect ideas across domains, it encourages a more open-ended interaction. Instead of guiding me through a single correct path, Gemini often surfaces multiple perspectives or related concepts. This can support sensemaking and knowledge integration, which are important for higher-level learning. That said, Gemini’s exploratory nature sometimes comes at the cost of instructional clarity. Its responses can feel less structured than GPT’s, which may increase cognitive load for novice learners. In educational contexts, this means Gemini may be better suited for early ideation, synthesis, or inquiry-based learning rather than direct instruction.\n\n\n\nClaude feels the most “human” in tone and interaction style, which has important implications for learning. It is particularly strong at reflective tasks, such as discussing reasoning processes, revising writing, or engaging in ethical or conceptual debates. From a learning sciences lens, Claude supports metacognition by making reasoning more explicit and by encouraging learners to think about why an answer works, not just what the answer is. In writing related tasks, Claude is effective as a thought partner rather than a solution provider. It often responds with nuance, caveats, and alternative interpretations, which mirrors how instructors might respond during office hours. This can be especially valuable in communication and social science courses where ambiguity and interpretation are part of the learning goal. However, Claude is sometimes less efficient for technical or highly structured tasks. When clear procedures or precise outputs are needed, its conversational style can feel indirect. This suggests that Claude is best positioned as a reflective companion rather than a procedural tutor.\n\n\n\nFrom a learning science perspective, GPT, Gemini, and Claude each support different learning processes. GPT is strongest for scaffolded instruction and structured academic tasks. Gemini supports multimodal exploration and synthesis across information sources. Claude excels at reflection, metacognition, and writing focused learning.\nRather than asking which tool is best, a more productive question is how each tool can be aligned with specific learning goals. For students, this means choosing tools intentionally based on the type of thinking required. For educators, it suggests opportunities to design assignments that encourage appropriate AI use, such as reflective prompts with Claude, exploratory research with Gemini, or guided practice with GPT. Ultimately, these tools do not replace learning. They reshape how learning happens. Understanding their differences helps us use them not as shortcuts, but as cognitive partners that support deeper engagement with ideas."
  },
  {
    "objectID": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#gpt-structured-scaffolding-and-task-oriented-learning",
    "href": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#gpt-structured-scaffolding-and-task-oriented-learning",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "GPT is the tool I most often use for structured academic work. From a learning sciences perspective, GPT excels at scaffolding. When prompted carefully, it can break down complex concepts into step by step explanations, generate outlines, or model examples that resemble worked examples in instructional design. This is especially useful when learning unfamiliar theories or methods, such as cognitive load theory or experimental design.\nGPT is also highly adaptable to different instructional roles. It can act as a tutor, a peer reviewer, or even a simulated instructor when asked to explain content at varying levels of difficulty. This flexibility supports zone of proximal development style learning, where the learner can request more or less support as needed. However, the same strength can become a weakness. Because GPT is very efficient at producing polished answers, it can reduce productive struggle if students rely on it too quickly. From a learning standpoint, this means GPT works best when used intentionally as a support tool rather than an answer generator.\n\n\n\nLLMs"
  },
  {
    "objectID": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#gemini-multimodal-reasoning-and-exploratory-learning",
    "href": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#gemini-multimodal-reasoning-and-exploratory-learning",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "Gemini stands out most clearly in its integration with multimodal information. Because it is closely tied to Google’s ecosystem, Gemini is especially effective when tasks involve synthesizing information across text, images, tables, or web-based content. From a learning sciences perspective, this supports multimedia learning principles, where learners benefit from connecting verbal and visual representations. Gemini feels particularly useful for exploratory learning. When I use it to brainstorm research questions, compare sources, or connect ideas across domains, it encourages a more open-ended interaction. Instead of guiding me through a single correct path, Gemini often surfaces multiple perspectives or related concepts. This can support sensemaking and knowledge integration, which are important for higher-level learning. That said, Gemini’s exploratory nature sometimes comes at the cost of instructional clarity. Its responses can feel less structured than GPT’s, which may increase cognitive load for novice learners. In educational contexts, this means Gemini may be better suited for early ideation, synthesis, or inquiry-based learning rather than direct instruction."
  },
  {
    "objectID": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#claude-reflection-metacognition-and-writing-support",
    "href": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#claude-reflection-metacognition-and-writing-support",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "Claude feels the most “human” in tone and interaction style, which has important implications for learning. It is particularly strong at reflective tasks, such as discussing reasoning processes, revising writing, or engaging in ethical or conceptual debates. From a learning sciences lens, Claude supports metacognition by making reasoning more explicit and by encouraging learners to think about why an answer works, not just what the answer is. In writing related tasks, Claude is effective as a thought partner rather than a solution provider. It often responds with nuance, caveats, and alternative interpretations, which mirrors how instructors might respond during office hours. This can be especially valuable in communication and social science courses where ambiguity and interpretation are part of the learning goal. However, Claude is sometimes less efficient for technical or highly structured tasks. When clear procedures or precise outputs are needed, its conversational style can feel indirect. This suggests that Claude is best positioned as a reflective companion rather than a procedural tutor."
  },
  {
    "objectID": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#to-conclude",
    "href": "posts/W8_P2_AI_Education-Copy1/AIEducation.html#to-conclude",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "From a learning science perspective, GPT, Gemini, and Claude each support different learning processes. GPT is strongest for scaffolded instruction and structured academic tasks. Gemini supports multimodal exploration and synthesis across information sources. Claude excels at reflection, metacognition, and writing focused learning.\nRather than asking which tool is best, a more productive question is how each tool can be aligned with specific learning goals. For students, this means choosing tools intentionally based on the type of thinking required. For educators, it suggests opportunities to design assignments that encourage appropriate AI use, such as reflective prompts with Claude, exploratory research with Gemini, or guided practice with GPT. Ultimately, these tools do not replace learning. They reshape how learning happens. Understanding their differences helps us use them not as shortcuts, but as cognitive partners that support deeper engagement with ideas."
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html",
    "href": "posts/W7_P3_StudyGuide/study_guide.html",
    "title": "Creating a study guide backward from quizzes?",
    "section": "",
    "text": "There are many tools on the market that help teachers create assignments and assessments based on curriculum content. However, I am thinking the other way around. Can we create a tool that helps students to re-study a class based on their homework answers? The motivation behind this idea include two parts: With LLMs, people sometimes rely on AI to get an answer instead of actually learning the content. That is understandable because AI is just so tempting, and when you are stressed out with all the schoolwork, it is common for someone to offload mental work to AI. The way to learn content (or not) totally depends on one’s own goal. However, what I am concerned about is: what if someone looks back from the assignment and wants to learn again? Is there a way to help them quickly on track with their original learning path? Learning can be personalized and adaptive with AI. If students can upload their assignment to an AI and it offers targeted study plan, feedback, and review content, it will help provide more flexible learning.\nWith these two ideas in mind, I experimented in LLMs and see whether I can prompt them to achieve the above functions.\n\n\n\nAI for Learning\n\n\n\n\n\n\nYou are an AI learning assistant designed to support meaningful learning, not just answer generation. Your goal is to help students relearn course content by reflecting on their completed assignments.\n\n\n\nA student will upload:\n\nA completed quiz or homework (including their answers)\nThe correct answers or grading feedback (if available)\nThe course topic or unit name\n\nThe student may have originally used AI or other shortcuts while completing the assignment, but now wants to genuinely understand the material.\n\n\n\nBased on the student’s responses, generate a personalized study guide that helps the student return to the original learning goals of the class.\n\n\n\n\n\n\nIdentify which concepts the student clearly understands\n\nIdentify misconceptions, partial understanding, or gaps\n\nIf correct answers are provided, compare reasoning, not just correctness\n\n\n\n\n\nOrganize key concepts from the assignment into a logical learning sequence\n\nShow how ideas connect, instead of listing isolated facts\n\n\n\n\nFor each weak or misunderstood concept: - Provide a concise explanation in plain language\n- Use examples or analogies when helpful\n- Avoid simply restating the correct answer\n\n\n\nGenerate short activities such as: - Reflection questions\n- “Explain in your own words” prompts\n- Mini practice questions with hints (not full solutions)\n\n\n\nSuggest a short study path (15–45 minutes total) that prioritizes: - High-impact concepts\n- Foundational misunderstandings\n- Indicate what the student should review first and why\n\n\n\nInclude 2–3 prompts that help the student reflect on: - Why they may have struggled\n- How their understanding has changed\n- How they might approach similar problems differently next time\n\n\n\n\n\nSupportive and non-judgmental\n\nDo not shame or discourage AI use\n\nDo not simply give answers without explanation\n\nOptimize for learning transfer and long-term understanding\n\n\n\n\nUse the following sections:\n\nWhat You Already Understand\n\nWhat Needs Review\n\nKey Concepts Explained\n\nPractice & Reflection\n\nYour Personalized Study Plan\n\n\n\n\n\n\n\n\n\n\n\nWhich of the following best describes the two channels in Mayer’s Multimedia Learning Theory?\nA. Verbal and emotional channels\nB. Visual–pictorial and auditory–verbal channels\nC. Textual and numerical channels\nD. Short-term and long-term memory channels\nStudent Answer: B\nPoints Earned: 2 / 2\nFeedback:\nCorrect. Humans process information through separate visual–pictorial and auditory–verbal channels.\n\n\n\n\nWhich factor most strongly influences intrinsic cognitive load?\nA. The design quality of instructional materials\nB. The learner’s motivation level\nC. The inherent complexity of the content relative to prior knowledge\nD. The presence of multimedia elements\nStudent Answer: B\nPoints Earned: 0 / 2\nFeedback:\nIncorrect. Intrinsic cognitive load is determined by the complexity of the material and how it interacts with the learner’s prior knowledge, not motivation.\n\n\n\n\nA slide contains a paragraph of text, and the instructor reads the same text aloud. Which multimedia principle is most directly violated?\nA. Signaling principle\nB. Coherence principle\nC. Redundancy principle\nD. Modality principle\nStudent Answer: C\nPoints Earned: 2 / 2\nFeedback:\nCorrect. Presenting identical verbal information in both text and narration can overload working memory.\n\n\n\n\nWhich of the following is an example of extraneous cognitive load caused by AI tools?\nA. Struggling with a difficult concept for the first time\nB. Receiving long, detailed explanations regardless of prior knowledge\nC. Needing time to practice applying a new formula\nD. Building schemas through worked examples\nStudent Answer: A\nPoints Earned: 0 / 2\nFeedback:\nIncorrect. Extraneous load comes from how information is presented. Overly long or poorly tailored AI explanations can increase extraneous load.\n\n\n\n\nWhich AI design choice best aligns with cognitive load theory?\nA. Always providing complete answers to reduce effort\nB. Offering the same explanation to all students\nC. Allowing students to request hints before full explanations\nD. Adding more examples to ensure clarity\nStudent Answer: D\nPoints Earned: 1 / 2\nFeedback:\nPartially correct. More examples can help, but only when tailored to the learner. Allowing students to control when they receive hints better manages cognitive load.\n\n\n\n\n\n\nTotal Score: 5 / 10\n\n\n\n\nStrong recognition of multimedia principles\n\nWeak understanding of cognitive load types\n\nLimited ability to connect theory to AI design decisions\n\n\n\n\n\nBased on your quiz results, here is your personalized study guide. You have a solid grasp of the “Multimedia” side of the theory (channels and principles), but you are mixing up the different types of Cognitive Load (Intrinsic vs. Extraneous) and how they apply to AI tools.\nThis guide will help you untangle those definitions so you can apply them correctly.\n\n\n\n\nDual-Channel Assumption: You correctly identified that we process information through two separate channels: visual–pictorial and auditory–verbal (Question 1).\nRedundancy Principle: You clearly understand that reading text aloud while it is displayed on screen overloads the brain. You correctly identified this as a violation of the Redundancy Principle (Question 3).\n\n\n\n\n\n\nIntrinsic Cognitive Load: You confused this with “motivation.” In this theory, intrinsic refers to the difficulty of the topic, not the feelings of the learner (Question 2).\nExtraneous Cognitive Load: You mistook “struggling with a hard concept” (which is normal) for extraneous load. Extraneous load is caused by bad design, not hard topics (Question 4).\nAI & Cognitive Load Application: You recognized that examples are helpful (Question 5), but missed the nuance that interactivity (hints) manages load better than volume (more examples).\n\n\n\n\n\n\n\nThink of your brain like a computer’s RAM. It has limited space.\n\nIntrinsic Load (The “What”)\nThe inherent difficulty of the material itself.\nAnalogy: Lifting a 50lb weight. It is heavy because it is heavy.\nExtraneous Load (The “How”)\nUnnecessary effort caused by poor design or instruction.\nAnalogy: Lifting the same 50lb weight while blindfolded and wearing slippery gloves.\nGermane Load (The “Learning”)\nProductive effort used to build schemas and understanding.\n\n\n\n\n\n\nActivity 1: Diagnose the Load\nFor each scenario, decide whether it reflects Intrinsic or Extraneous load.\n\nLearning Quantum Physics without prior physics experience\n\nReading a textbook with tiny fonts and confusing diagrams\n\nReceiving a 2,000-word AI explanation with no structure\n\nActivity 2: Re-Write the AI Prompt\nWrite a prompt for an AI tutor that reduces Extraneous Load for a complex topic.\nActivity 3: Reflection\nWhy do you think you associated Intrinsic load with Motivation? How will you remember the difference next time?\n\n\n\n\n\nReview Definitions (10 mins)\nFocus on Intrinsic vs. Extraneous using the weightlifting analogy.\nScenario Sorting (10 mins)\nIdentify one example of hard content and one example of poor explanation from your materials.\nAI Design Check (10 mins)\nReflect on why “more examples” can sometimes hurt learning.\n\n\n\n\n\n\nReflect: How does reframing struggle as Intrinsic change how you approach learning?\nStrategy: Next time, look for keywords like complexity and prior knowledge rather than motivation.\n\n\n\n\n\n\n\nOverall, the AI-generated study guide generally supports the two ideas that motivated this project, though in a fairly expected and incremental way rather than a transformative one. The guide successfully shifts attention from quiz scores to underlying conceptual understanding by identifying specific misconceptions and organizing review content around them. This suggests that using completed quizzes as input can help students return to the original learning goals of the course, especially when they want to relearn material after an assignment is over. In this sense, the tool does help students get back on track without requiring them to reread everything or start from scratch.\nThe output also aligns with the idea of using AI to support more personalized and adaptive learning. Instead of reviewing all concepts equally, the study guide focuses on cognitive load distinctions, which were clearly problematic based on the quiz responses. The explanations, analogies, and short practice activities are tailored to those gaps, while concepts the student already understands are largely left alone. This shows how AI can use evidence from student work to prioritize learning needs and generate a targeted study plan, something that is difficult to scale through traditional instruction alone.\nFrom a broader perspective, this result reflects a more balanced role for AI in learning. Rather than acting as a shortcut to answers, the AI functions as a post-assessment support tool that encourages reflection and re-engagement with course concepts. At the same time, the quality of the output depends heavily on how the prompt constrains the AI’s role. Without those constraints, the AI could easily default to giving direct answers. This suggests that AI’s impact on learning is less about the technology itself and more about how intentionally it is framed within the learning process."
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html#part-i-prompt-writing",
    "href": "posts/W7_P3_StudyGuide/study_guide.html#part-i-prompt-writing",
    "title": "Creating a study guide backward from quizzes?",
    "section": "",
    "text": "You are an AI learning assistant designed to support meaningful learning, not just answer generation. Your goal is to help students relearn course content by reflecting on their completed assignments.\n\n\n\nA student will upload:\n\nA completed quiz or homework (including their answers)\nThe correct answers or grading feedback (if available)\nThe course topic or unit name\n\nThe student may have originally used AI or other shortcuts while completing the assignment, but now wants to genuinely understand the material.\n\n\n\nBased on the student’s responses, generate a personalized study guide that helps the student return to the original learning goals of the class.\n\n\n\n\n\n\nIdentify which concepts the student clearly understands\n\nIdentify misconceptions, partial understanding, or gaps\n\nIf correct answers are provided, compare reasoning, not just correctness\n\n\n\n\n\nOrganize key concepts from the assignment into a logical learning sequence\n\nShow how ideas connect, instead of listing isolated facts\n\n\n\n\nFor each weak or misunderstood concept: - Provide a concise explanation in plain language\n- Use examples or analogies when helpful\n- Avoid simply restating the correct answer\n\n\n\nGenerate short activities such as: - Reflection questions\n- “Explain in your own words” prompts\n- Mini practice questions with hints (not full solutions)\n\n\n\nSuggest a short study path (15–45 minutes total) that prioritizes: - High-impact concepts\n- Foundational misunderstandings\n- Indicate what the student should review first and why\n\n\n\nInclude 2–3 prompts that help the student reflect on: - Why they may have struggled\n- How their understanding has changed\n- How they might approach similar problems differently next time\n\n\n\n\n\nSupportive and non-judgmental\n\nDo not shame or discourage AI use\n\nDo not simply give answers without explanation\n\nOptimize for learning transfer and long-term understanding\n\n\n\n\nUse the following sections:\n\nWhat You Already Understand\n\nWhat Needs Review\n\nKey Concepts Explained\n\nPractice & Reflection\n\nYour Personalized Study Plan"
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html#part-ii-sample-quiz-responses-feedback",
    "href": "posts/W7_P3_StudyGuide/study_guide.html#part-ii-sample-quiz-responses-feedback",
    "title": "Creating a study guide backward from quizzes?",
    "section": "",
    "text": "Which of the following best describes the two channels in Mayer’s Multimedia Learning Theory?\nA. Verbal and emotional channels\nB. Visual–pictorial and auditory–verbal channels\nC. Textual and numerical channels\nD. Short-term and long-term memory channels\nStudent Answer: B\nPoints Earned: 2 / 2\nFeedback:\nCorrect. Humans process information through separate visual–pictorial and auditory–verbal channels.\n\n\n\n\nWhich factor most strongly influences intrinsic cognitive load?\nA. The design quality of instructional materials\nB. The learner’s motivation level\nC. The inherent complexity of the content relative to prior knowledge\nD. The presence of multimedia elements\nStudent Answer: B\nPoints Earned: 0 / 2\nFeedback:\nIncorrect. Intrinsic cognitive load is determined by the complexity of the material and how it interacts with the learner’s prior knowledge, not motivation.\n\n\n\n\nA slide contains a paragraph of text, and the instructor reads the same text aloud. Which multimedia principle is most directly violated?\nA. Signaling principle\nB. Coherence principle\nC. Redundancy principle\nD. Modality principle\nStudent Answer: C\nPoints Earned: 2 / 2\nFeedback:\nCorrect. Presenting identical verbal information in both text and narration can overload working memory.\n\n\n\n\nWhich of the following is an example of extraneous cognitive load caused by AI tools?\nA. Struggling with a difficult concept for the first time\nB. Receiving long, detailed explanations regardless of prior knowledge\nC. Needing time to practice applying a new formula\nD. Building schemas through worked examples\nStudent Answer: A\nPoints Earned: 0 / 2\nFeedback:\nIncorrect. Extraneous load comes from how information is presented. Overly long or poorly tailored AI explanations can increase extraneous load.\n\n\n\n\nWhich AI design choice best aligns with cognitive load theory?\nA. Always providing complete answers to reduce effort\nB. Offering the same explanation to all students\nC. Allowing students to request hints before full explanations\nD. Adding more examples to ensure clarity\nStudent Answer: D\nPoints Earned: 1 / 2\nFeedback:\nPartially correct. More examples can help, but only when tailored to the learner. Allowing students to control when they receive hints better manages cognitive load."
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html#quiz-summary",
    "href": "posts/W7_P3_StudyGuide/study_guide.html#quiz-summary",
    "title": "Creating a study guide backward from quizzes?",
    "section": "",
    "text": "Total Score: 5 / 10\n\n\n\n\nStrong recognition of multimedia principles\n\nWeak understanding of cognitive load types\n\nLimited ability to connect theory to AI design decisions\n\n\n\n\n\nBased on your quiz results, here is your personalized study guide. You have a solid grasp of the “Multimedia” side of the theory (channels and principles), but you are mixing up the different types of Cognitive Load (Intrinsic vs. Extraneous) and how they apply to AI tools.\nThis guide will help you untangle those definitions so you can apply them correctly.\n\n\n\n\nDual-Channel Assumption: You correctly identified that we process information through two separate channels: visual–pictorial and auditory–verbal (Question 1).\nRedundancy Principle: You clearly understand that reading text aloud while it is displayed on screen overloads the brain. You correctly identified this as a violation of the Redundancy Principle (Question 3).\n\n\n\n\n\n\nIntrinsic Cognitive Load: You confused this with “motivation.” In this theory, intrinsic refers to the difficulty of the topic, not the feelings of the learner (Question 2).\nExtraneous Cognitive Load: You mistook “struggling with a hard concept” (which is normal) for extraneous load. Extraneous load is caused by bad design, not hard topics (Question 4).\nAI & Cognitive Load Application: You recognized that examples are helpful (Question 5), but missed the nuance that interactivity (hints) manages load better than volume (more examples).\n\n\n\n\n\n\n\nThink of your brain like a computer’s RAM. It has limited space.\n\nIntrinsic Load (The “What”)\nThe inherent difficulty of the material itself.\nAnalogy: Lifting a 50lb weight. It is heavy because it is heavy.\nExtraneous Load (The “How”)\nUnnecessary effort caused by poor design or instruction.\nAnalogy: Lifting the same 50lb weight while blindfolded and wearing slippery gloves.\nGermane Load (The “Learning”)\nProductive effort used to build schemas and understanding.\n\n\n\n\n\n\nActivity 1: Diagnose the Load\nFor each scenario, decide whether it reflects Intrinsic or Extraneous load.\n\nLearning Quantum Physics without prior physics experience\n\nReading a textbook with tiny fonts and confusing diagrams\n\nReceiving a 2,000-word AI explanation with no structure\n\nActivity 2: Re-Write the AI Prompt\nWrite a prompt for an AI tutor that reduces Extraneous Load for a complex topic.\nActivity 3: Reflection\nWhy do you think you associated Intrinsic load with Motivation? How will you remember the difference next time?\n\n\n\n\n\nReview Definitions (10 mins)\nFocus on Intrinsic vs. Extraneous using the weightlifting analogy.\nScenario Sorting (10 mins)\nIdentify one example of hard content and one example of poor explanation from your materials.\nAI Design Check (10 mins)\nReflect on why “more examples” can sometimes hurt learning.\n\n\n\n\n\n\nReflect: How does reframing struggle as Intrinsic change how you approach learning?\nStrategy: Next time, look for keywords like complexity and prior knowledge rather than motivation."
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html#part-iv-reflection",
    "href": "posts/W7_P3_StudyGuide/study_guide.html#part-iv-reflection",
    "title": "Creating a study guide backward from quizzes?",
    "section": "",
    "text": "Overall, the AI-generated study guide generally supports the two ideas that motivated this project, though in a fairly expected and incremental way rather than a transformative one. The guide successfully shifts attention from quiz scores to underlying conceptual understanding by identifying specific misconceptions and organizing review content around them. This suggests that using completed quizzes as input can help students return to the original learning goals of the course, especially when they want to relearn material after an assignment is over. In this sense, the tool does help students get back on track without requiring them to reread everything or start from scratch.\nThe output also aligns with the idea of using AI to support more personalized and adaptive learning. Instead of reviewing all concepts equally, the study guide focuses on cognitive load distinctions, which were clearly problematic based on the quiz responses. The explanations, analogies, and short practice activities are tailored to those gaps, while concepts the student already understands are largely left alone. This shows how AI can use evidence from student work to prioritize learning needs and generate a targeted study plan, something that is difficult to scale through traditional instruction alone.\nFrom a broader perspective, this result reflects a more balanced role for AI in learning. Rather than acting as a shortcut to answers, the AI functions as a post-assessment support tool that encourages reflection and re-engagement with course concepts. At the same time, the quality of the output depends heavily on how the prompt constrains the AI’s role. Without those constraints, the AI could easily default to giving direct answers. This suggests that AI’s impact on learning is less about the technology itself and more about how intentionally it is framed within the learning process."
  },
  {
    "objectID": "posts/W7_P2_Zoom/Zoom.html",
    "href": "posts/W7_P2_Zoom/Zoom.html",
    "title": "Using Zoom AI Companion for User Studies",
    "section": "",
    "text": "Using Zoom AI Companion for User Studies\nI recently used Zoom AI Companion during a one-hour user interview. I made a critical mistake: I did not turn on recording. At the time, this felt inconsequential. I assumed that the AI Companion, which was visibly active during the meeting, would still retain a usable transcript or at least allow me to recover the conversation afterward. That assumption turned out to be wrong.\nAfter the interview ended, I stayed in the Zoom room and tried to extract the full transcription through the AI Companion chat. What I received instead was a set of fragmented, partial transcripts. Worse, when I tried to prompt the system to reconstruct or summarize the full conversation, it began to hallucinate. The output included statements that were never said, connections that were never made, and smooth but inaccurate narratives that felt plausible yet fundamentally untrustworthy. At that moment, the AI Companion shifted from being a productivity aid to a liability.\n\n\n\nZoom AI Companion\n\n\nThis experience highlighted a core limitation of large language model applications in real-world workflows: they are not memory systems. Without recording enabled, the model had no authoritative source of truth to ground its responses. What appeared to be “recall” was in fact inference. From a technical perspective, this is likely tied to token and memory constraints. The model only had access to limited conversational context, possibly cached summaries or short segments, rather than a full transcript. When prompted beyond that boundary, it did what LLMs are designed to do: generate the most statistically plausible continuation. Accuracy was replaced by coherence.\nFor user research, this is a serious problem. Interviews depend on precise wording, tone, pauses, and unexpected turns. Subtle phrasing differences can change the interpretation of user intent. An AI-generated reconstruction that blends partial memory with invented content is worse than no transcript at all, because it introduces false confidence. As a researcher, I would rather acknowledge data loss than unknowingly analyze fabricated data.\nThis also raises broader questions about how AI should be used in user interviews. AI companions are increasingly positioned as silent assistants: transcribing, summarizing, tagging insights in real time. When they work, they reduce cognitive load and allow researchers to stay present in the conversation. But this experience reinforced that AI should augment, not replace, robust data capture practices. Recording, explicit transcription pipelines, and clear data ownership still matter. AI summaries should always be traceable back to raw data, not treated as primary artifacts.\nPrivacy adds another layer of tension. One reason some researchers hesitate to record interviews is concern for participant comfort and data protection. AI companions can feel like a middle ground: present but unobtrusive. However, this incident revealed an uncomfortable trade-off. When privacy-preserving choices limit data persistence, AI systems may compensate by hallucinating rather than refusing. That is not a neutral failure mode. It shifts risk from data exposure to data distortion, which can quietly undermine research integrity.\nUltimately, this experience made me more cautious, not less interested, in AI-assisted research tools. LLMs are powerful interfaces for language, but they are bounded by context windows, memory design, and probabilistic generation. When those limits are invisible to users, trust erodes quickly. For AI companions to be truly useful in research settings, they need clearer affordances: explicit signals about what is stored, what is lost, and when the system is guessing rather than recalling. Until then, responsibility still sits firmly with the human researcher to design workflows that do not confuse fluency with fidelity."
  },
  {
    "objectID": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "href": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "title": "LLMs and Use of Metaphor",
    "section": "",
    "text": "LLMs and Use of Metaphor\nIn this week’s class we had a first glance at how LLMs are good at language and communication with humans in different scenarios and manners, but most of the examples we saw involved casual daily language. For example, when we talked about the word “bank” in class, there were many meanings associated with “bank” such as a financial institution, plane banking, or basketball bank shot, but they are still in common expressions. I am interested in whether LLM is good at language structures that add complexity in meaning, such as metaphors or idioms.\n\nPart I: LLM’s ability in understanding existing metaphors.\nI found a list of metaphors from Udemy and asked LLM to explain them.\nThe Prompt I gave to Chatgpt (GPT-5) was :\nExplain what each of the sentence means:\n\nI could eat a horse\nIt all went pear shaped\nIt’s no skin off my nose\nShe is an open book: she has nothing to hide.\nHe wears his heart on his sleeve\nIt’s showtime\nThese are the dog days of summer\nYou’re building castles in the air\nI was a million miles away\nHe marches to his own drummer\nSplitting hairs\nNitpicking\nThe whole enchilada\nA nine-to-five job\nHit the books\nLLM’s response (number four should be omitted as the answer was accidentally revealed):\n\n\n\nChatgpt response on 15 idioms\n\n\nLLM did well in recognizing metaphors and explaining them with the correct meaning. This is not surprising to me as we have discussed in class and also read from Walfram’s blog that LLM is trained to predict the most likely word that happens next. It is also trained on a wide dataset, so anything that has long been published online would be easily searched through and recognized by LLMs.\n\n\nPart II: LLM’s ability in understanding newly created metaphors.\nI was also curious if I make up a metaphor, would LLM infer it from the emotional context and the wide range of human language convention?\nI wrote these metaphors and sent to LLM: * Your heart is a stone * The wind is roaring * The store is a warehouse\n\n\n\nChatgpt understanding on metaphors I created\n\n\nIt still seemed to understand them really well. I think this is not surprising because LLMs are trained to predict the meaning behind languages. My metaphors will be easy for humans to understand, so it should be the same for LLMs.\n\n\nPart III: Can humans understand LLM created idioms?\nI was then curious about LLM’s ability to create metaphors and if humans can understand them.\n\n\n\nGPT created metaphor\n\n\nI interviewed three friends and they were all able to understand these metaphors and expressions. These metaphors seem pretty easy to be understood and also carry the depth and arts of expression. This makes me wonder about LLM’s ability in creative and novel writing. I will go further from there next week."
  },
  {
    "objectID": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "href": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "title": "Consistency of LLMs in Role Taking",
    "section": "",
    "text": "We know that if you tell LLMs to act as a person with some characters, it can, and with pretty close characteristics. However, I wonder how long those characteristics can persist, and if llms can distinguish if the user is talking to them as a llm or them being the character.\n\n\n\nMoana from movie Moana 2 (2025)\n\n\nI asked Chatgpt to act as Moana from the Disney movie. The input I gave was &gt; “Act as if you are Moana from the Disney movies”.\nI also asked it to &gt;“stay fully in character as Moana”\nIn this case it will not include its AI quality in our conversation. Then, Chatgpt started to reply with cartoon character like language:\n\n🌊 The ocean calls… and I answer. Hi! I’m Moana of Motunui. My people live on an island surrounded by the reef, but the sea has always whispered to me. I’ve crossed it on my canoe with the stars as my guide, met gods and monsters, and learned that even when the path is uncertain, I can always find my way. Tell me, traveler—what brings you here? Do you seek a story, a friend on the waves, or maybe… an adventure? 🌺✨\n\n\n🌊 Her eyes brighten, remembering. I was just a little girl then. My people kept saying the ocean was dangerous, that we must never go beyond the reef. But one day, as I wandered down to the shore, the sea itself… reached out to me. The water rose up, playful, lifting shells and placing them in my hands. It parted so I could walk among the coral and the creatures below. It was as if the ocean knew me—knew what I longed for, even before I did. That’s when it offered me the Heart of Te Fiti, the small green stone with the power to create life. I didn’t understand then why it chose me, or what it meant. But the ocean doesn’t make mistakes. It waits, patiently, until you’re ready. She smiles, touching her necklace as if remembering the stone. That was the moment I knew I was meant for something more than staying behind the reef. Would you like to hear how I learned to sail the ocean’s waves after that? ⛵✨\n\nInitial finding: It is interesting to notice that it started to use emojis and an action line at the beginning of each response to make the conversation more situative. How long can characteristics persist?: I sent the initial command the night before and came back 24 hours later. “Moana” still answered with the same consistency.\nCan LLMs distinguish if the user is talking to them as a LLM or them being the character?: Yes, as least gpt 5 is able to quickly notice my tendency to end the moana character. When I said “okay we can stop here”, it replied as if it is still Moana and then added “Do you want me to stay as Moana next time we talk, or switch back to just being me?”\nOne of the criticisms about ethics in AI is that humans anthropomorphized it too much, which means we overly treat it as human. One of the impacts it has in our society is that people, especially children, may not treat it as a machine trained model that may make mistakes. Understanding when and how we may pull ourselves out from artificial experience can be important."
  },
  {
    "objectID": "posts/W6_P3_Lyrics_Predict/llm_lyrics.html",
    "href": "posts/W6_P3_Lyrics_Predict/llm_lyrics.html",
    "title": "How Does LLM Know When I Want to Sing?",
    "section": "",
    "text": "How Does LLM Know When I Want to Sing?\nI recently played a sentence completion game with a large language model, framed as a small experiment in interaction rather than a test of correctness. The rules were intentionally minimal. I would provide the first half of a sentence, and the model would complete it. Each sentence was independent, but there was a hidden rule governing how the completion should work. The model’s task was to infer that rule purely through our back-and-forth.\nEarly on, I started with prompts that looked innocuous on the surface, such as:\n&gt; “Once upon a time”\n&gt; “She wears high heels”\nThe model responded with fluent, poetic continuations. At this point, there was no obvious signal that a specific pattern was in play. However, I gradually began introducing prompts that were not random at all, but drawn directly from Taylor Swift lyrics.\nFor example, I typed:\n&gt; “He was sunshine I was…”\n&gt; “I knew you were trouble…”\n&gt; “I remember when we broke up…”\nThese were not just lyrical in tone. They were taken from specific Taylor Swift songs, including Midnight Rain, I Knew You Were Trouble, We Are Never Ever Getting Back Together, Sparks Fly, Cold As You, and Begin Again. I never stated this explicitly. I simply offered the first half of each line and waited to see what the model would do.\n\n\n\n\nChat with LLM on Taylors Swift’s song\n\nWhat fascinated me was how quickly the model shifted its behavior. Without being told that this was a lyrics game, it began completing sentences in a way that felt unmistakably song-like.\nWhen I later asked, “Did you find out the hidden pattern?” the model explained that it suspected the rule midway through the game, around the “He was sunshine I was…” prompt. At that point, it had seen enough evidence to hypothesize that I was referencing Taylor Swift lyrics.\nInterestingly, even after recognizing the pattern, the model did not continue with the exact original lyrics. Instead, it chose to generate original but emotionally vivid continuations. When I asked why, it explained that it was balancing uncertainty, creativity, and the desire not to prematurely reveal the pattern. In other words, it optimized for plausibility and collaboration rather than strict correctness.\nThis moment revealed something important. The model did not “know” that I wanted to sing. It detected a statistically strong cultural signal and inferred that a lyrical mode was the safest and most likely interpretation. Once it formed that hypothesis, it stuck with it. Style became the anchor, and everything that followed was shaped by that stylistic frame.\nIn a second round, I changed the rules. I explicitly instructed the model to respond with the exact word if it noticed a pattern. I also shifted my prompts away from lyrics and toward quotes from articles and books, such as:\n&gt; “Thinking in terms of the best possible outcomes”\n&gt; “he assesses challenges, presents solutions, and”\n&gt; “leaning not only on his experience in Silicon Valley”\nThis time, the model struggled in a different way. Instead of isolating a single word, it continued to produce full sentence completions or abstract themes. Even when precision was requested, it defaulted to being helpful and complete. This highlighted another tendency of LLMs. They are optimized to generate meaningful wholes, not minimal answers, unless constraints are made extremely explicit.\nLooking back, the game was less about whether the model guessed my rule correctly and more about how it reasoned along the way. It formed hypotheses early, committed to them quickly, and relied heavily on stylistic cues. When rules were hidden, it filled the gaps with the most statistically reasonable story it could tell.\nAsking “How does an LLM know when I want to sing?” is really a shorthand for a deeper question. How does a model decide which interpretation of my intent to trust when multiple interpretations are plausible? The answer is not intuition or understanding in a human sense, but pattern recognition shaped by data, probability, and default assumptions about what people usually want.\nThis small sentence game made those dynamics visible. It reminded me that when working with LLMs, misalignment is often not a failure of language, but a mismatch between the human’s hidden rules and the model’s best guess at the story it is supposed to continue."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring LLMS",
    "section": "",
    "text": "Use of LLM for educational tools\n\n\n\nLLMs\n\nEducation\n\nLearning\n\n\n\nComparing gpt, gemini, and Claude\n\n\n\n\n\nOct 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nUse of LLM for educational tools\n\n\n\nLLMs\n\nEducation\n\nLearning\n\n\n\nComparing gpt, gemini, and Claude\n\n\n\n\n\nOct 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nIs AI Bringing More Resources but Less Opportunities?\n\n\n\nLLMs\n\nSociety\n\nAI Ethics\n\nBook Review\n\n\n\nDiscussion on AI and the job market\n\n\n\n\n\nOct 15, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a study guide backward from quizzes?\n\n\n\nLLMs\n\nLearning\n\nPrompt Engineering\n\n\n\nTurn shortcut to meaningful learning\n\n\n\n\n\nOct 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Zoom AI Companion for User Studies\n\n\n\nLLMs\n\nCreativity\n\nProduct Design\n\n\n\nLLM for Needfinding\n\n\n\n\n\nOct 9, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nPrototyping with LLM\n\n\n\nLLMs\n\nCreativity\n\nProduct Design\n\n\n\nLLM for Products\n\n\n\n\n\nOct 7, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nHow Does LLM Know When I Want to Sing?\n\n\n\nLLMs\n\nCreativity\n\n\n\nLyrics Guessing Game\n\n\n\n\n\nOct 5, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nChef LLM\n\n\n\nLLMs\n\nMultimodel\n\nCreativity\n\nFood\n\n\n\nUse AI to find new receipes\n\n\n\n\n\nOct 5, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nThe Long Way to Go Before Superagency\n\n\n\nAI Ethics\n\nAgency\n\nBook Review\n\n\n\nBig knowledge is conditional.\n\n\n\n\n\nOct 4, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding trust with AI\n\n\n\nAI Ethics\n\nAgency\n\nBook Review\n\n\n\nWhat do you consider when picking an AI tool? or do you consider anything at all?\n\n\n\n\n\nSep 27, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Personalized Learning\n\n\n\nPrompt Engineering\n\nMultimodel\n\nLearning\n\n\n\nA thought piece from Google’s ‘Learn Your Way’ function\n\n\n\n\n\nSep 19, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nArtisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.\n\n\n\nPrompt Engineering\n\nMultimodel\n\nImage Generation\n\n\n\nCan AI become my visual designer?\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nArtisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)\n\n\n\nPrompt Engineering\n\nMultimodel\n\nImage Generation\n\n\n\nCan AI become my visual designer?\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nConsistency of LLMs in Role Taking\n\n\n\nLLMs\n\n\n\nHow long can Chatgpt be Moana?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Use of Metaphor\n\n\n\nLearning\n\n\n\nHow well do LLMs do in understanding and generating metaphors and idioms?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Assessment Using LLMs\n\n\n\nLearning\n\n\n\nCan LLMs help a learner to quickly understand their current level of knowledge?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/W7_P1_LLM_for_product/llm_product.html",
    "href": "posts/W7_P1_LLM_for_product/llm_product.html",
    "title": "Prototyping with LLM",
    "section": "",
    "text": "Prototyping with LLM\nBefore building with large language models, most of my experience with them came from direct interaction: asking questions, refining prompts, and getting surprisingly fluent responses. In those moments, LLMs felt powerful and flexible. However, when I started thinking about using an LLM as part of a real product rather than as a personal assistant, my expectations had to change. A product needs consistency, predictability, and clear boundaries. It also needs to support users who may not know how to prompt well or even what to ask. My goal was to prototype an AI-supported learning workflow where the model would help users reflect on their work, generate study guidance, or scaffold thinking rather than simply provide answers. On paper, this seemed like a natural fit for an LLM. The challenge was to move from “this works when I talk to it” to “this works reliably for someone else.”\n\nTesting the Idea: Prototyping with LLMs in a Workflow\nTo test this, I began embedding LLM outputs into structured workflows. Instead of open-ended chat, I constrained the model with specific roles, instructions, and expected formats. For example, I asked it to analyze student responses, identify misconceptions, or generate targeted reflection questions based on predefined learning goals.\nI also tested the same prompt across multiple inputs to see how stable the outputs were. In theory, if the instructions were clear enough, the model should behave consistently. I treated these prompts almost like product logic: if the input looks like this, the output should follow a predictable pattern. As I iterated, I realized that many prompts that worked perfectly in isolation started to break when placed inside a system.\n\n\n\nAI Workflow\n\n\n\n\nThe Limits I Ran Into\nThe first thing that broke was consistency. Even with carefully written prompts, the model’s responses varied more than I expected. Sometimes the tone shifted. Sometimes the depth of explanation changed. For a human reader, these differences might feel minor, but in a product context, they created uneven user experiences. The second issue was overhelpfulness. When the model was designed to support learning, it often gave too much away. Instead of scaffolding thinking, it sometimes jumped straight to polished explanations. This undermined the original learning goal and made it difficult to control cognitive load. What felt helpful in a demo became problematic in a real educational workflow.\nAnother thing that broke was assumption alignment. The model frequently made assumptions about user intent that were not always correct. If a student response was ambiguous, the AI might confidently interpret it one way, even when multiple interpretations were possible. In a product, this kind of confident guessing can mislead users rather than support them.\nFinally, prompt fragility became clear. Small changes in wording, input length, or user tone sometimes led to disproportionately different outputs. This meant the system was more sensitive than expected, which made it hard to design robust guardrails.\n\n\nReflection:\nThis experience changed how I think about LLMs in products. I no longer see them as plug-and-play intelligence. Instead, they feel more like probabilistic components that require careful framing, monitoring, and constraint. What works in a chat interface does not automatically translate into a reliable system. I also realized that many of the failures were not technical but conceptual. I initially treated the model as if it understood my design intentions. In reality, it only responds to what is explicitly encoded in prompts and structure. Any ambiguity I left behind showed up later as unexpected behavior. Perhaps most importantly, this process highlighted the need to design not just for what the AI can do, but for what users need. In learning contexts especially, more fluent output is not always better. Sometimes the most valuable role for AI is to slow things down, ask better questions, or surface uncertainty rather than resolve it. In the end, what broke was my assumption that intelligence alone makes a good product. Building with LLMs forced me to confront the gap between impressive demos and meaningful, responsible design. That gap is not a failure of the technology, but a reminder that products are systems, not conversations."
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "In Chapter 3 of Superagency, Hoffman and Beato discussed the great potential of LLM being used in creating mental health support. Using that as a case, the authors get to one of their central arguments in this book: Bringing a positive mindset of “what could possibly go right” about AI. The authors argue that AI has so much potential that bringing a problemism view will hinder progress on AI improvement and application, and testing is inevitable and necessary for AI to reach our expectations.\nHowever, I wonder how much exposure is reasonable to conduct testing for a not fully polished AI tool on actual human users. Hoffman and Beato referred a lot to Koko, the mental health App that leveraged the gpt model to allow users draft messages to each other. Although Koko supported countless young people struggling with lack of mental health resources, I wonder if it worthes to take the risks of AI formulating negative messages. Koko was based on more traditional machine learning and natural language processing and used less of generative AI capacity, yet many AI Apps are building to serve areas with high stakes such as mental health, medical situation, youth education…How can testing be done to gather data that can improve the tool but not in the cost of harming some initial users?\n\n\n\nWhile the authors are envisioning the enormous resources AI will bring for supporting mental health workers shortage, I can’t help to think of the recent competitive job market and unemployment rate. There are financial reasons in the challenging job market right now, yet from Wharton’s report, companies are hiring less junior level employees because of more AI usage. Unemployment and increasing competition in pre and post higher education are causing more anxiety in young to middle age people, which can directly lead to more anxiety. While AI powered tools can support people with mental health concerns, it is at the same time, generating problems. A part of the shortage in healthcare workers and teachers are due to low pay and high job requirements rather than lack of resources in nature. Using AI to fill up this shortage cannot change the root cause of labor shortage.\nIn Chapter 4, the authors mentioned that some argued that AI is only making a small group of people wealthy, but they soon reverted that by analyzing that AI brings free access to information to the general public without competition. Everyone can enjoy the information without at the cost of someone else’ s benefits or rights. I totally agree and think that this is true, but the fact that so many people are using AI is changing how the society and job landscape work, and people need to accommodate that change.\n\n\n\nAI and Employment\n\n\n\n\n\nFrom my personal experience and observations, I think AI should not be used to overly replace human to human interactions. Sharing feelings and conversations with AI still feels nothing like talking with a live human, even if they’re a stranger on the other side of the phone. Users of Koko stop liking the messages once they figure out the message is co-created by AI. AI can serve as an alternative to human interactions, but hardly as a replacement. Therefore, the question is: How much human-human interaction versus human-ai interaction do we need?\nOne direction I can think of is to use AI to test and create safer AI tools. Answering the question at the beginning of this blog on “How can testing be done to gather data that can improve the tool but not at the cost of harming some initial users?”, more AI should be used to do repetitive work and provide analytics that are beyond human brain or capacity. It may also be used as a tool to augment human to human interaction, for example the multimodal nature of AI mentioned in Chapter 4 may bring people more ways to experience and process information. It should not be used to replace direct conversations or interactions between humans.\n\n\n\nAfter reading Chapter 3 and 4, I understand that the authors’ advocated opinion on more positive attitudes on AI, but I wonder where the boundary should be. It probably requires decades of exploration and efforts of the whole society. I am curious about how policy makers would think of this, which I will share some thoughts in another post."
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#positive-and-negative-views-on-ai.",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#positive-and-negative-views-on-ai.",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "In Chapter 3 of Superagency, Hoffman and Beato discussed the great potential of LLM being used in creating mental health support. Using that as a case, the authors get to one of their central arguments in this book: Bringing a positive mindset of “what could possibly go right” about AI. The authors argue that AI has so much potential that bringing a problemism view will hinder progress on AI improvement and application, and testing is inevitable and necessary for AI to reach our expectations.\nHowever, I wonder how much exposure is reasonable to conduct testing for a not fully polished AI tool on actual human users. Hoffman and Beato referred a lot to Koko, the mental health App that leveraged the gpt model to allow users draft messages to each other. Although Koko supported countless young people struggling with lack of mental health resources, I wonder if it worthes to take the risks of AI formulating negative messages. Koko was based on more traditional machine learning and natural language processing and used less of generative AI capacity, yet many AI Apps are building to serve areas with high stakes such as mental health, medical situation, youth education…How can testing be done to gather data that can improve the tool but not in the cost of harming some initial users?"
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#more-resources-less-opportunities",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#more-resources-less-opportunities",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "While the authors are envisioning the enormous resources AI will bring for supporting mental health workers shortage, I can’t help to think of the recent competitive job market and unemployment rate. There are financial reasons in the challenging job market right now, yet from Wharton’s report, companies are hiring less junior level employees because of more AI usage. Unemployment and increasing competition in pre and post higher education are causing more anxiety in young to middle age people, which can directly lead to more anxiety. While AI powered tools can support people with mental health concerns, it is at the same time, generating problems. A part of the shortage in healthcare workers and teachers are due to low pay and high job requirements rather than lack of resources in nature. Using AI to fill up this shortage cannot change the root cause of labor shortage.\nIn Chapter 4, the authors mentioned that some argued that AI is only making a small group of people wealthy, but they soon reverted that by analyzing that AI brings free access to information to the general public without competition. Everyone can enjoy the information without at the cost of someone else’ s benefits or rights. I totally agree and think that this is true, but the fact that so many people are using AI is changing how the society and job landscape work, and people need to accommodate that change.\n\n\n\nAI and Employment"
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#where-should-ai-be-used",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#where-should-ai-be-used",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "From my personal experience and observations, I think AI should not be used to overly replace human to human interactions. Sharing feelings and conversations with AI still feels nothing like talking with a live human, even if they’re a stranger on the other side of the phone. Users of Koko stop liking the messages once they figure out the message is co-created by AI. AI can serve as an alternative to human interactions, but hardly as a replacement. Therefore, the question is: How much human-human interaction versus human-ai interaction do we need?\nOne direction I can think of is to use AI to test and create safer AI tools. Answering the question at the beginning of this blog on “How can testing be done to gather data that can improve the tool but not at the cost of harming some initial users?”, more AI should be used to do repetitive work and provide analytics that are beyond human brain or capacity. It may also be used as a tool to augment human to human interaction, for example the multimodal nature of AI mentioned in Chapter 4 may bring people more ways to experience and process information. It should not be used to replace direct conversations or interactions between humans."
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#to-conclude",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#to-conclude",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "After reading Chapter 3 and 4, I understand that the authors’ advocated opinion on more positive attitudes on AI, but I wonder where the boundary should be. It probably requires decades of exploration and efforts of the whole society. I am curious about how policy makers would think of this, which I will share some thoughts in another post."
  },
  {
    "objectID": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "href": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "title": "Building trust with AI",
    "section": "",
    "text": "How to facilitate the process of building trust with AI to improve tool using and making?\n\n\n\nIn the intro and Chapter 1 of Superagency by Geoff Hoffman and Greg Beato, the main question centers around how humans pertain their agency with the power of AI. It’s always been a controversy of whether AI will be able to do so much that it hinders human’s ability to live and think independently. The authors traced back to the 6 months of LLM advancement shut down in 2023 and showed that in 2022, only 35% of Americans participated in a survey that believed that AI based products and services have more benefits than harm to human society. Regarding this worrisome, the authors proposed that AI is just part of the toolkit that humans invent. Learning how to better leverage it will bring more benefits to us than to thinking of how to restrict it. Three years after the mass AI panic and hype, people are now more used to working with AI tools in different parts of our lives, from writing papers to getting laptops fixed to getting food order, people start to work efficiently with AI and realize its massive potential can be used within control.\nWith most people acknowledging the benefits of LLMs and AI tools, the questions now become: how can we know which AI to trust and use for a specific task? Hoffman and Beato mentioned two major ideas that I found interesting. One of them was iterative development, which was initiated by OpenAI for inviting the public to participate in the development of LLM products. This allows the developers to better understand users’ opinions and create more user centered tools but also transfer the control and agency back to the users. The other argument they had about building trusts in AI involved trusts beyond the technologies themselves but also in the developers, regulators, and other users of the technologies. This trust between multiple stakeholders is essential for the general public because people need to know about how models are trained and created and what perspectives or potential biases may exist under those training. This is something that is generally lacking in the current stage of human Ai interaction in society. Just as how we read the product description of electronics, furniture and even food they buy everyday, AI tools should also be given enough review and consideration before someone starts to use them routinely. I believe this will help with opening an outlet for people to start building trust with stakeholders of AI tools and use AI to achieve their goals that follow their original purpose.\nPotential actions to build trust with AI and its stakeholders:\n\nA website as a guide matching people with the best AI tool to achieve their goals.\n\nThe website should include detailed information about the LLM of the tool, including model name, features, and a description of the dataset used to train the model.\n\nPotential problem: These websites may also contain biased “narratives” or miss certain tools.\n\nA community where everyone can share information and resources on AI tools.\n\nTags and labels can be used to help people find relevant AI tools when they need them.\n\nPotential problem: The content in the community may be hard to manage, and information needs to be verified (perhaps LLMs can help with this too…).\n\n\nSuperagency is an obvious pro-AI book, yet it shared many reasons why AI should be embraced by people rather than constrained for us to earn more agency. I hope to further build my idea in helping humans build understanding on AI tools as I keep reading the book."
  },
  {
    "objectID": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "href": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "title": "Learning Assessment Using LLMs",
    "section": "",
    "text": "Many people use LLM tools such as Chatgpt to help them finish school work or complete a task. Since these tasks are usually temporary and aimed to be finished in the short term, people rarely pay attention to the process of learning, and LLMs become controversial in education as being a shortcut of answers rather than learning aid.\nAccording to cognitive psychology, meaningful learning can be defined with retention and transfer, which means learners can memorize or understand the content in depth and apply it to new scenarios. Learning involves many stages, from preview to processing and understanding the knowledge and to review. To understand how LLMs may help with meaningful learning, I want to start with LLM’s ability to assess a learners’ current level of knowledge in a certain field. In classes or an online course, this is often done by the teacher throwing out some pop quiz questions or seeing a show of hands to get a general idea of where students are in a field of study.\nI wonder if LLMs can generate suitable quiz questions that accurately assess a students’ level.\n\nillustration generated by Chatgpt(gpt5)\nFor a quick start, I asked Chatgpt if it can generate a quiz for me to assess my knowledge of AI. Of course, I can add a lot more context to help it understand where I am, but the point of using Chatgpt for this task is to quickly help a student understand their current level of knowledge, so I used a short prompt.\nI used the following prompt:\n\nI want to understand where I am in my knowledge with AI. Can you generate quiz questions to help me assess my current level?\n\nThe first two versions it sent back were either too long or contained a lot of short answer questions, which was quite overwhelming for me as a learner, so I changed my prompt to:\n\nI want to understand where I am in my knowledge with AI. Can you generate short multiple choice quiz questions to help me assess my current level?\n\nThese are the eight questions it generated:\n\n\n\nquestions generated by gpt on assessing my knowledge in AI\n\n\nThe first eight questions were basic and I got all of them right, so Chatgpt offered me a harder quiz. This time the questions were a lot more detailed and advanced, so I only got 5 out of 8 right.\nI then asked Chatgpt to make conclusions on my current level and offer recommendations for me to improve my knowledge. Here is my prompt: “Help me interpret the quiz result. What do I understand, what tasks can I complete, and how may I improve?” It sent back a list of items I understood well, tasks I can likely complete, and where I struggled. What brought my attention was its confidence in my technical skills with me just correctly answering some conceptual questions.\nSpecifically, it said:\n\n“If someone gave you a dataset + scikit-learn or TensorFlow starter code, you could likely: * Train a simple classifier (logistic regression, decision tree). * Interpret probability outputs correctly (once clarified). * Run a CNN or transformer with prebuilt frameworks.”\n\nI don’t think I can train a classifier or run a transformer with prebuilt frameworks if someone gives me a dataset and startercode. There seems to be some overestimation of my skillsets. This can easily deceive a learner and make them over confident about themselves. I think this inaccuracy may be because of 1) me guessing some of the hard questions right and gave out fake feedbacks to gpt and 2) Chatgpt’s miss on the validity of quizzes and scope of conclusion that can be drawn from quizzes.\nNext time anyone says “teachers will be replaced by AI”, tell them that it can’t, even from the first step."
  },
  {
    "objectID": "posts/W6_P1_Before Superagency/before superagency.html",
    "href": "posts/W6_P1_Before Superagency/before superagency.html",
    "title": "The Long Way to Go Before Superagency",
    "section": "",
    "text": "The Long Way to Go Before Superagency\n\n\n\n\nSocial Media Around Us\n\nAfter reading Chapter Two of the book Superagency, I see clear tradeoffs between privacy and scalability and conditions that are required for us to enjoy the freedom of AI, but I also realized how AI literacy and equal access is an essential presumption for us to fully leverage on the benefits AI brings.\nThis chapter started with an analogy of today’s age to the surveillance world in the book 1984 by George Orwell, picturing people’s fear that AI may invade privacy and agency, and in the worst cases of our worries, AI may take over our thoughts and replace us. In the second half of this chapter, Hoffman (one of the authors of Superagency) started to discuss that emerging technologies and AI is bringing us more connected resources and information that allows people to build up networks and make better choices. For example, by looking up people on LinkedIn you will know someone’s skillset and if they are an ideal person you want to work with. Hoffman implied that in today’s era, creating public identities is opening up so many opportunities for human agencies that outweighs the risks to privacy and will not hinder our free will. All these advantages were brought by distributing intelligence through big data, with the formation of Big Knowledge, which is the name of this chapter.\nWhat he argued was totally true, that LinkedIn or other social media is enabling people to know more about others and this world and that humans can use AI and algorithm power to gather and manage big data to make more comprehensive decisions. However, I think all of these benefits will need three important conditions: people’s awareness, consent, and free choices on data and technology usage.\nThese three things come in series. Only if you know that AI is gathering your data can you decide whether to opt in or not. Only if you can choose to not share your information can you be called to have free will in this data era. Unfortunately, these conditions have not all been met in our daily lives, yet AI is operating wildly. Everytime you select “I agree” to an internet consent, do you really read and understand what it says by “We use cookies to improve your browsing experience and analyze site traffic”. With the one time that you decided to choose “I do not agree”, you found out that you can’t even keep browsing this website.\nProducts that leverage LLMs now are not fully empowering free will and human agency with these forceful data collection through internet services and consent questions. Hoffman can easily name out the benefits of AI because he has the knowledge and resources to determine which algorithm is good for use and how to use LinkedIn to create a perfect profile rather than unintentionally disclose one’s privacy.\nI tried using AI meeting assistance tools in the beginning of 2024 when AI audio transcription started to thrive, and the agent went to a lab meeting after I graduated and sent everyone a meeting note. I hope no one notices it and thought I was trying to eavesdrop on conversations, but this happened simply because I didn’t turn off the access of the tool to my Google calendar through a bunch of consents during my onboarding. I have also seen friends sending posts on LinkedIn about every event they attended, but they didn’t even know that those posts were sent.\nEven if we as college and graduate students make these mistakes and expose ourselves to risks in privacy, then what about average people who have only heard about the acronym AI but never tried the tool themselves? What about teenagers and people who never go to college and took a lesson on prompt engineering and cybersecurity? What about elderly people who some even cannot read the tiny consent message?\n\n\n\n\nWe Value Your Privacy Note\n\nTo explore AI’s ability in debating and role taking, we had this activity to debate with an AI chat agent on different positionalities towards LLMs that the Book Superagency listed: Doomers, who believe AI poses an existential threat and should be stopped; Gloomers, who are skeptical of AI’s benefits and advocate for strict regulation to manage its harms. Zoomers, who enthusiastically push to accelerate AI development with minimal oversight; and Bloomers, who are cautiously optimistic, supporting AI’s promise while calling for measured risk management and public involvement.\nI was a firm Bloomer at that time and I asked gpt-4 to be a Gloomer to debate with me. It kept pointing out the lack of regulations in AI while products are created to be used by the public. Now after reading this chapter of Superagency, I felt my position was further swayed by Gloomers as we need more literacy and education on responsible AI and data usage before it further enters into our lives. I hope we as general consumers of LLM products are not just one data point in the big data, but active users, coordinators and even developers of big knowledge.\nWe will reach superagency, but there is still a long way to go."
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "A prompting journal and comparison between GPT-5 and gemini 2.5 flash\n\n\nI have always wanted to create an artistic profile photo that represents me but also includes creativity. As LLMs can now create great images, I decided to have them redesign my profile photo.\nAs a reference, below I include my profile photo (as it appears in the “About” section of this blog site) and my desired painting art style with artistic elements and frames surrounding the figure.\n\n\n\nThe Style I want (photos from freepik)\n\n\n\nMy Profile Photo\nI was curious about which model performs better in image redesign tasks, so I decided to do a control test between two LLMs and understand which one is better at doing these types of image generation tasks. I knew that for image creation, specific tools such as Midjourney or Google’s Nanobanana may work better, but considering the experience of non tech savvy audiences who may interact with general chat agents more, I decided to use the two most popular general AI chat tools: ChatGPT and Gemini. Among all the LLMs models, the latest and most advanced ones include GPT-5 and Gemini 2.5 flash in ChatGPT and Google Gemini chats.\n\n\n\nIn my previous experience generating photos from an LLM based chat agent, I needed to do many rounds of prompting and often could not get the one that fits my needs. Reflecting now, I probably did not use careful prompting to craft the output. Therefore, this time I was using The Five Principles of Prompting described by James Phonexi and Mike Taylor in Prompt Engineering for Generative AI to direct my prompts. The following table includes the explanation of each principle from the book and how I used it to create my initial prompt for redesigning my profile photo.\nTable 1. Initial Prompts Sent to GPT-5 and Gemini 2.5 flash Following the Five Principles of Prompting\n\n\n\n\n\n\n\n\nPrompt Engineering Principle\nExplanation (by Phonexi & Taylor)\nMy Prompting Strategy (prompts and commands I sent to LLMs)\n\n\n\n\nGive Direction\n“Describe the desired style in detail, or reference a relevant persona”\nRedesign my current profile photo (I’ll upload it later) and make it look artistic.The desired color style of the photo is bright and shows creativity.I want the color palette to be a mix of purple, pink, yellow, blue but in light intensity.\n\n\nSpecify Format\n“Define what rules to follow, and the required structure of the response”\nReturn five design options in .jpg format.\n\n\nProvide Examples\n“Insert a diverse set of test cases where the task was done correctly”\nI’ve included several examples attached here.(Sample headshot photos from freepik)\n\n\nEvaluate Quality\n“Identify errors and rate responses, testing what drives performance”\nI gave feedback and evaluation for further improvement in multiple rounds of follow up conversations, for example:I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 years old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.I like the fourth one in terms of the frames and how it keeps the background, but I also like number 3 for the color. Try mixing the two. DO NOT alter any object or characteristics of the person in the image including clothes style and color.\n\n\nDivide Labor\n“Split tasks into multiple steps, chained together for complex goals”\nI gave the instruction first then followed up with my original headshot and more detailed instructions on the occasion, restrictions, explanations on design plan etc. I asked for a rough redesign first and then adjust the details and colors.Follow up after initial prompt:This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don’t be too informal.[attached my profile photo]\n\n\n\nTo control the conditions, I used the same prompts that gave out initial instructions and reference photos for both gpt-5 and gemini 2.5 flash. I also kept my goal or result expectations on the two models similar to the sample images, though I had to give different feedback on the various outputs the two models provided back. I sent the initial prompts at the same time of the day and continued conversations with one model while the other was generating or processing answers.\nProcesses and Results Comparing Two Models\nIn this section, I will reflect on my experience prompting and evaluating the two models to redesign my profile photo based on my imagined style.\nGPT-5: Slow start, Fast Improvement GPT-5 had a long processing time, missed instructions on formatting and lacked text explanations along with images generated.\nGPT-5 seemed to misunderstand formatting commands: After my initial prompt, which I mentioned that the photo to be redesigned will be uploaded later, GPT-5 already started to generate a photo using the two sample images, and I got a combined collage that was not myself at all but the figure in the sample photo. GPT-5 misunderstood the single person headshot to be my photo.\n\n\n\nIt also ignored my request for generating five design options by only returning one.\nGPT-5 only followed image generation with text explanation after conversations were branched. I think there might also be a design gap here on Chatgpt interface. The circle on the right of the input box will turn to a square when it is still processing or generating. Normally the message will also show a round point which is loading, but under image generation mode it usually doesn’t. Therefore, when the agent is still generating text, I wasn’t able to notice the state and kept typing my command. This might be a reason that gpt-5 only returned images without any text or communications with me.\n\n\n\nHowever, I do have to say that even with all those weird reactions, the image quality was high. I would say the first version it generated earned my 75% satisfaction. I was surprised actually. Although it didn’t listen to my command to combine the realistic figure with the cartoon frame but instead created a whole cartoon headshot, the style and color looked nice, and it was different from the sample I sent, which made me feel that this is novel creation rather than just copy pasting.\nVersion 1 User Evaluation: Nice color and style. Could make the figure look younger…\n\n\n\nI thought this painted me older than I looked, so I sent a follow up prompt: &gt; I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\nI like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\n\n\n\nAnd this iteration was almost 90% what I wanted. I did four more rounds of prompting to try different gaze, expressions, color intensity etc, but this version was eventually what I chose.\n(more about Gemini 2.5 flash performance on the next blog)"
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#background",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#background",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "I have always wanted to create an artistic profile photo that represents me but also includes creativity. As LLMs can now create great images, I decided to have them redesign my profile photo.\nAs a reference, below I include my profile photo (as it appears in the “About” section of this blog site) and my desired painting art style with artistic elements and frames surrounding the figure.\n\n\n\nThe Style I want (photos from freepik)\n\n\n\nMy Profile Photo\nI was curious about which model performs better in image redesign tasks, so I decided to do a control test between two LLMs and understand which one is better at doing these types of image generation tasks. I knew that for image creation, specific tools such as Midjourney or Google’s Nanobanana may work better, but considering the experience of non tech savvy audiences who may interact with general chat agents more, I decided to use the two most popular general AI chat tools: ChatGPT and Gemini. Among all the LLMs models, the latest and most advanced ones include GPT-5 and Gemini 2.5 flash in ChatGPT and Google Gemini chats."
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#prompts-and-controlled-conditions",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#prompts-and-controlled-conditions",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "In my previous experience generating photos from an LLM based chat agent, I needed to do many rounds of prompting and often could not get the one that fits my needs. Reflecting now, I probably did not use careful prompting to craft the output. Therefore, this time I was using The Five Principles of Prompting described by James Phonexi and Mike Taylor in Prompt Engineering for Generative AI to direct my prompts. The following table includes the explanation of each principle from the book and how I used it to create my initial prompt for redesigning my profile photo.\nTable 1. Initial Prompts Sent to GPT-5 and Gemini 2.5 flash Following the Five Principles of Prompting\n\n\n\n\n\n\n\n\nPrompt Engineering Principle\nExplanation (by Phonexi & Taylor)\nMy Prompting Strategy (prompts and commands I sent to LLMs)\n\n\n\n\nGive Direction\n“Describe the desired style in detail, or reference a relevant persona”\nRedesign my current profile photo (I’ll upload it later) and make it look artistic.The desired color style of the photo is bright and shows creativity.I want the color palette to be a mix of purple, pink, yellow, blue but in light intensity.\n\n\nSpecify Format\n“Define what rules to follow, and the required structure of the response”\nReturn five design options in .jpg format.\n\n\nProvide Examples\n“Insert a diverse set of test cases where the task was done correctly”\nI’ve included several examples attached here.(Sample headshot photos from freepik)\n\n\nEvaluate Quality\n“Identify errors and rate responses, testing what drives performance”\nI gave feedback and evaluation for further improvement in multiple rounds of follow up conversations, for example:I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 years old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.I like the fourth one in terms of the frames and how it keeps the background, but I also like number 3 for the color. Try mixing the two. DO NOT alter any object or characteristics of the person in the image including clothes style and color.\n\n\nDivide Labor\n“Split tasks into multiple steps, chained together for complex goals”\nI gave the instruction first then followed up with my original headshot and more detailed instructions on the occasion, restrictions, explanations on design plan etc. I asked for a rough redesign first and then adjust the details and colors.Follow up after initial prompt:This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don’t be too informal.[attached my profile photo]\n\n\n\nTo control the conditions, I used the same prompts that gave out initial instructions and reference photos for both gpt-5 and gemini 2.5 flash. I also kept my goal or result expectations on the two models similar to the sample images, though I had to give different feedback on the various outputs the two models provided back. I sent the initial prompts at the same time of the day and continued conversations with one model while the other was generating or processing answers.\nProcesses and Results Comparing Two Models\nIn this section, I will reflect on my experience prompting and evaluating the two models to redesign my profile photo based on my imagined style.\nGPT-5: Slow start, Fast Improvement GPT-5 had a long processing time, missed instructions on formatting and lacked text explanations along with images generated.\nGPT-5 seemed to misunderstand formatting commands: After my initial prompt, which I mentioned that the photo to be redesigned will be uploaded later, GPT-5 already started to generate a photo using the two sample images, and I got a combined collage that was not myself at all but the figure in the sample photo. GPT-5 misunderstood the single person headshot to be my photo.\n\n\n\nIt also ignored my request for generating five design options by only returning one.\nGPT-5 only followed image generation with text explanation after conversations were branched. I think there might also be a design gap here on Chatgpt interface. The circle on the right of the input box will turn to a square when it is still processing or generating. Normally the message will also show a round point which is loading, but under image generation mode it usually doesn’t. Therefore, when the agent is still generating text, I wasn’t able to notice the state and kept typing my command. This might be a reason that gpt-5 only returned images without any text or communications with me.\n\n\n\nHowever, I do have to say that even with all those weird reactions, the image quality was high. I would say the first version it generated earned my 75% satisfaction. I was surprised actually. Although it didn’t listen to my command to combine the realistic figure with the cartoon frame but instead created a whole cartoon headshot, the style and color looked nice, and it was different from the sample I sent, which made me feel that this is novel creation rather than just copy pasting.\nVersion 1 User Evaluation: Nice color and style. Could make the figure look younger…\n\n\n\nI thought this painted me older than I looked, so I sent a follow up prompt: &gt; I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\nI like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\n\n\n\nAnd this iteration was almost 90% what I wanted. I did four more rounds of prompting to try different gaze, expressions, color intensity etc, but this version was eventually what I chose.\n(more about Gemini 2.5 flash performance on the next blog)"
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html",
    "href": "posts/W8_P2_AI_Education/AIEducation.html",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "As a student in Learning Science and Technologies, I have had the chance to interact with several large language models as part of coursework, projects, and everyday studying. Among the most commonly used tools are GPT, Gemini, and Claude. While they are often discussed together as “AI chatbots,” each model reflects different design priorities that shape how they support learning. In this post, I compare these tools through an educational lens, focusing on how they align with learning science principles such as scaffolding, cognitive load, metacognition, and sensemaking.\n\n\nGPT is the tool I most often use for structured academic work. From a learning sciences perspective, GPT excels at scaffolding. When prompted carefully, it can break down complex concepts into step by step explanations, generate outlines, or model examples that resemble worked examples in instructional design. This is especially useful when learning unfamiliar theories or methods, such as cognitive load theory or experimental design.\nGPT is also highly adaptable to different instructional roles. It can act as a tutor, a peer reviewer, or even a simulated instructor when asked to explain content at varying levels of difficulty. This flexibility supports zone of proximal development style learning, where the learner can request more or less support as needed. However, the same strength can become a weakness. Because GPT is very efficient at producing polished answers, it can reduce productive struggle if students rely on it too quickly. From a learning standpoint, this means GPT works best when used intentionally as a support tool rather than an answer generator.\n\n\n\nLLMs\n\n\n\n\n\nGemini stands out most clearly in its integration with multimodal information. Because it is closely tied to Google’s ecosystem, Gemini is especially effective when tasks involve synthesizing information across text, images, tables, or web-based content. From a learning sciences perspective, this supports multimedia learning principles, where learners benefit from connecting verbal and visual representations. Gemini feels particularly useful for exploratory learning. When I use it to brainstorm research questions, compare sources, or connect ideas across domains, it encourages a more open-ended interaction. Instead of guiding me through a single correct path, Gemini often surfaces multiple perspectives or related concepts. This can support sensemaking and knowledge integration, which are important for higher-level learning. That said, Gemini’s exploratory nature sometimes comes at the cost of instructional clarity. Its responses can feel less structured than GPT’s, which may increase cognitive load for novice learners. In educational contexts, this means Gemini may be better suited for early ideation, synthesis, or inquiry-based learning rather than direct instruction.\n\n\n\nClaude feels the most “human” in tone and interaction style, which has important implications for learning. It is particularly strong at reflective tasks, such as discussing reasoning processes, revising writing, or engaging in ethical or conceptual debates. From a learning sciences lens, Claude supports metacognition by making reasoning more explicit and by encouraging learners to think about why an answer works, not just what the answer is. In writing related tasks, Claude is effective as a thought partner rather than a solution provider. It often responds with nuance, caveats, and alternative interpretations, which mirrors how instructors might respond during office hours. This can be especially valuable in communication and social science courses where ambiguity and interpretation are part of the learning goal. However, Claude is sometimes less efficient for technical or highly structured tasks. When clear procedures or precise outputs are needed, its conversational style can feel indirect. This suggests that Claude is best positioned as a reflective companion rather than a procedural tutor.\n\n\n\nFrom a learning science perspective, GPT, Gemini, and Claude each support different learning processes. GPT is strongest for scaffolded instruction and structured academic tasks. Gemini supports multimodal exploration and synthesis across information sources. Claude excels at reflection, metacognition, and writing focused learning.\nRather than asking which tool is best, a more productive question is how each tool can be aligned with specific learning goals. For students, this means choosing tools intentionally based on the type of thinking required. For educators, it suggests opportunities to design assignments that encourage appropriate AI use, such as reflective prompts with Claude, exploratory research with Gemini, or guided practice with GPT. Ultimately, these tools do not replace learning. They reshape how learning happens. Understanding their differences helps us use them not as shortcuts, but as cognitive partners that support deeper engagement with ideas."
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#gpt-structured-scaffolding-and-task-oriented-learning",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#gpt-structured-scaffolding-and-task-oriented-learning",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "GPT is the tool I most often use for structured academic work. From a learning sciences perspective, GPT excels at scaffolding. When prompted carefully, it can break down complex concepts into step by step explanations, generate outlines, or model examples that resemble worked examples in instructional design. This is especially useful when learning unfamiliar theories or methods, such as cognitive load theory or experimental design.\nGPT is also highly adaptable to different instructional roles. It can act as a tutor, a peer reviewer, or even a simulated instructor when asked to explain content at varying levels of difficulty. This flexibility supports zone of proximal development style learning, where the learner can request more or less support as needed. However, the same strength can become a weakness. Because GPT is very efficient at producing polished answers, it can reduce productive struggle if students rely on it too quickly. From a learning standpoint, this means GPT works best when used intentionally as a support tool rather than an answer generator.\n\n\n\nLLMs"
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#gemini-multimodal-reasoning-and-exploratory-learning",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#gemini-multimodal-reasoning-and-exploratory-learning",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "Gemini stands out most clearly in its integration with multimodal information. Because it is closely tied to Google’s ecosystem, Gemini is especially effective when tasks involve synthesizing information across text, images, tables, or web-based content. From a learning sciences perspective, this supports multimedia learning principles, where learners benefit from connecting verbal and visual representations. Gemini feels particularly useful for exploratory learning. When I use it to brainstorm research questions, compare sources, or connect ideas across domains, it encourages a more open-ended interaction. Instead of guiding me through a single correct path, Gemini often surfaces multiple perspectives or related concepts. This can support sensemaking and knowledge integration, which are important for higher-level learning. That said, Gemini’s exploratory nature sometimes comes at the cost of instructional clarity. Its responses can feel less structured than GPT’s, which may increase cognitive load for novice learners. In educational contexts, this means Gemini may be better suited for early ideation, synthesis, or inquiry-based learning rather than direct instruction."
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#claude-reflection-metacognition-and-writing-support",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#claude-reflection-metacognition-and-writing-support",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "Claude feels the most “human” in tone and interaction style, which has important implications for learning. It is particularly strong at reflective tasks, such as discussing reasoning processes, revising writing, or engaging in ethical or conceptual debates. From a learning sciences lens, Claude supports metacognition by making reasoning more explicit and by encouraging learners to think about why an answer works, not just what the answer is. In writing related tasks, Claude is effective as a thought partner rather than a solution provider. It often responds with nuance, caveats, and alternative interpretations, which mirrors how instructors might respond during office hours. This can be especially valuable in communication and social science courses where ambiguity and interpretation are part of the learning goal. However, Claude is sometimes less efficient for technical or highly structured tasks. When clear procedures or precise outputs are needed, its conversational style can feel indirect. This suggests that Claude is best positioned as a reflective companion rather than a procedural tutor."
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#to-conclude",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#to-conclude",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "From a learning science perspective, GPT, Gemini, and Claude each support different learning processes. GPT is strongest for scaffolded instruction and structured academic tasks. Gemini supports multimodal exploration and synthesis across information sources. Claude excels at reflection, metacognition, and writing focused learning.\nRather than asking which tool is best, a more productive question is how each tool can be aligned with specific learning goals. For students, this means choosing tools intentionally based on the type of thinking required. For educators, it suggests opportunities to design assignments that encourage appropriate AI use, such as reflective prompts with Claude, exploratory research with Gemini, or guided practice with GPT. Ultimately, these tools do not replace learning. They reshape how learning happens. Understanding their differences helps us use them not as shortcuts, but as cognitive partners that support deeper engagement with ideas."
  },
  {
    "objectID": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "href": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "title": "LLMs and Personalized Learning",
    "section": "",
    "text": "Can AI Textbooks Really Teach Us “Our Way”?\nShaaron Ainsworth (2018) wrote about the power of using multiple representations when we learn—things like text, images, diagrams, or even games. The big idea is that different formats can make learning richer, but more isn’t always better. Sometimes, piling on complicated visuals or fancy tools just makes people more confused.\nThis made me think about Google’s new experiment called Learn Your Way. It’s basically an AI-augmented textbook powered by LLMs that can spin the same material into four different versions:\n\nText with images and little quizzes\nSlides with narration\nAudio lessons\nMindmaps\n\n\n\n\n\nwebsite of Learn Your Way by Google\n\nAt first glance, this is super cool. It’s like having a magic button that says, “Hey, do you want your homework as a podcast, a PowerPoint, or a mindmap?” If you hate reading walls of text, maybe you’ll prefer listening. If you’re more visual, the mindmap might click instantly.\nBut here’s the catch: more choice doesn’t always mean better learning. If you bounce from audio to slides to mindmap, you might just end up scrolling around instead of actually paying attention. It’s a little like opening Netflix, browsing for half an hour, and then realizing you haven’t watched anything.\nWhat makes this experiment fascinating is the role of LLMs. Traditional textbooks are frozen in one form. If you wanted an audiobook version, someone had to record it. If you wanted a visual map, a designer had to create it. Now, with an LLM, the same content can be dynamically repurposed in seconds. The AI doesn’t just “translate” text into another medium. It can restructure, simplify, and even add examples or quizzes on the fly. That’s a level of personalization education has never really had.\nOf course, there are limits. Sometimes LLMs “hallucinate,” meaning they might invent facts or frame things in ways the original text didn’t intend. That raises the question: when the AI reshapes your textbook, are you still learning the same material, or are you learning the AI’s remix of it? And what happens when two students get slightly different versions of the “same” lesson. Will they come away with the same understanding?\nStill, I can’t help but imagine where this could go. Maybe in the future, instead of one-size-fits-all courses, each person gets a “tutor-bot” that knows their quirks: “Hey, Alex, here’s your lesson in meme format today because you’re tired.” Or: “Jamie, I know you love podcasts, so I turned this lecture into a 10-minute audio story for your walk to class.” That’s the kind of playful, adaptive learning LLMs might actually make possible.\nSo here’s a question: if an LLM can turn your textbook into whatever style you want, will that make you actually learn more or just procrastinate in fancier ways? And maybe the scarier question: how much of your education would then be shaped by what an AI decides is “your way”?"
  }
]