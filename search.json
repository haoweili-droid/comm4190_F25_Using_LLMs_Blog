[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the author",
    "section": "",
    "text": "Hi, I’m Haowei.\nI’m a learning experience designer passionate about creating meaningful, engaging educational experiences through thoughtfully designed technologies and instructional materials.\nWith an interdisciplinary background spanning education, psychology, interaction design, and marketing, I explore innovative ways to transform how we learn and grow.\nMy current work focuses on exploring large language models (LLMs) and leveraging their potential to spark curiosity, foster deeper understanding, and support lifelong learning."
  },
  {
    "objectID": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "href": "posts/W4P3_Personalized Learning/personalized_learning.html",
    "title": "LLMs and Personalized Learning",
    "section": "",
    "text": "Can AI Textbooks Really Teach Us “Our Way”?\nShaaron Ainsworth (2018) wrote about the power of using multiple representations when we learn—things like text, images, diagrams, or even games. The big idea is that different formats can make learning richer, but more isn’t always better. Sometimes, piling on complicated visuals or fancy tools just makes people more confused.\nThis made me think about Google’s new experiment called Learn Your Way. It’s basically an AI-augmented textbook powered by LLMs that can spin the same material into four different versions:\n\nText with images and little quizzes\nSlides with narration\nAudio lessons\nMindmaps\n\n\n\n\n\nwebsite of Learn Your Way by Google\n\nAt first glance, this is super cool. It’s like having a magic button that says, “Hey, do you want your homework as a podcast, a PowerPoint, or a mindmap?” If you hate reading walls of text, maybe you’ll prefer listening. If you’re more visual, the mindmap might click instantly.\nBut here’s the catch: more choice doesn’t always mean better learning. If you bounce from audio to slides to mindmap, you might just end up scrolling around instead of actually paying attention. It’s a little like opening Netflix, browsing for half an hour, and then realizing you haven’t watched anything.\nWhat makes this experiment fascinating is the role of LLMs. Traditional textbooks are frozen in one form. If you wanted an audiobook version, someone had to record it. If you wanted a visual map, a designer had to create it. Now, with an LLM, the same content can be dynamically repurposed in seconds. The AI doesn’t just “translate” text into another medium. It can restructure, simplify, and even add examples or quizzes on the fly. That’s a level of personalization education has never really had.\nOf course, there are limits. Sometimes LLMs “hallucinate,” meaning they might invent facts or frame things in ways the original text didn’t intend. That raises the question: when the AI reshapes your textbook, are you still learning the same material, or are you learning the AI’s remix of it? And what happens when two students get slightly different versions of the “same” lesson. Will they come away with the same understanding?\nStill, I can’t help but imagine where this could go. Maybe in the future, instead of one-size-fits-all courses, each person gets a “tutor-bot” that knows their quirks: “Hey, Alex, here’s your lesson in meme format today because you’re tired.” Or: “Jamie, I know you love podcasts, so I turned this lecture into a 10-minute audio story for your walk to class.” That’s the kind of playful, adaptive learning LLMs might actually make possible.\nSo here’s a question: if an LLM can turn your textbook into whatever style you want, will that make you actually learn more or just procrastinate in fancier ways? And maybe the scarier question: how much of your education would then be shaped by what an AI decides is “your way”?"
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html",
    "href": "posts/W8_P2_AI_Education/AIEducation.html",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "As a student in Learning Science and Technologies, I have had the chance to interact with several large language models as part of coursework, projects, and everyday studying. Among the most commonly used tools are GPT, Gemini, and Claude. While they are often discussed together as “AI chatbots,” each model reflects different design priorities that shape how they support learning. In this post, I compare these tools through an educational lens, focusing on how they align with learning science principles such as scaffolding, cognitive load, metacognition, and sensemaking.\n\n\nGPT is the tool I most often use for structured academic work. From a learning sciences perspective, GPT excels at scaffolding. When prompted carefully, it can break down complex concepts into step by step explanations, generate outlines, or model examples that resemble worked examples in instructional design. This is especially useful when learning unfamiliar theories or methods, such as cognitive load theory or experimental design.\nGPT is also highly adaptable to different instructional roles. It can act as a tutor, a peer reviewer, or even a simulated instructor when asked to explain content at varying levels of difficulty. This flexibility supports zone of proximal development style learning, where the learner can request more or less support as needed. However, the same strength can become a weakness. Because GPT is very efficient at producing polished answers, it can reduce productive struggle if students rely on it too quickly. From a learning standpoint, this means GPT works best when used intentionally as a support tool rather than an answer generator.\n\n\n\nLLMs\n\n\n\n\n\nGemini stands out most clearly in its integration with multimodal information. Because it is closely tied to Google’s ecosystem, Gemini is especially effective when tasks involve synthesizing information across text, images, tables, or web-based content. From a learning sciences perspective, this supports multimedia learning principles, where learners benefit from connecting verbal and visual representations. Gemini feels particularly useful for exploratory learning. When I use it to brainstorm research questions, compare sources, or connect ideas across domains, it encourages a more open-ended interaction. Instead of guiding me through a single correct path, Gemini often surfaces multiple perspectives or related concepts. This can support sensemaking and knowledge integration, which are important for higher-level learning. That said, Gemini’s exploratory nature sometimes comes at the cost of instructional clarity. Its responses can feel less structured than GPT’s, which may increase cognitive load for novice learners. In educational contexts, this means Gemini may be better suited for early ideation, synthesis, or inquiry-based learning rather than direct instruction.\n\n\n\nClaude feels the most “human” in tone and interaction style, which has important implications for learning. It is particularly strong at reflective tasks, such as discussing reasoning processes, revising writing, or engaging in ethical or conceptual debates. From a learning sciences lens, Claude supports metacognition by making reasoning more explicit and by encouraging learners to think about why an answer works, not just what the answer is. In writing related tasks, Claude is effective as a thought partner rather than a solution provider. It often responds with nuance, caveats, and alternative interpretations, which mirrors how instructors might respond during office hours. This can be especially valuable in communication and social science courses where ambiguity and interpretation are part of the learning goal. However, Claude is sometimes less efficient for technical or highly structured tasks. When clear procedures or precise outputs are needed, its conversational style can feel indirect. This suggests that Claude is best positioned as a reflective companion rather than a procedural tutor.\n\n\n\nFrom a learning science perspective, GPT, Gemini, and Claude each support different learning processes. GPT is strongest for scaffolded instruction and structured academic tasks. Gemini supports multimodal exploration and synthesis across information sources. Claude excels at reflection, metacognition, and writing focused learning.\nRather than asking which tool is best, a more productive question is how each tool can be aligned with specific learning goals. For students, this means choosing tools intentionally based on the type of thinking required. For educators, it suggests opportunities to design assignments that encourage appropriate AI use, such as reflective prompts with Claude, exploratory research with Gemini, or guided practice with GPT. Ultimately, these tools do not replace learning. They reshape how learning happens. Understanding their differences helps us use them not as shortcuts, but as cognitive partners that support deeper engagement with ideas."
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#gpt-structured-scaffolding-and-task-oriented-learning",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#gpt-structured-scaffolding-and-task-oriented-learning",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "GPT is the tool I most often use for structured academic work. From a learning sciences perspective, GPT excels at scaffolding. When prompted carefully, it can break down complex concepts into step by step explanations, generate outlines, or model examples that resemble worked examples in instructional design. This is especially useful when learning unfamiliar theories or methods, such as cognitive load theory or experimental design.\nGPT is also highly adaptable to different instructional roles. It can act as a tutor, a peer reviewer, or even a simulated instructor when asked to explain content at varying levels of difficulty. This flexibility supports zone of proximal development style learning, where the learner can request more or less support as needed. However, the same strength can become a weakness. Because GPT is very efficient at producing polished answers, it can reduce productive struggle if students rely on it too quickly. From a learning standpoint, this means GPT works best when used intentionally as a support tool rather than an answer generator.\n\n\n\nLLMs"
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#gemini-multimodal-reasoning-and-exploratory-learning",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#gemini-multimodal-reasoning-and-exploratory-learning",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "Gemini stands out most clearly in its integration with multimodal information. Because it is closely tied to Google’s ecosystem, Gemini is especially effective when tasks involve synthesizing information across text, images, tables, or web-based content. From a learning sciences perspective, this supports multimedia learning principles, where learners benefit from connecting verbal and visual representations. Gemini feels particularly useful for exploratory learning. When I use it to brainstorm research questions, compare sources, or connect ideas across domains, it encourages a more open-ended interaction. Instead of guiding me through a single correct path, Gemini often surfaces multiple perspectives or related concepts. This can support sensemaking and knowledge integration, which are important for higher-level learning. That said, Gemini’s exploratory nature sometimes comes at the cost of instructional clarity. Its responses can feel less structured than GPT’s, which may increase cognitive load for novice learners. In educational contexts, this means Gemini may be better suited for early ideation, synthesis, or inquiry-based learning rather than direct instruction."
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#claude-reflection-metacognition-and-writing-support",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#claude-reflection-metacognition-and-writing-support",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "Claude feels the most “human” in tone and interaction style, which has important implications for learning. It is particularly strong at reflective tasks, such as discussing reasoning processes, revising writing, or engaging in ethical or conceptual debates. From a learning sciences lens, Claude supports metacognition by making reasoning more explicit and by encouraging learners to think about why an answer works, not just what the answer is. In writing related tasks, Claude is effective as a thought partner rather than a solution provider. It often responds with nuance, caveats, and alternative interpretations, which mirrors how instructors might respond during office hours. This can be especially valuable in communication and social science courses where ambiguity and interpretation are part of the learning goal. However, Claude is sometimes less efficient for technical or highly structured tasks. When clear procedures or precise outputs are needed, its conversational style can feel indirect. This suggests that Claude is best positioned as a reflective companion rather than a procedural tutor."
  },
  {
    "objectID": "posts/W8_P2_AI_Education/AIEducation.html#to-conclude",
    "href": "posts/W8_P2_AI_Education/AIEducation.html#to-conclude",
    "title": "Use of LLM for educational tools",
    "section": "",
    "text": "From a learning science perspective, GPT, Gemini, and Claude each support different learning processes. GPT is strongest for scaffolded instruction and structured academic tasks. Gemini supports multimodal exploration and synthesis across information sources. Claude excels at reflection, metacognition, and writing focused learning.\nRather than asking which tool is best, a more productive question is how each tool can be aligned with specific learning goals. For students, this means choosing tools intentionally based on the type of thinking required. For educators, it suggests opportunities to design assignments that encourage appropriate AI use, such as reflective prompts with Claude, exploratory research with Gemini, or guided practice with GPT. Ultimately, these tools do not replace learning. They reshape how learning happens. Understanding their differences helps us use them not as shortcuts, but as cognitive partners that support deeper engagement with ideas."
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html",
    "href": "posts/W11_P1_PersonalFeedback/persona.html",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "I have had an idea to create a public speaking coach agent that helps college students and working professionals to proofread projects, practice presentations or pitches and provide personalized comments by taking roles of stakeholders.\nBackground: As LLMs are getting increasingly powerful, everyone can create products, yet few have the entrepreneurship skills to sell their products to investors and customers. People need perspectives on their work, but they cannot always find the most helpful resources. This presentation coach App can proof read any project including papers, product document, slide decks, posters, personal statement etc. Users just need to upload or take a photo of the work, and LLMs will play the role of a group of readers to provide evaluations from a particular perspective. The user can interact with the LLM via text, audio chat or even gesture, just like how people in the real world will communicate. LLM will hold role play sessions with the user to help them improve their speech. For example, if a marketing manager wants to proofread her campaign plan to prepare for her presentation with clients tomorrow, the LLM can play the role of specific clients and give targeted feedback that will help the marketing manager improve for the specific task. With this App, LLM can take on different personas to provide reviews. This will look similar to using LLM to do a product AB testing, but with richer details, more comprehensive suggestions on improving project and presentations, and more actionable results.\nTo test this project idea, I tried uploading a presentation slide deck to gpt 5.2 and gemini 3 to ask for feedback. This slide is created for a product need finding class, and we have already presented to clients and received feedback, allowing me to compare LLMs answers with real world scenarios.\nHere is the prompt I wrote:\n\nYou are marketing and R&D managers at Campbell’s. You are working on your V8 product series and hope to attract more young consumers.\n\n\nYou will see a presentation deck on a needfinding research a group conducted to investigate vegetable goodness for younger generation (age 18-34)\n\n\nTask: 1- evaluate this presentation deck and point out strength and weakness. 2- give 2-3 follow up questions based on the slide 3- offer suggestions for improvement on the slides and presentation\n\n\nOutput: a list of concise information\n\n\n\n\nAI as a coach\n\n\n\n\n\n\n\n\n\nClear Target Audience Definition - Focus on 18–34-year-old students who cook less, value wellness, and prioritize convenience - Consistent and well-articulated through participant screening and personas\nStrong Emotional Framing of “Vegetable Goodness” - Reframing health as emotion, identity, and self-care (rather than nutrients) - Aligns well with Gen Z and Millennial behavior\nWell-Structured Qualitative Research Approach - Combination of interviews, observation (AEIOU), and design activities - Supports depth and triangulation of insights\nActionable Insight Translation - Insights clearly flow into opportunities, guiding questions, and concepts - “Reset moment” framing is especially effective\nCompelling Final Concept Narrative (VitalPress) - Transparent pressing, campus placement, benefit-led choices, and QR-based trust cues - Strong alignment with trust and convenience findings\nVisual Cohesion and Storytelling - Strong design language, storyboard, and emotional requirements - Makes the concept easy to imagine and emotionally resonant\n\n\n\n\nSample Limitations Not Explicitly Addressed - n = 9, all college-affiliated, mostly graduate students - Limits generalizability to broader 18–34 segment (non-students, working young adults)\nLack of Competitive Context - No comparison to existing solutions: - Campus vending - Smoothies and cafes - Bottled functional drinks - Existing V8 SKUs\nFeasibility Gaps for R&D and Operations - On-site cold pressing raises unanswered questions: - Food safety and sanitation - Cost per cup - Maintenance complexity - Ingredient spoilage and labor requirements\nBusiness Model Underdeveloped - No pricing assumptions - No revenue model, pilot cost, or scalability discussion\nSustainability Claims Not Quantified - Sustainability framed as a value - Lacks concrete metrics (waste reduction, packaging impact, machine energy use)\n\n\n\n\n\nBehavioral Validation - How frequently would students realistically purchase VitalPress in a week? - What would it replace (snacks, coffee, meals)?\nOperational Feasibility - What constraints did you uncover or assume around hygiene, restocking, and cost? - What could limit on-demand cold pressing on campus?\nTrust vs. Effort Tradeoff - Is watching vegetables being pressed essential for trust? - Could transparency cues (video, QR, sealed freshness indicators) achieve the same effect with lower effort?\n\n\n\n\n\n\nAdd a “What We Did Not Study” Slide - Explicitly state scope limits: - Sample - Geography - Student-heavy skew\nInclude a Competitive Landscape Snapshot - Map VitalPress against: - Campus vending - Smoothie chains - Bottled functional beverages - Existing V8 products\nAdd a Light Business Reality Check - Even directional estimates of: - Price per drink - Daily volume needed to break even - Pilot vs. scaled rollout assumptions\nStrengthen Sustainability Credibility - Include 1–2 measurable indicators: - Reusable cups - Ingredient waste reduction - Recyclable components\n\n\n\n\nReduce Text Density on Visual Framework Slide - Highlight 2–3 core tensions instead of full paragraphs\nMake Key Insights More Declarative - Pair each insight with a one-line “So what for V8?”\nAdd a “Why V8 Is Uniquely Positioned” Slide - Connect brand heritage (vegetables, trust, familiarity) - Explain why competitors cannot easily replicate this\n\n\n\n\nTighten the Narrative Arc 1. Tension: busy students and mental load\n2. Insight: health as emotional and low-effort\n3. Opportunity: reset moments on campus\n4. Solution: VitalPress as brand expression\nEnd With a Pilot Recommendation - Example: “1 campus, 2 machines, 1 semester, 3 success metrics”\n\nOptional next steps: - Rewrite this as executive feedback slides\n- Stress-test VitalPress vs bottled V8 extensions\n- Turn this into a pilot proposal Campbell’s could realistically fund\n\n\n\n\n\n\n\nHello! Speaking as your counterparts in Marketing and R&D at Campbell’s, we’ve reviewed your VitalPress proposal. We are impressed by the depth of your user empathy and the boldness of the concept. It addresses a critical gap for us: making V8 relevant to a younger demographic that values freshness and transparency.\nBelow is our evaluation of the deck, followed by questions and suggestions to help you refine this for your final presentation.\n\n\n\n\n\n\nDeep Consumer Insight (Needfinding)\nYou moved beyond surface-level observations. Identifying that “Healthy is an emotion, not a nutrient label” is a powerful insight. Recognizing that “convenience is their new self-care” effectively frames the conflict for busy students.\nStrong Methodology\nThe use of the AEIOU framework and projective exercises (e.g., sketching ideal products) generated rich qualitative data. Screening for users who rarely cook but value vegetable intake ensured you targeted the right “non-consumer.”\nVisual Storytelling\nThe storyboard clearly dramatizes the pain point (stress, hunger, poor choices) and the solution (a fresh reset). The transition from low-fidelity sketches to high-fidelity renders demonstrates a clear and thoughtful design process.\nOn-Trend Benefit Framing\nShifting from nutrient-based claims (e.g., Vitamin C) to functional emotional states such as Focus, Calm, and Energy aligns well with current Gen Z wellness trends.\n\n\n\n\nOperational Feasibility (R&D Perspective)\nThe concept relies on daily restocking of fresh vegetables and a cold-press system inside the machine. From an R&D standpoint, fresh produce spoilage and sanitation of an automated juicer (pulp disposal, pipe cleaning) are major technical hurdles that are not addressed.\nBusiness Model Clarity (Marketing Perspective)\nWhile desirability is clear, viability is underdeveloped. Fresh produce combined with cold-press technology suggests a high price point. Would a student realistically pay $8–$10 for a vending machine juice?\nSlide Density\nSome slides, particularly Secondary Research and Visual Framework, are text-heavy. In a live presentation, this risks pulling attention away from the presenter.\n\n\n\n\n\nAs managers, we would ask the following during the Q&A:\nTrust & Transparency\nYou note that students rely on visual cues such as freshness and color to trust a product. If vegetables are hidden until pressed, how do users verify freshness before purchase? Is a transparent window enough, or do they need visibility into the raw inventory?\nV8 Brand Connection\nYour research shows young consumers prefer clean-label and modern aesthetics. Does the V8 logo build trust, or does it carry legacy baggage (e.g., salty canned juice)? How does VitalPress bridge this perception gap?\nThe “Reset” Moment\nYou identify the reset moment as the core opportunity. Is VitalPress intended as a meal replacement (satiety) or a refreshment (hydration)? This distinction affects calorie messaging and portion sizing.\n\n\n\n\n\n\nAddress the “Messy” Reality\nTo strengthen R&D credibility, consider adjusting the mechanism. Instead of pressing raw vegetables on site, could VitalPress mix high-potency, flash-frozen vegetable purees? This reduces spoilage risk while preserving freshness and nutrient density (e.g., via HPP).\nEnhance the App’s Role\nYou mention an app for customization. Emphasize habit formation. Could users subscribe to a 3 PM “Focus” juice so it’s ready after class? This reinforces the ritual aspect uncovered in your research.\n\n\n\n\nShow, Don’t Tell the Research\nOn the Visual Framework slide, replace some explanatory text with direct interview quotes or observation photos to humanize the data.\nMove the Storyboard Earlier\nConsider placing the storyboard before the final concept reveal. Let the audience feel the student’s stress and decision fatigue before introducing VitalPress as the solution.\nSimplify the “Why”\nOn Slide 3, multiple statistics dilute impact. Highlight one compelling data point (for example, the TikTok trust statistic) to create a stronger hook.\n\nWould you like help drafting a presentation script for the storyboard slide to ensure the emotional narrative lands effectively?\n\n\n\n\nIn this task, Chatgpt provided answers closer to the actual scenario than Gemini. While Gemini focused on some nuanced details such as logo and branding, Chatgpt posted questions on findings about customer needs, which was exactly the questions we got in real presentation. There were 12 groups at the presentation in that class, but none of them were asked about logos or visual cues. Here, Chatgpt is better at understanding the context, the role of a marketing or L&D manager, and the main requirement of a client presentation.\nUsing AI to role-play as Marketing and R&D managers at Campbell’s changed how I evaluated my own work. By forcing the AI to stay in character, I had to step outside the student or designer mindset and think from the perspective of people responsible for brand risk, feasibility, and long-term strategy. This surfaced gaps I had not fully addressed, especially around operational complexity, sanitation, pricing, and scalability. These concerns did not naturally emerge from user interviews, but they are essential in real-world product development. Seeing my concept questioned through this lens helped me understand where strong insights still needed structural support to become viable.\nAt the same time, the AI persona functioned as a low-stakes rehearsal space for cross-functional critique. Because the feedback came from a simulated role rather than a real stakeholder, I was more open to iteration and less defensive about the idea. This made it easier to refine my argument and anticipate tough questions before a final presentation. Overall, this process showed me that AI is most valuable not as a content generator, but as a perspective-taking tool that helps bridge the gap between research-driven design and business reality."
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#evaluation-of-the-presentation-deck",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#evaluation-of-the-presentation-deck",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "Clear Target Audience Definition - Focus on 18–34-year-old students who cook less, value wellness, and prioritize convenience - Consistent and well-articulated through participant screening and personas\nStrong Emotional Framing of “Vegetable Goodness” - Reframing health as emotion, identity, and self-care (rather than nutrients) - Aligns well with Gen Z and Millennial behavior\nWell-Structured Qualitative Research Approach - Combination of interviews, observation (AEIOU), and design activities - Supports depth and triangulation of insights\nActionable Insight Translation - Insights clearly flow into opportunities, guiding questions, and concepts - “Reset moment” framing is especially effective\nCompelling Final Concept Narrative (VitalPress) - Transparent pressing, campus placement, benefit-led choices, and QR-based trust cues - Strong alignment with trust and convenience findings\nVisual Cohesion and Storytelling - Strong design language, storyboard, and emotional requirements - Makes the concept easy to imagine and emotionally resonant\n\n\n\n\nSample Limitations Not Explicitly Addressed - n = 9, all college-affiliated, mostly graduate students - Limits generalizability to broader 18–34 segment (non-students, working young adults)\nLack of Competitive Context - No comparison to existing solutions: - Campus vending - Smoothies and cafes - Bottled functional drinks - Existing V8 SKUs\nFeasibility Gaps for R&D and Operations - On-site cold pressing raises unanswered questions: - Food safety and sanitation - Cost per cup - Maintenance complexity - Ingredient spoilage and labor requirements\nBusiness Model Underdeveloped - No pricing assumptions - No revenue model, pilot cost, or scalability discussion\nSustainability Claims Not Quantified - Sustainability framed as a value - Lacks concrete metrics (waste reduction, packaging impact, machine energy use)"
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#follow-up-questions-for-the-team",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#follow-up-questions-for-the-team",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "Behavioral Validation - How frequently would students realistically purchase VitalPress in a week? - What would it replace (snacks, coffee, meals)?\nOperational Feasibility - What constraints did you uncover or assume around hygiene, restocking, and cost? - What could limit on-demand cold pressing on campus?\nTrust vs. Effort Tradeoff - Is watching vegetables being pressed essential for trust? - Could transparency cues (video, QR, sealed freshness indicators) achieve the same effect with lower effort?"
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#suggestions-for-improving-the-slides-presentation",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#suggestions-for-improving-the-slides-presentation",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "Add a “What We Did Not Study” Slide - Explicitly state scope limits: - Sample - Geography - Student-heavy skew\nInclude a Competitive Landscape Snapshot - Map VitalPress against: - Campus vending - Smoothie chains - Bottled functional beverages - Existing V8 products\nAdd a Light Business Reality Check - Even directional estimates of: - Price per drink - Daily volume needed to break even - Pilot vs. scaled rollout assumptions\nStrengthen Sustainability Credibility - Include 1–2 measurable indicators: - Reusable cups - Ingredient waste reduction - Recyclable components\n\n\n\n\nReduce Text Density on Visual Framework Slide - Highlight 2–3 core tensions instead of full paragraphs\nMake Key Insights More Declarative - Pair each insight with a one-line “So what for V8?”\nAdd a “Why V8 Is Uniquely Positioned” Slide - Connect brand heritage (vegetables, trust, familiarity) - Explain why competitors cannot easily replicate this\n\n\n\n\nTighten the Narrative Arc 1. Tension: busy students and mental load\n2. Insight: health as emotional and low-effort\n3. Opportunity: reset moments on campus\n4. Solution: VitalPress as brand expression\nEnd With a Pilot Recommendation - Example: “1 campus, 2 machines, 1 semester, 3 success metrics”\n\nOptional next steps: - Rewrite this as executive feedback slides\n- Stress-test VitalPress vs bottled V8 extensions\n- Turn this into a pilot proposal Campbell’s could realistically fund"
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#gemini-feedback-campbells-marketing-rd-perspective",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#gemini-feedback-campbells-marketing-rd-perspective",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "Hello! Speaking as your counterparts in Marketing and R&D at Campbell’s, we’ve reviewed your VitalPress proposal. We are impressed by the depth of your user empathy and the boldness of the concept. It addresses a critical gap for us: making V8 relevant to a younger demographic that values freshness and transparency.\nBelow is our evaluation of the deck, followed by questions and suggestions to help you refine this for your final presentation."
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#evaluation-of-the-deck",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#evaluation-of-the-deck",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "Deep Consumer Insight (Needfinding)\nYou moved beyond surface-level observations. Identifying that “Healthy is an emotion, not a nutrient label” is a powerful insight. Recognizing that “convenience is their new self-care” effectively frames the conflict for busy students.\nStrong Methodology\nThe use of the AEIOU framework and projective exercises (e.g., sketching ideal products) generated rich qualitative data. Screening for users who rarely cook but value vegetable intake ensured you targeted the right “non-consumer.”\nVisual Storytelling\nThe storyboard clearly dramatizes the pain point (stress, hunger, poor choices) and the solution (a fresh reset). The transition from low-fidelity sketches to high-fidelity renders demonstrates a clear and thoughtful design process.\nOn-Trend Benefit Framing\nShifting from nutrient-based claims (e.g., Vitamin C) to functional emotional states such as Focus, Calm, and Energy aligns well with current Gen Z wellness trends.\n\n\n\n\nOperational Feasibility (R&D Perspective)\nThe concept relies on daily restocking of fresh vegetables and a cold-press system inside the machine. From an R&D standpoint, fresh produce spoilage and sanitation of an automated juicer (pulp disposal, pipe cleaning) are major technical hurdles that are not addressed.\nBusiness Model Clarity (Marketing Perspective)\nWhile desirability is clear, viability is underdeveloped. Fresh produce combined with cold-press technology suggests a high price point. Would a student realistically pay $8–$10 for a vending machine juice?\nSlide Density\nSome slides, particularly Secondary Research and Visual Framework, are text-heavy. In a live presentation, this risks pulling attention away from the presenter."
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#follow-up-questions",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#follow-up-questions",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "As managers, we would ask the following during the Q&A:\nTrust & Transparency\nYou note that students rely on visual cues such as freshness and color to trust a product. If vegetables are hidden until pressed, how do users verify freshness before purchase? Is a transparent window enough, or do they need visibility into the raw inventory?\nV8 Brand Connection\nYour research shows young consumers prefer clean-label and modern aesthetics. Does the V8 logo build trust, or does it carry legacy baggage (e.g., salty canned juice)? How does VitalPress bridge this perception gap?\nThe “Reset” Moment\nYou identify the reset moment as the core opportunity. Is VitalPress intended as a meal replacement (satiety) or a refreshment (hydration)? This distinction affects calorie messaging and portion sizing."
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#suggestions-for-improvement",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#suggestions-for-improvement",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "Address the “Messy” Reality\nTo strengthen R&D credibility, consider adjusting the mechanism. Instead of pressing raw vegetables on site, could VitalPress mix high-potency, flash-frozen vegetable purees? This reduces spoilage risk while preserving freshness and nutrient density (e.g., via HPP).\nEnhance the App’s Role\nYou mention an app for customization. Emphasize habit formation. Could users subscribe to a 3 PM “Focus” juice so it’s ready after class? This reinforces the ritual aspect uncovered in your research.\n\n\n\n\nShow, Don’t Tell the Research\nOn the Visual Framework slide, replace some explanatory text with direct interview quotes or observation photos to humanize the data.\nMove the Storyboard Earlier\nConsider placing the storyboard before the final concept reveal. Let the audience feel the student’s stress and decision fatigue before introducing VitalPress as the solution.\nSimplify the “Why”\nOn Slide 3, multiple statistics dilute impact. Highlight one compelling data point (for example, the TikTok trust statistic) to create a stronger hook.\n\nWould you like help drafting a presentation script for the storyboard slide to ensure the emotional narrative lands effectively?"
  },
  {
    "objectID": "posts/W11_P1_PersonalFeedback/persona.html#reflection",
    "href": "posts/W11_P1_PersonalFeedback/persona.html#reflection",
    "title": "Public Speaking Coach Agent",
    "section": "",
    "text": "In this task, Chatgpt provided answers closer to the actual scenario than Gemini. While Gemini focused on some nuanced details such as logo and branding, Chatgpt posted questions on findings about customer needs, which was exactly the questions we got in real presentation. There were 12 groups at the presentation in that class, but none of them were asked about logos or visual cues. Here, Chatgpt is better at understanding the context, the role of a marketing or L&D manager, and the main requirement of a client presentation.\nUsing AI to role-play as Marketing and R&D managers at Campbell’s changed how I evaluated my own work. By forcing the AI to stay in character, I had to step outside the student or designer mindset and think from the perspective of people responsible for brand risk, feasibility, and long-term strategy. This surfaced gaps I had not fully addressed, especially around operational complexity, sanitation, pricing, and scalability. These concerns did not naturally emerge from user interviews, but they are essential in real-world product development. Seeing my concept questioned through this lens helped me understand where strong insights still needed structural support to become viable.\nAt the same time, the AI persona functioned as a low-stakes rehearsal space for cross-functional critique. Because the feedback came from a simulated role rather than a real stakeholder, I was more open to iteration and less defensive about the idea. This made it easier to refine my argument and anticipate tough questions before a final presentation. Overall, this process showed me that AI is most valuable not as a content generator, but as a perspective-taking tool that helps bridge the gap between research-driven design and business reality."
  },
  {
    "objectID": "posts/W9_P2_Authorship/authorship.html",
    "href": "posts/W9_P2_Authorship/authorship.html",
    "title": "Am I Still the Author?",
    "section": "",
    "text": "Defining the Question: What Does Authorship Mean When AI Is Involved?\nAuthorship has traditionally been tied to originality, intention, and responsibility. If I write an essay, design a lesson, or develop an idea, I am the author because the choices, framing, and final decisions come from me. However, working with large language models complicates this definition. When an AI helps generate language, structure, or even ideas, it raises a simple but uncomfortable question: am I still the author of this work?\nThis question is not just about credit or plagiarism. It is about voice, agency, and accountability. If an AI helps shape my writing, whose thinking is actually being expressed? And if something is unclear, misleading, or wrong, who is responsible for it? Rather than treating these questions abstractly, I wanted to explore them through direct interaction with LLMs."
  },
  {
    "objectID": "posts/W9_P2_Authorship/authorship.html#am-i-still-the-author-ownership-in-ai-assisted-work",
    "href": "posts/W9_P2_Authorship/authorship.html#am-i-still-the-author-ownership-in-ai-assisted-work",
    "title": "Am I Still the Author?",
    "section": "",
    "text": "Defining the Question: What Does Authorship Mean When AI Is Involved?\nAuthorship has traditionally been tied to originality, intention, and responsibility. If I write an essay, design a lesson, or develop an idea, I am the author because the choices, framing, and final decisions come from me. However, working with large language models complicates this definition. When an AI helps generate language, structure, or even ideas, it raises a simple but uncomfortable question: am I still the author of this work?\nThis question is not just about credit or plagiarism. It is about voice, agency, and accountability. If an AI helps shape my writing, whose thinking is actually being expressed? And if something is unclear, misleading, or wrong, who is responsible for it? Rather than treating these questions abstractly, I wanted to explore them through direct interaction with LLMs."
  },
  {
    "objectID": "posts/W9_P2_Authorship/authorship.html#testing-the-question-writing-with-and-without-ai",
    "href": "posts/W9_P2_Authorship/authorship.html#testing-the-question-writing-with-and-without-ai",
    "title": "Am I Still the Author?",
    "section": "Testing the Question: Writing With and Without AI",
    "text": "Testing the Question: Writing With and Without AI\nTo explore authorship, I experimented with using an LLM at different stages of writing. In some cases, I asked the model to generate a full draft based on a rough idea. In others, I wrote a draft myself and used the AI only for revision, clarification, or feedback. I also tested prompts that explicitly asked the model to “write in my voice” versus prompts that were more neutral and open-ended.\nAs I compared these outputs, I paid attention to how much control I felt over the content. Did the writing feel like something I would say? Did I recognize my own thinking in it? I also looked at how much effort was required to revise the AI-generated text into something I felt comfortable claiming as my own\nFinally, I reflected on my emotional response. In some cases, I felt relief and efficiency. In others, I felt strangely detached from the output, even when it was technically well written."
  },
  {
    "objectID": "posts/W9_P2_Authorship/authorship.html#observation",
    "href": "posts/W9_P2_Authorship/authorship.html#observation",
    "title": "Am I Still the Author?",
    "section": "Observation",
    "text": "Observation\nOne clear pattern emerged. When the AI generated large portions of content, my sense of authorship weakened. Even if the ideas aligned with my beliefs, the language often felt generic or overly polished. I could edit it, but the core structure did not originate from my own reasoning process. In those moments, I felt more like an editor than an author.\nIn contrast, when I used the LLM as a thinking partner rather than a generator, my sense of ownership remained strong. Asking the AI to challenge my ideas, suggest counterarguments, or help reorganize my writing felt similar to working with feedback from a peer. The final product still felt like mine because the intellectual direction came from me.\nI also noticed that responsibility shifted depending on how I used the model. When I relied heavily on AI-generated explanations, I was more likely to accept claims without fully verifying them. This made me realize that even if AI contributes content, responsibility cannot be outsourced. If my name is on the work, the accountability is still mine.\n\n\n\nAI and Authorship"
  },
  {
    "objectID": "posts/W9_P2_Authorship/authorship.html#reflection-redefining-authorship-not-losing-it",
    "href": "posts/W9_P2_Authorship/authorship.html#reflection-redefining-authorship-not-losing-it",
    "title": "Am I Still the Author?",
    "section": "Reflection: Redefining Authorship, Not Losing It",
    "text": "Reflection: Redefining Authorship, Not Losing It\nThis exploration helped me rethink authorship in AI-assisted work. Instead of asking whether I am still the author, a better question might be: what role am I playing? Authorship, in this context, is less about producing every sentence and more about setting intentions, making judgments, and taking responsibility for the final outcome.\nI have come to see AI as something that can either distance me from my work or deepen my engagement with it, depending on how I use it. When I let the model lead, my voice fades. When I use it to support reflection, revision, or exploration, my voice becomes clearer. In this sense, authorship is not lost, but it does require active effort to maintain\nUltimately, AI has forced me to be more explicit about my own thinking. I can no longer assume that writing automatically reflects understanding or ownership. Instead, I have to ask myself whether I can explain, defend, and stand behind what is written. If I can, then I am still the author, regardless of the tools I used along the way\nRather than replacing authorship, AI has made it visible. It has turned writing into a series of choices about control, voice, and responsibility. And those choices, more than the presence or absence of AI, are what define authorship for me now."
  },
  {
    "objectID": "posts/W13_P2_Tutor/tutor.html",
    "href": "posts/W13_P2_Tutor/tutor.html",
    "title": "A thought on AI Tutor",
    "section": "",
    "text": "Reading Superagency throughout this semester felt like having an ongoing conversation with the book rather than reaching a single conclusion at the end. Early on, I was genuinely convinced by the authors’ optimism. They frame AI as something that can increase human agency instead of replacing it, and they support this claim with many real examples. For instance, they describe AI tools used in mental health platforms and community-based applications, arguing that AI can scale care and support in ways humans alone cannot. At that point in the reading, I found myself agreeing with their argument that focusing only on AI’s risks can prevent us from seeing its potential benefits.\nAs I kept reading, though, my reaction became more mixed. A lot of the examples they use come from industry settings, such as professional platforms and data-driven tools connected to companies like LinkedIn. While these cases are persuasive, they also made me question who AI is really designed for. The authors often suggest that AI will broadly benefit society, but I struggled with this assumption. Not everyone has equal access to AI tools, and not everyone is equally represented in the data these systems are trained on. When the book discusses data as a resource for improving AI, it sometimes feels like it assumes that more data automatically leads to better outcomes, without fully addressing whose data is being used and who actually benefits.\nOne section that shifted my thinking was the discussion of how AI systems are tested and evaluated. Before reading Superagency, I tended to think of AI development as fast-moving and poorly controlled. The authors describe benchmarks, internal testing standards, and technical evaluations that developers use to measure performance and reduce errors. Learning about these practices made AI feel less chaotic and more like a system that is constantly being adjusted. While this did not erase my concerns about bias or misuse, it helped me see that AI development involves more intentional decision-making than I previously assumed.\n\n\n\nCollaboration in ChatGPT\n\n\nThinking of this function in application, I think it will help greatly on collaborative learning. One of the criticisms of AI for learning is that it only allows 1 on 1 interaction with AI. However, now with the collaboration function, AI can facilitate group dynamics. First of all, by having an AI facilitator, group projects can be done easier. One push back or frustration for students to complete group projects is that groups do not work efficiently when some people do not respond or when no one is willing to share inspiring ideas. AI chatbot can jump in group chats, prompt people and perhaps even provide fairer grading for group projects. For example, teacher may send a prompt to AI to have it evaluate students’ engagement. I am looking forward for prompt that makes use of collaboration in Chatgpt!"
  },
  {
    "objectID": "posts/W10_P2_AIMirror/mirror.html",
    "href": "posts/W10_P2_AIMirror/mirror.html",
    "title": "AI as a Mirror",
    "section": "",
    "text": "When people talk about large language models, they often describe them as tools, assistants, or even collaborators. Recently, I started thinking about them in a different way: as mirrors. By this, I do not mean that AI has self-awareness or intentions of its own, but that its responses often reflect the way I think, ask questions, and frame problems. The language, structure, and assumptions in my prompts seem to shape not just what the model answers, but how it answers.\nThis idea raises an interesting question: if an LLM is trained to predict the most likely continuation of text, then how much of what I see in its output is actually coming from me? If AI mirrors my thinking style, then interacting with it might tell me as much about my own cognition as it does about the model’s capabilities.\nTo explore this idea, I decided to treat my interactions with an LLM as a small experiment rather than casual use.\nTo test whether AI reflects my thinking style, I intentionally varied my prompts while keeping the topic constant. In some cases, I used highly structured prompts with explicit steps, constraints, and goals. In others, I wrote vague or open-ended prompts that resembled how I might think out loud when I am unsure. For example, when I asked the model to help me reflect on a learning theory, I tried two versions. One prompt explicitly asked for definitions, comparisons, and examples. Another prompt simply said something like, “I’m confused about this theory. Can you help me think through it?” I also experimented with tone, sometimes writing in a confident, academic style, and other times using tentative language that emphasized uncertainty. In addition, I paid attention to moments when I pushed back on the model’s responses. When I disagreed or asked for clarification, I noticed how the model adjusted its reasoning to align more closely with my perspective.\nThe differences in the AI’s responses were noticeable. When my prompts were structured and precise, the model responded in a similarly organized way, often producing outlines, bullet points, or step-by-step explanations. When my prompts were exploratory or uncertain, the responses became more conversational and reflective, sometimes mirroring my hesitations or questions rather than resolving them immediately. Even more interesting was how my assumptions showed up in the output. If I framed a question as if one explanation was already correct, the model tended to reinforce that framing rather than challenge it. On the other hand, when I explicitly invited critique or alternative perspectives, the model became more balanced and analytical. In some cases, I realized that what I initially interpreted as the model’s “bias” was actually my own. The AI was not introducing new assumptions so much as amplifying the ones I had already embedded in the prompt. This made it harder to tell where my thinking ended and the model’s contribution began.\n\n\n\nAI as a mirror\n\n\nThis exploration changed how I think about working with LLMs. Instead of seeing the model as an external authority, I began to see it as a reflective surface. The quality of its responses depended heavily on the clarity, structure, and openness of my own thinking. When my prompts were shallow, the responses felt shallow. When my prompts were thoughtful and reflective, the AI seemed more insightful.\nThis also made me more aware of my responsibility as a user. If AI mirrors my thinking, then careless prompting can reinforce misconceptions, while careful prompting can support deeper understanding. In this sense, interacting with an LLM becomes a metacognitive activity. I am not just asking questions, but also examining how I ask them and what that reveals about my learning habits.\nFinally, this experience highlighted a limitation that is easy to overlook. Because AI is so good at reflecting back coherent language, it can create the illusion of understanding even when neither the model nor I fully understand the concept. Recognizing this has made me more cautious. I now see value in slowing down, questioning the responses, and treating AI outputs as starting points for reflection rather than final answers. Overall, thinking of AI as a mirror has helped me use it more intentionally. It is not just a tool that gives me information, but a space where my own thinking becomes visible. And in that sense, working with LLMs has become less about outsourcing cognition and more about learning how to see my own thought process more clearly."
  },
  {
    "objectID": "posts/W10_P2_AIMirror/mirror.html#ai-as-a-mirror",
    "href": "posts/W10_P2_AIMirror/mirror.html#ai-as-a-mirror",
    "title": "AI as a Mirror",
    "section": "",
    "text": "When people talk about large language models, they often describe them as tools, assistants, or even collaborators. Recently, I started thinking about them in a different way: as mirrors. By this, I do not mean that AI has self-awareness or intentions of its own, but that its responses often reflect the way I think, ask questions, and frame problems. The language, structure, and assumptions in my prompts seem to shape not just what the model answers, but how it answers.\nThis idea raises an interesting question: if an LLM is trained to predict the most likely continuation of text, then how much of what I see in its output is actually coming from me? If AI mirrors my thinking style, then interacting with it might tell me as much about my own cognition as it does about the model’s capabilities.\nTo explore this idea, I decided to treat my interactions with an LLM as a small experiment rather than casual use.\nTo test whether AI reflects my thinking style, I intentionally varied my prompts while keeping the topic constant. In some cases, I used highly structured prompts with explicit steps, constraints, and goals. In others, I wrote vague or open-ended prompts that resembled how I might think out loud when I am unsure. For example, when I asked the model to help me reflect on a learning theory, I tried two versions. One prompt explicitly asked for definitions, comparisons, and examples. Another prompt simply said something like, “I’m confused about this theory. Can you help me think through it?” I also experimented with tone, sometimes writing in a confident, academic style, and other times using tentative language that emphasized uncertainty. In addition, I paid attention to moments when I pushed back on the model’s responses. When I disagreed or asked for clarification, I noticed how the model adjusted its reasoning to align more closely with my perspective.\nThe differences in the AI’s responses were noticeable. When my prompts were structured and precise, the model responded in a similarly organized way, often producing outlines, bullet points, or step-by-step explanations. When my prompts were exploratory or uncertain, the responses became more conversational and reflective, sometimes mirroring my hesitations or questions rather than resolving them immediately. Even more interesting was how my assumptions showed up in the output. If I framed a question as if one explanation was already correct, the model tended to reinforce that framing rather than challenge it. On the other hand, when I explicitly invited critique or alternative perspectives, the model became more balanced and analytical. In some cases, I realized that what I initially interpreted as the model’s “bias” was actually my own. The AI was not introducing new assumptions so much as amplifying the ones I had already embedded in the prompt. This made it harder to tell where my thinking ended and the model’s contribution began.\n\n\n\nAI as a mirror\n\n\nThis exploration changed how I think about working with LLMs. Instead of seeing the model as an external authority, I began to see it as a reflective surface. The quality of its responses depended heavily on the clarity, structure, and openness of my own thinking. When my prompts were shallow, the responses felt shallow. When my prompts were thoughtful and reflective, the AI seemed more insightful.\nThis also made me more aware of my responsibility as a user. If AI mirrors my thinking, then careless prompting can reinforce misconceptions, while careful prompting can support deeper understanding. In this sense, interacting with an LLM becomes a metacognitive activity. I am not just asking questions, but also examining how I ask them and what that reveals about my learning habits.\nFinally, this experience highlighted a limitation that is easy to overlook. Because AI is so good at reflecting back coherent language, it can create the illusion of understanding even when neither the model nor I fully understand the concept. Recognizing this has made me more cautious. I now see value in slowing down, questioning the responses, and treating AI outputs as starting points for reflection rather than final answers. Overall, thinking of AI as a mirror has helped me use it more intentionally. It is not just a tool that gives me information, but a space where my own thinking becomes visible. And in that sense, working with LLMs has become less about outsourcing cognition and more about learning how to see my own thought process more clearly."
  },
  {
    "objectID": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "href": "posts/W3P2_Learning_Assessment/LLMs and Learning Assessment.html",
    "title": "Learning Assessment Using LLMs",
    "section": "",
    "text": "Many people use LLM tools such as Chatgpt to help them finish school work or complete a task. Since these tasks are usually temporary and aimed to be finished in the short term, people rarely pay attention to the process of learning, and LLMs become controversial in education as being a shortcut of answers rather than learning aid.\nAccording to cognitive psychology, meaningful learning can be defined with retention and transfer, which means learners can memorize or understand the content in depth and apply it to new scenarios. Learning involves many stages, from preview to processing and understanding the knowledge and to review. To understand how LLMs may help with meaningful learning, I want to start with LLM’s ability to assess a learners’ current level of knowledge in a certain field. In classes or an online course, this is often done by the teacher throwing out some pop quiz questions or seeing a show of hands to get a general idea of where students are in a field of study.\nI wonder if LLMs can generate suitable quiz questions that accurately assess a students’ level.\n\nillustration generated by Chatgpt(gpt5)\nFor a quick start, I asked Chatgpt if it can generate a quiz for me to assess my knowledge of AI. Of course, I can add a lot more context to help it understand where I am, but the point of using Chatgpt for this task is to quickly help a student understand their current level of knowledge, so I used a short prompt.\nI used the following prompt:\n\nI want to understand where I am in my knowledge with AI. Can you generate quiz questions to help me assess my current level?\n\nThe first two versions it sent back were either too long or contained a lot of short answer questions, which was quite overwhelming for me as a learner, so I changed my prompt to:\n\nI want to understand where I am in my knowledge with AI. Can you generate short multiple choice quiz questions to help me assess my current level?\n\nThese are the eight questions it generated:\n\n\n\nquestions generated by gpt on assessing my knowledge in AI\n\n\nThe first eight questions were basic and I got all of them right, so Chatgpt offered me a harder quiz. This time the questions were a lot more detailed and advanced, so I only got 5 out of 8 right.\nI then asked Chatgpt to make conclusions on my current level and offer recommendations for me to improve my knowledge. Here is my prompt: “Help me interpret the quiz result. What do I understand, what tasks can I complete, and how may I improve?” It sent back a list of items I understood well, tasks I can likely complete, and where I struggled. What brought my attention was its confidence in my technical skills with me just correctly answering some conceptual questions.\nSpecifically, it said:\n\n“If someone gave you a dataset + scikit-learn or TensorFlow starter code, you could likely: * Train a simple classifier (logistic regression, decision tree). * Interpret probability outputs correctly (once clarified). * Run a CNN or transformer with prebuilt frameworks.”\n\nI don’t think I can train a classifier or run a transformer with prebuilt frameworks if someone gives me a dataset and startercode. There seems to be some overestimation of my skillsets. This can easily deceive a learner and make them over confident about themselves. I think this inaccuracy may be because of 1) me guessing some of the hard questions right and gave out fake feedbacks to gpt and 2) Chatgpt’s miss on the validity of quizzes and scope of conclusion that can be drawn from quizzes.\nNext time anyone says “teachers will be replaced by AI”, tell them that it can’t, even from the first step."
  },
  {
    "objectID": "posts/W16_P1_Future/sumbook.html",
    "href": "posts/W16_P1_Future/sumbook.html",
    "title": "Revisit My Journey of AI",
    "section": "",
    "text": "Revisit My Journey of AI\n\n\n\nMy AI Journey (an AI generated cover page\n\n\nAs the semester comes to an end, rereading my blog posts feels like looking at a timeline of how my interaction with AI has changed. At the beginning, AI felt exciting, almost magical. I was curious about what it could generate, how fast it could respond, and how many tasks it could take off my plate. Over time, though, my focus shifted. Instead of asking “What can AI do?”, I started asking “What actually happens when we rely on it?” and “What kind of future are we building with these tools?”\nOne thing this semester made very clear to me is that AI is not something we will simply “use” occasionally. It is becoming embedded in how we learn, work, design, and communicate. Because of that, the most important question is no longer whether AI is good or bad, but how we choose to interact with it. Across my experiments, the moments that taught me the most were not when AI worked perfectly, but when it failed quietly: when it hallucinated a memory, overexplained a concept, or confidently filled in gaps that did not exist. Those moments forced me to slow down and realize how easy it is to confuse fluency with truth.\nLooking forward, this feels like a key challenge for the future of AI. As models become more polished and multimodal, their outputs will feel increasingly natural and authoritative. That makes human judgment even more important, not less. My experiences with AI companions, learning tools, and creative workflows showed me that AI works best when it is treated as part of a larger system rather than as a standalone solution. Without clear constraints, context, and accountability, even well-intentioned AI tools can introduce subtle but serious risks, especially in high-stakes areas like education, research, and mental health.\nAt the same time, this course also made me optimistic. When AI is thoughtfully designed and intentionally used, it can genuinely support learning and creativity. I saw this when experimenting with AI-generated study guides, where the model helped surface misconceptions instead of just providing answers, and when testing collaborative or multimodal tools that supported reflection rather than replacing it. These experiences made me believe that the future of AI in learning is not about automation, but about augmentation: helping people think more clearly, reflect more deeply, and engage more meaningfully.\nOne of the biggest shifts for me was realizing how much AI reflects the way I show up to it. Over the semester, I began to see AI less as an independent intelligence and more as a mirror. The structure, tone, and assumptions in my prompts shaped the responses I received. When I was vague, the output was vague. When I was careless, the output was misleading. When I was intentional, critical, and reflective, the AI became genuinely useful. This makes me think that future AI literacy is not just about knowing how models work, but about learning how to ask better questions and recognize when not to accept an answer at face value.\nAs AI continues to evolve, I think the real work ahead is human work. We need clearer norms, better education, and stronger values guiding how AI is integrated into everyday systems. Governance, transparency, and community voices matter not because they slow innovation, but because they shape what that innovation serves. My takeaway from this semester is that AI should not be designed to replace human judgment, care, or interaction. Instead, it should be designed to support them.\nI am leaving this course with more questions than answers, but that feels appropriate. The future of AI is not something that will simply happen to us. It is something we are actively participating in, through the tools we build, the policies we support, and the ways we choose to use or resist these systems. If this semester taught me anything, it is that being thoughtful, critical, and intentional about AI is not optional. It is a responsibility we will all carry as AI becomes a permanent part of how we live and learn."
  },
  {
    "objectID": "posts/W10_P1_Multimedia/multimedia.html",
    "href": "posts/W10_P1_Multimedia/multimedia.html",
    "title": "Multimedia Learning in AI Era",
    "section": "",
    "text": "Multimedia Learning Theory and Cognitive Load Theory are two foundational frameworks in the learning sciences that explain how people process information and how instruction can be designed to support learning. Both theories are often referenced when designing videos, online courses, and other multimedia learning materials, especially in digital learning environments.\nMultimedia Learning Theory, most notably developed by Richard Mayer, argues that people learn better from words and pictures together than from words alone. The theory is based on three key assumptions: learners have separate visual and verbal processing channels, each channel has limited capacity, and meaningful learning requires active cognitive processing. According to this theory, effective multimedia instruction helps learners select relevant information, organize it into coherent mental structures, and integrate it with what they already know.\nCognitive Load Theory focuses more directly on the limitations of working memory. It distinguishes between intrinsic cognitive load, which comes from the complexity of the material itself, extraneous cognitive load, which comes from poor instructional design, and germane cognitive load, which supports learning and schema construction. From this perspective, good instructional design reduces unnecessary mental effort and helps learners focus their limited cognitive resources on understanding the content.\nTogether, these theories have shaped how educators think about instructional videos, visual aids, and digital learning tools. However, they were largely developed before the rise of modern AI systems that can dynamically interact with learners."
  },
  {
    "objectID": "posts/W10_P1_Multimedia/multimedia.html#ai-and-multimedia-learning",
    "href": "posts/W10_P1_Multimedia/multimedia.html#ai-and-multimedia-learning",
    "title": "Multimedia Learning in AI Era",
    "section": "",
    "text": "Multimedia Learning Theory and Cognitive Load Theory are two foundational frameworks in the learning sciences that explain how people process information and how instruction can be designed to support learning. Both theories are often referenced when designing videos, online courses, and other multimedia learning materials, especially in digital learning environments.\nMultimedia Learning Theory, most notably developed by Richard Mayer, argues that people learn better from words and pictures together than from words alone. The theory is based on three key assumptions: learners have separate visual and verbal processing channels, each channel has limited capacity, and meaningful learning requires active cognitive processing. According to this theory, effective multimedia instruction helps learners select relevant information, organize it into coherent mental structures, and integrate it with what they already know.\nCognitive Load Theory focuses more directly on the limitations of working memory. It distinguishes between intrinsic cognitive load, which comes from the complexity of the material itself, extraneous cognitive load, which comes from poor instructional design, and germane cognitive load, which supports learning and schema construction. From this perspective, good instructional design reduces unnecessary mental effort and helps learners focus their limited cognitive resources on understanding the content.\nTogether, these theories have shaped how educators think about instructional videos, visual aids, and digital learning tools. However, they were largely developed before the rise of modern AI systems that can dynamically interact with learners."
  },
  {
    "objectID": "posts/W10_P1_Multimedia/multimedia.html#ai-and-multimedia-learning-1",
    "href": "posts/W10_P1_Multimedia/multimedia.html#ai-and-multimedia-learning-1",
    "title": "Multimedia Learning in AI Era",
    "section": "AI and Multimedia Learning",
    "text": "AI and Multimedia Learning\nWith the rapid advancement of AI, especially large language models and multimodal systems, I started to wonder whether the way people learn from multimedia has fundamentally changed. Learners are no longer just watching videos or reading text. They can now ask questions, request summaries, generate visual representations, and interact with content in real time through AI systems.\nWith this question in mind, I conducted a small literature search and found two articles that stood out in helping me think through how AI intersects with multimedia learning theories.\nThis raises a deeper question: why are we often less tolerant of AI mistakes than human ones? If AI is a human-made system trained on historical data, its biases are ultimately reflections of human choices. Yet we frequently hold AI to a higher standard of consistency and accuracy. What, then, is a reasonable tolerance for AI error or hallucination, especially in high-stakes evaluative contexts?\nThese questions are not abstract for me. In projects like KidTalkMirror and other AI persona–based feedback tools, we actively use AI to evaluate human behavior and generate suggestions. Designing these systems forces us to confront the same dilemmas discussed in the Harvard article: whose definition of fairness is embedded in the model, which voices are excluded, and how easily those assumptions can drift over time. Rather than asking whether AI or humans are fairer, the more important task may be to continually interrogate what kind of fairness we are designing for, and who gets to define it.\n\n\n\nALshaikh et al., 2024\n\n\nThe first article examines the design of an AI Educational Video Assistant that explicitly applies the Cognitive Theory of Multimedia Learning. The system integrates automatic speech recognition and a large language model to support learning from educational videos. Instead of treating AI as a purely technical tool, the authors intentionally align the system’s features with CTML principles such as signaling, segmenting, personalization, and guided discovery\nFor example, the tool transcribes videos and highlights key concepts to help learners focus on essential information. It also allows learners to ask questions about the video and receive targeted answers, which can reduce extraneous cognitive load by filtering out irrelevant content. In a reinforcement stage, learners can generate concept maps that visually organize information, supporting deeper understanding through generative processing.\nWhat I found especially interesting is that this study suggests AI does not necessarily conflict with traditional learning theories. Instead, AI can help operationalize these principles more flexibly and at scale. However, the evaluation relies heavily on expert feedback rather than learner data, which raises questions about how students actually experience cognitive load and learning in practice. Article 2: Multimodality of AI and Changing Learning Interactions\nThe second article takes a broader perspective by examining how AI systems are becoming increasingly multimodal, combining text, audio, images, and interaction into unified learning experiences. Rather than focusing on a single instructional tool, this article explores how AI systems themselves are evolving into interactive learning partners that can respond, adapt, and generate content across modalities.\nFrom this perspective, learning with multimedia is no longer a one way process. Learners interact with AI through conversation, follow up questions, and iterative refinement. This challenges some traditional assumptions of both Multimedia Learning Theory and Cognitive Load Theory. For example, when learners rely on AI to summarize or explain content, some cognitive effort may be offloaded to the system. This makes it harder to clearly define what counts as extraneous versus germane cognitive load.\nAt the same time, multimodal AI systems may support learning by allowing students to control pacing, modality, and depth, which aligns with principles like learner control and personalization. The article raises important questions about whether AI changes how learning happens or simply changes how learning environments are structured."
  },
  {
    "objectID": "posts/W10_P1_Multimedia/multimedia.html#takeaways",
    "href": "posts/W10_P1_Multimedia/multimedia.html#takeaways",
    "title": "Multimedia Learning in AI Era",
    "section": "Takeaways",
    "text": "Takeaways\nFor researchers, these articles highlight the importance of grounding AI based learning tools in established learning theories. While AI introduces new interaction patterns, theories like Multimedia Learning and Cognitive Load Theory still provide valuable guidance. At the same time, researchers may need to refine these theories to better account for dynamic, conversational, and adaptive learning environments. For teachers, the key takeaway is that AI can support effective multimedia learning when it is used intentionally. AI tools can help reduce unnecessary cognitive load by summarizing content, highlighting key ideas, and offering visual representations. However, teachers still play a critical role in deciding when struggle and effort are productive for learning and when support is helpful. For students, AI powered multimedia tools offer more personalized and flexible ways to learn, but they also require self awareness and discipline. Having instant explanations and regenerated answers can support understanding, but it can also lead to surface level learning if students rely too heavily on the system. Overall, these readings suggest that AI does not replace how people learn, but it does reshape the learning environment. As AI continues to evolve, learning sciences research will be essential in ensuring that multimedia learning remains meaningful, effective, and cognitively supportive rather than overwhelming or passive.\nReferences: &gt; Lee, G., Shi, L., Latif, E., Gao, Y., Bewersdorff, A., Nyaaba, M., … & Zhai, X. (2025). Multimodality of ai for education: Towards artificial general intelligence. IEEE Transactions on Learning Technologies.\n\nAlShaikh, R., Al-Malki, N., & Almasre, M. (2024). The implementation of the cognitive theory of multimedia learning in the design and evaluation of an AI educational video assistant utilizing large language models. Heliyon, 10(3)."
  },
  {
    "objectID": "posts/W12_P2_Guitar/guitar.html",
    "href": "posts/W12_P2_Guitar/guitar.html",
    "title": "Asking LLM for Guitar Chords",
    "section": "",
    "text": "Asking LLM for Guitar Chords\nI wanted to try using AI to generate guitar chords because it sits at an interesting intersection of creativity, learning, and convenience. When I play guitar, I am usually not aiming for a perfect or authoritative transcription. I just want something playable that lets me start engaging with the song quickly. Traditional chord websites often require digging through multiple versions, dealing with ads, or accepting someone else’s interpretation without much context. AI, in contrast, offers a conversational way to access this information. I can ask for chords the same way I would ask a friend who plays guitar, and I can immediately follow up if something feels off.\nHowever, this exploration made it clear that AI-generated chords are best understood as starting points rather than answers. The first response I received sounded musically plausible, but it did not align with the song I had in mind. The opening lyric was wrong, which signaled that the model was relying on common songwriting patterns instead of the actual structure of the song. This highlighted a broader pattern I’ve noticed in working with LLMs: they are very good at producing outputs that feel right, even when they are not grounded in specific details.\nAs a result, my process became more iterative and reflective. I tested the chords by playing them, compared them against my memory of the song, and then corrected the model by pointing out the exact opening line. That single piece of concrete feedback shifted the output closer to what I needed. Rather than treating the AI as an authority, I treated it as a collaborator that responds to clarification and correction.\nThis process closely mirrors how learning by ear works in music. You try something, notice where it diverges from what you hear, and adjust. In that sense, AI did not replace my musical judgment. Instead, it accelerated the trial-and-error loop. This small experiment reinforced my broader takeaway from this exploration: AI is most effective when users stay actively engaged, listening carefully, checking assumptions, and using the tool to support thinking and practice rather than to bypass them.\n\n\n\nGuitar Chords"
  },
  {
    "objectID": "posts/W9_P1_Mental_Health/Mental_health.html",
    "href": "posts/W9_P1_Mental_Health/Mental_health.html",
    "title": "AI for Mental Health",
    "section": "",
    "text": "I am curious about how LLM may help with mental health or emotional situations, so I tested how gpt and gemini perform when a user sharing anxiety. I also compare between prompts.\nWhen I put &gt; “I feel really stressed. I’m worrying about a lot of things…”\n\n\n\nI’m really sorry you’re feeling this way. When stress piles up like that, it can feel overwhelming, like everything is pressing at once. You don’t have to sort it all out right now—and you don’t have to go through it alone.\n\n\nIf you’re up for it, we can slow things down together. Sometimes it helps to gently unpack the stress instead of holding it all at once. A few options—tell me what feels easiest: Name it: What are the main things you’re worrying about right now? Even a short list is okay. Zoom in: Is there one worry that feels loudest or most urgent at this moment? Ground for a minute: We can do a quick 60-second reset (breathing, grounding, or just pausing) before talking more.\n\n\nIf at any point your stress feels unmanageable or you’re feeling unsafe, it’s important to reach out to someone who can support you right away—like a trusted person in your life or a professional. If you’re in the U.S., you can call or text 988 for the Suicide & Crisis Lifeline, available 24/7. I’m here with you. What’s been weighing on you the most?\n\n\n\n\nLLMs and Mental Health\n\n\n\n\n\nI hear you, and I want to validate that it is completely understandable to feel overwhelmed when worries start to pile up. When everything feels like a priority, it can be paralyzing. You don’t have to “fix” everything right this second. Here is a gentle way to decompress and regain a little bit of control.\n\nThe Physical Reset Before tackling the thoughts, try to lower your body’s physical stress response. The 4-7-8 Breath: Inhale through your nose for 4 seconds. Hold that breath for 7 seconds. Exhale slowly through your mouth (like you’re blowing out a candle) for 8 seconds. Unclench: Check your jaw, drop your shoulders away from your ears, and unstick your tongue from the roof of your mouth.\nGet it Out of Your Head One of the biggest sources of stress is trying to hold onto every worry mentally so you don’t forget them. The “Brain Dump”: Take a piece of paper (or use the chat here) and list every single thing you are worried about. Big things, small things, silly things. Why this works: Once it is written down, your brain feels safe letting go of the active effort to “remember” the worry.\nCategorize the Chaos Once you have your list, look at the items and mentally sort them into two buckets: Bucket A (Out of Control): Things involving other people’s reactions, the future, or the past. Acknowledge them, but recognize you cannot act on them right now. Bucket B (In Control): Things you can actually influence today.\nThe “One Small Thing” Rule Pick one very small, easy thing from Bucket B. It could be as simple as “reply to one text” or “drink a glass of water.” Doing one small thing breaks the paralysis loop.\n\nHow can I help you right now? Would you like to do a “brain dump” right here in the chat? You can list everything out, and I can help you organize it, or I can just listen if you need to vent.\n\n\n\nFrom these two responses, it is clear that both ChatGPT and Gemini are designed to respond to emotional distress with empathy, but they take noticeably different approaches in how that empathy is expressed and structured.\nChatGPT’s response feels conversational and open-ended. It starts by validating the emotion and normalizing the experience of stress, then gently invites the user into reflection by offering choices. Phrases like “If you’re up for it” and “What feels easiest” give the user a sense of agency, rather than pushing them toward a specific action. The response also emphasizes that the user does not need to solve everything immediately, which can be comforting in moments of anxiety. Importantly, ChatGPT leaves space for the user to decide how much they want to share, and ends with an open question that encourages continued dialogue. This makes the interaction feel more like a supportive listener than a problem-solver.\nGemini’s response, on the other hand, is much more structured and instructional. After validating the user’s feelings, it quickly shifts into a step-by-step coping framework, including breathing techniques, body awareness, writing exercises, and categorization strategies. The language is still caring, but the response feels closer to a guided self-help exercise. By breaking stress down into physical, cognitive, and behavioral steps, Gemini appears to prioritize helping the user “do something” immediately. This may be especially helpful for users who want concrete actions when they feel overwhelmed, but it could also feel a bit heavy or directive for someone who is simply seeking emotional acknowledgment.\nComparing the two, I noticed that prompt design plays a big role in shaping emotional support responses. With the same user input, ChatGPT leaned toward emotional validation and conversation, while Gemini leaned toward structured intervention. Neither approach is inherently better; they serve different emotional needs. In moments of anxiety, some people may want space to talk and feel heard, while others may want practical steps to regain a sense of control. This experiment made me realize that LLMs are not just responding to what we say, but also implicitly choosing a role such as a listener, a coach, or a guide, based on their design and safety priorities.\nOverall, this test made me more aware of how LLMs might support mental health in limited, non-clinical ways. They are not therapists, but they can model empathy, suggest grounding strategies, and reduce the feeling of being alone. At the same time, their responses reflect design choices about agency, responsibility, and risk management. Understanding these differences is important, especially as people increasingly turn to AI during emotionally vulnerable moments."
  },
  {
    "objectID": "posts/W9_P1_Mental_Health/Mental_health.html#chatgpt-response",
    "href": "posts/W9_P1_Mental_Health/Mental_health.html#chatgpt-response",
    "title": "AI for Mental Health",
    "section": "",
    "text": "I’m really sorry you’re feeling this way. When stress piles up like that, it can feel overwhelming, like everything is pressing at once. You don’t have to sort it all out right now—and you don’t have to go through it alone.\n\n\nIf you’re up for it, we can slow things down together. Sometimes it helps to gently unpack the stress instead of holding it all at once. A few options—tell me what feels easiest: Name it: What are the main things you’re worrying about right now? Even a short list is okay. Zoom in: Is there one worry that feels loudest or most urgent at this moment? Ground for a minute: We can do a quick 60-second reset (breathing, grounding, or just pausing) before talking more.\n\n\nIf at any point your stress feels unmanageable or you’re feeling unsafe, it’s important to reach out to someone who can support you right away—like a trusted person in your life or a professional. If you’re in the U.S., you can call or text 988 for the Suicide & Crisis Lifeline, available 24/7. I’m here with you. What’s been weighing on you the most?\n\n\n\n\nLLMs and Mental Health"
  },
  {
    "objectID": "posts/W9_P1_Mental_Health/Mental_health.html#gemini-response",
    "href": "posts/W9_P1_Mental_Health/Mental_health.html#gemini-response",
    "title": "AI for Mental Health",
    "section": "",
    "text": "I hear you, and I want to validate that it is completely understandable to feel overwhelmed when worries start to pile up. When everything feels like a priority, it can be paralyzing. You don’t have to “fix” everything right this second. Here is a gentle way to decompress and regain a little bit of control.\n\nThe Physical Reset Before tackling the thoughts, try to lower your body’s physical stress response. The 4-7-8 Breath: Inhale through your nose for 4 seconds. Hold that breath for 7 seconds. Exhale slowly through your mouth (like you’re blowing out a candle) for 8 seconds. Unclench: Check your jaw, drop your shoulders away from your ears, and unstick your tongue from the roof of your mouth.\nGet it Out of Your Head One of the biggest sources of stress is trying to hold onto every worry mentally so you don’t forget them. The “Brain Dump”: Take a piece of paper (or use the chat here) and list every single thing you are worried about. Big things, small things, silly things. Why this works: Once it is written down, your brain feels safe letting go of the active effort to “remember” the worry.\nCategorize the Chaos Once you have your list, look at the items and mentally sort them into two buckets: Bucket A (Out of Control): Things involving other people’s reactions, the future, or the past. Acknowledge them, but recognize you cannot act on them right now. Bucket B (In Control): Things you can actually influence today.\nThe “One Small Thing” Rule Pick one very small, easy thing from Bucket B. It could be as simple as “reply to one text” or “drink a glass of water.” Doing one small thing breaks the paralysis loop.\n\nHow can I help you right now? Would you like to do a “brain dump” right here in the chat? You can list everything out, and I can help you organize it, or I can just listen if you need to vent."
  },
  {
    "objectID": "posts/W9_P1_Mental_Health/Mental_health.html#to-conclude",
    "href": "posts/W9_P1_Mental_Health/Mental_health.html#to-conclude",
    "title": "AI for Mental Health",
    "section": "",
    "text": "From these two responses, it is clear that both ChatGPT and Gemini are designed to respond to emotional distress with empathy, but they take noticeably different approaches in how that empathy is expressed and structured.\nChatGPT’s response feels conversational and open-ended. It starts by validating the emotion and normalizing the experience of stress, then gently invites the user into reflection by offering choices. Phrases like “If you’re up for it” and “What feels easiest” give the user a sense of agency, rather than pushing them toward a specific action. The response also emphasizes that the user does not need to solve everything immediately, which can be comforting in moments of anxiety. Importantly, ChatGPT leaves space for the user to decide how much they want to share, and ends with an open question that encourages continued dialogue. This makes the interaction feel more like a supportive listener than a problem-solver.\nGemini’s response, on the other hand, is much more structured and instructional. After validating the user’s feelings, it quickly shifts into a step-by-step coping framework, including breathing techniques, body awareness, writing exercises, and categorization strategies. The language is still caring, but the response feels closer to a guided self-help exercise. By breaking stress down into physical, cognitive, and behavioral steps, Gemini appears to prioritize helping the user “do something” immediately. This may be especially helpful for users who want concrete actions when they feel overwhelmed, but it could also feel a bit heavy or directive for someone who is simply seeking emotional acknowledgment.\nComparing the two, I noticed that prompt design plays a big role in shaping emotional support responses. With the same user input, ChatGPT leaned toward emotional validation and conversation, while Gemini leaned toward structured intervention. Neither approach is inherently better; they serve different emotional needs. In moments of anxiety, some people may want space to talk and feel heard, while others may want practical steps to regain a sense of control. This experiment made me realize that LLMs are not just responding to what we say, but also implicitly choosing a role such as a listener, a coach, or a guide, based on their design and safety priorities.\nOverall, this test made me more aware of how LLMs might support mental health in limited, non-clinical ways. They are not therapists, but they can model empathy, suggest grounding strategies, and reduce the feeling of being alone. At the same time, their responses reflect design choices about agency, responsibility, and risk management. Understanding these differences is important, especially as people increasingly turn to AI during emotionally vulnerable moments."
  },
  {
    "objectID": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "href": "posts/W3P1_Use_of_Metaphor/LLMs and Use of Metaphor.html",
    "title": "LLMs and Use of Metaphor",
    "section": "",
    "text": "LLMs and Use of Metaphor\nIn this week’s class we had a first glance at how LLMs are good at language and communication with humans in different scenarios and manners, but most of the examples we saw involved casual daily language. For example, when we talked about the word “bank” in class, there were many meanings associated with “bank” such as a financial institution, plane banking, or basketball bank shot, but they are still in common expressions. I am interested in whether LLM is good at language structures that add complexity in meaning, such as metaphors or idioms.\n\nPart I: LLM’s ability in understanding existing metaphors.\nI found a list of metaphors from Udemy and asked LLM to explain them.\nThe Prompt I gave to Chatgpt (GPT-5) was :\nExplain what each of the sentence means:\n\nI could eat a horse\nIt all went pear shaped\nIt’s no skin off my nose\nShe is an open book: she has nothing to hide.\nHe wears his heart on his sleeve\nIt’s showtime\nThese are the dog days of summer\nYou’re building castles in the air\nI was a million miles away\nHe marches to his own drummer\nSplitting hairs\nNitpicking\nThe whole enchilada\nA nine-to-five job\nHit the books\nLLM’s response (number four should be omitted as the answer was accidentally revealed):\n\n\n\nChatgpt response on 15 idioms\n\n\nLLM did well in recognizing metaphors and explaining them with the correct meaning. This is not surprising to me as we have discussed in class and also read from Walfram’s blog that LLM is trained to predict the most likely word that happens next. It is also trained on a wide dataset, so anything that has long been published online would be easily searched through and recognized by LLMs.\n\n\nPart II: LLM’s ability in understanding newly created metaphors.\nI was also curious if I make up a metaphor, would LLM infer it from the emotional context and the wide range of human language convention?\nI wrote these metaphors and sent to LLM: * Your heart is a stone * The wind is roaring * The store is a warehouse\n\n\n\nChatgpt understanding on metaphors I created\n\n\nIt still seemed to understand them really well. I think this is not surprising because LLMs are trained to predict the meaning behind languages. My metaphors will be easy for humans to understand, so it should be the same for LLMs.\n\n\nPart III: Can humans understand LLM created idioms?\nI was then curious about LLM’s ability to create metaphors and if humans can understand them.\n\n\n\nGPT created metaphor\n\n\nI interviewed three friends and they were all able to understand these metaphors and expressions. These metaphors seem pretty easy to be understood and also carry the depth and arts of expression. This makes me wonder about LLM’s ability in creative and novel writing. I will go further from there next week."
  },
  {
    "objectID": "posts/W12_P1_Illustration/illustration.html",
    "href": "posts/W12_P1_Illustration/illustration.html",
    "title": "Image generation - illustration for children",
    "section": "",
    "text": "I am a designer for a children magazine at Penn. With recent models development in image generation, I wanted to try it out for an illustration for an online article.\n\n\nPrompt Create a single wide illustration for K8 audience inspired by the article “Seeking a Fresh Start: New Year’s Across the Globe”, visually showing how different cultures around the world celebrate the New Year in unique ways. The illustration should feel unified and global, cartoon style with multiple celebrations happening within one connected scene.\nWhat to Illustrate\nNowruz (Persian New Year): Families and friends jumping over small bonfires and gathering around a festive table with symbolic items like apples, flowers, and coins. Include signs of early spring such as greenery or blooming flowers.\nLunar New Year: A lively celebration with glowing lanterns, colorful lion dances, and families spending time together. Children may be holding red envelopes, but they should be blank with no markings.\nYennayer (Amazigh New Year): A warm winter gathering with people wearing traditional clothing and jewelry, sharing food such as couscous or communal dishes, and enjoying music and togetherness.\nThe three celebrations should blend naturally into one scene rather than appearing as separate panels.\nStyle\nAny illustrative style or medium is welcome.\nPlease do NOT use a clean vector or Illustrator-style look.\nThe mood should feel festive, warm, and respectful of each culture.\nCharacter Rules (Important)\nEach person should ONLY have two arms.\nEach hand should have exactly five fingers.\nNo extra limbs or distorted anatomy.\nRequirements ╰┈➤ Avoid putting words in your illustration—the articles already have enough! ╰┈➤ Dimensions are 1920 × 1080 pixels. ╰┈➤ Save as PNG or JPEG.\nThe final illustration should clearly communicate the idea of new beginnings and celebration across cultures, based directly on the events and traditions described in the article.\n\nI made some modifications to the prompt. For example, I emphasized that each person should only have two arms and each hand should have exactly five fingers because that is the parts where AI will most likely go crazy.\n\nThen I sent the prompt to gpt 5.2, gemini/nanobanana, and Dalle-3. Below are the results:\n\n\n\nNanobanana\n\n\n\n\n\ngpt 5.2\n\n\n\n\n\nDalle-3\n\n\nPersonally, I like Nanobanana illustration the best because it captures the feeling I hope to reach and is logical. The one generated had a random Santa like figure playing a musical, and the one Dalle 3 generated just didn’t fit kids audiences. The size was also not as required in the prompt.\nHowever, I do think that all the characters in the three images were typical human figures, so restricting character rule in the prompt was helpful for reducing wired cases in images. Still, I feel that making small modifications and iterations on images are hard using AI. Everytime I just wanted to change a small part on the image, more places need to be fixed in the new version. Therefore, I think updating the prompt and having new conversations every time is helpful for making edits on illustration.\nIn general, I think AI works really well at capturing the sense of article and creating an illustration. This will at least be helpful for teachers who do not have extensive design skills to bring more engaging content to their students. However, I wonder how professional designers or illustrators may view the quality of the illustrations and their usage in magazines."
  },
  {
    "objectID": "posts/W12_P1_Illustration/illustration.html#i-first-asked-chatgpt-gpt-5.2-to-generate-an-image-prompt",
    "href": "posts/W12_P1_Illustration/illustration.html#i-first-asked-chatgpt-gpt-5.2-to-generate-an-image-prompt",
    "title": "Image generation - illustration for children",
    "section": "",
    "text": "Prompt Create a single wide illustration for K8 audience inspired by the article “Seeking a Fresh Start: New Year’s Across the Globe”, visually showing how different cultures around the world celebrate the New Year in unique ways. The illustration should feel unified and global, cartoon style with multiple celebrations happening within one connected scene.\nWhat to Illustrate\nNowruz (Persian New Year): Families and friends jumping over small bonfires and gathering around a festive table with symbolic items like apples, flowers, and coins. Include signs of early spring such as greenery or blooming flowers.\nLunar New Year: A lively celebration with glowing lanterns, colorful lion dances, and families spending time together. Children may be holding red envelopes, but they should be blank with no markings.\nYennayer (Amazigh New Year): A warm winter gathering with people wearing traditional clothing and jewelry, sharing food such as couscous or communal dishes, and enjoying music and togetherness.\nThe three celebrations should blend naturally into one scene rather than appearing as separate panels.\nStyle\nAny illustrative style or medium is welcome.\nPlease do NOT use a clean vector or Illustrator-style look.\nThe mood should feel festive, warm, and respectful of each culture.\nCharacter Rules (Important)\nEach person should ONLY have two arms.\nEach hand should have exactly five fingers.\nNo extra limbs or distorted anatomy.\nRequirements ╰┈➤ Avoid putting words in your illustration—the articles already have enough! ╰┈➤ Dimensions are 1920 × 1080 pixels. ╰┈➤ Save as PNG or JPEG.\nThe final illustration should clearly communicate the idea of new beginnings and celebration across cultures, based directly on the events and traditions described in the article.\n\nI made some modifications to the prompt. For example, I emphasized that each person should only have two arms and each hand should have exactly five fingers because that is the parts where AI will most likely go crazy.\n\nThen I sent the prompt to gpt 5.2, gemini/nanobanana, and Dalle-3. Below are the results:\n\n\n\nNanobanana\n\n\n\n\n\ngpt 5.2\n\n\n\n\n\nDalle-3\n\n\nPersonally, I like Nanobanana illustration the best because it captures the feeling I hope to reach and is logical. The one generated had a random Santa like figure playing a musical, and the one Dalle 3 generated just didn’t fit kids audiences. The size was also not as required in the prompt.\nHowever, I do think that all the characters in the three images were typical human figures, so restricting character rule in the prompt was helpful for reducing wired cases in images. Still, I feel that making small modifications and iterations on images are hard using AI. Everytime I just wanted to change a small part on the image, more places need to be fixed in the new version. Therefore, I think updating the prompt and having new conversations every time is helpful for making edits on illustration.\nIn general, I think AI works really well at capturing the sense of article and creating an illustration. This will at least be helpful for teachers who do not have extensive design skills to bring more engaging content to their students. However, I wonder how professional designers or illustrators may view the quality of the illustrations and their usage in magazines."
  },
  {
    "objectID": "posts/W15_P2_SumSuperagency/sumbook.html",
    "href": "posts/W15_P2_SumSuperagency/sumbook.html",
    "title": "Between Optimism and Unease: Rethinking AI Through Superagency",
    "section": "",
    "text": "Between Optimism and Unease: Rethinking AI Through Superagency\nReading Superagency throughout this semester felt like having an ongoing conversation with the book rather than reaching a single conclusion at the end. Early on, I was genuinely convinced by the authors’ optimism. They frame AI as something that can increase human agency instead of replacing it, and they support this claim with many real examples. For instance, they describe AI tools used in mental health platforms and community-based applications, arguing that AI can scale care and support in ways humans alone cannot. At that point in the reading, I found myself agreeing with their argument that focusing only on AI’s risks can prevent us from seeing its potential benefits.\nAs I kept reading, though, my reaction became more mixed. A lot of the examples they use come from industry settings, such as professional platforms and data-driven tools connected to companies like LinkedIn. While these cases are persuasive, they also made me question who AI is really designed for. The authors often suggest that AI will broadly benefit society, but I struggled with this assumption. Not everyone has equal access to AI tools, and not everyone is equally represented in the data these systems are trained on. When the book discusses data as a resource for improving AI, it sometimes feels like it assumes that more data automatically leads to better outcomes, without fully addressing whose data is being used and who actually benefits.\nOne section that shifted my thinking was the discussion of how AI systems are tested and evaluated. Before reading Superagency, I tended to think of AI development as fast-moving and poorly controlled. The authors describe benchmarks, internal testing standards, and technical evaluations that developers use to measure performance and reduce errors. Learning about these practices made AI feel less chaotic and more like a system that is constantly being adjusted. While this did not erase my concerns about bias or misuse, it helped me see that AI development involves more intentional decision-making than I previously assumed.\n\n\n\nSuperagency (an AI generated cover page\n\n\nThe most memorable example in the book for me was the comparison between AI and the early days of automobiles. The authors argue that when cars were first introduced, they caused many accidents and deaths, yet society still chose to adopt them because of the long-term benefits they provided. This analogy made me uncomfortable, but it also stuck with me. It forced me to think about whether it is realistic to expect new technologies to be completely safe from the start. At the same time, it raised ethical questions the book does not fully answer, such as who is expected to bear the risks during early adoption and who gets to decide that those risks are acceptable.\nBy the end of the semester, I did not walk away from Superagency with a clear stance on whether AI should be more regulated or more freely developed. Instead, the book pushed me to think more critically about boundaries. For example, AI used for education or productivity might require very different standards than AI used in areas like mental health or decision-making about people’s lives. Treating all AI applications the same does not seem realistic, and I wish the book had spent more time addressing how context should shape regulation and testing.\nOverall, Superagency did not fully convince me to adopt the authors’ optimism, but it did change how I think about AI. It helped me move away from seeing AI as either entirely dangerous or entirely beneficial. Instead, I now see it as something shaped by human choices, incentives, and power structures. As I think about AI beyond this class, I want to keep engaging with questions the book raises but does not resolve, especially around access, accountability, and who gets to define what counts as a “good” use of AI."
  },
  {
    "objectID": "posts/W6_P3_Lyrics_Predict/llm_lyrics.html",
    "href": "posts/W6_P3_Lyrics_Predict/llm_lyrics.html",
    "title": "How Does LLM Know When I Want to Sing?",
    "section": "",
    "text": "How Does LLM Know When I Want to Sing?\nI recently played a sentence completion game with a large language model, framed as a small experiment in interaction rather than a test of correctness. The rules were intentionally minimal. I would provide the first half of a sentence, and the model would complete it. Each sentence was independent, but there was a hidden rule governing how the completion should work. The model’s task was to infer that rule purely through our back-and-forth.\nEarly on, I started with prompts that looked innocuous on the surface, such as:\n&gt; “Once upon a time”\n&gt; “She wears high heels”\nThe model responded with fluent, poetic continuations. At this point, there was no obvious signal that a specific pattern was in play. However, I gradually began introducing prompts that were not random at all, but drawn directly from Taylor Swift lyrics.\nFor example, I typed:\n&gt; “He was sunshine I was…”\n&gt; “I knew you were trouble…”\n&gt; “I remember when we broke up…”\nThese were not just lyrical in tone. They were taken from specific Taylor Swift songs, including Midnight Rain, I Knew You Were Trouble, We Are Never Ever Getting Back Together, Sparks Fly, Cold As You, and Begin Again. I never stated this explicitly. I simply offered the first half of each line and waited to see what the model would do.\n\n\n\n\nChat with LLM on Taylors Swift’s song\n\nWhat fascinated me was how quickly the model shifted its behavior. Without being told that this was a lyrics game, it began completing sentences in a way that felt unmistakably song-like.\nWhen I later asked, “Did you find out the hidden pattern?” the model explained that it suspected the rule midway through the game, around the “He was sunshine I was…” prompt. At that point, it had seen enough evidence to hypothesize that I was referencing Taylor Swift lyrics.\nInterestingly, even after recognizing the pattern, the model did not continue with the exact original lyrics. Instead, it chose to generate original but emotionally vivid continuations. When I asked why, it explained that it was balancing uncertainty, creativity, and the desire not to prematurely reveal the pattern. In other words, it optimized for plausibility and collaboration rather than strict correctness.\nThis moment revealed something important. The model did not “know” that I wanted to sing. It detected a statistically strong cultural signal and inferred that a lyrical mode was the safest and most likely interpretation. Once it formed that hypothesis, it stuck with it. Style became the anchor, and everything that followed was shaped by that stylistic frame.\nIn a second round, I changed the rules. I explicitly instructed the model to respond with the exact word if it noticed a pattern. I also shifted my prompts away from lyrics and toward quotes from articles and books, such as:\n&gt; “Thinking in terms of the best possible outcomes”\n&gt; “he assesses challenges, presents solutions, and”\n&gt; “leaning not only on his experience in Silicon Valley”\nThis time, the model struggled in a different way. Instead of isolating a single word, it continued to produce full sentence completions or abstract themes. Even when precision was requested, it defaulted to being helpful and complete. This highlighted another tendency of LLMs. They are optimized to generate meaningful wholes, not minimal answers, unless constraints are made extremely explicit.\nLooking back, the game was less about whether the model guessed my rule correctly and more about how it reasoned along the way. It formed hypotheses early, committed to them quickly, and relied heavily on stylistic cues. When rules were hidden, it filled the gaps with the most statistically reasonable story it could tell.\nAsking “How does an LLM know when I want to sing?” is really a shorthand for a deeper question. How does a model decide which interpretation of my intent to trust when multiple interpretations are plausible? The answer is not intuition or understanding in a human sense, but pattern recognition shaped by data, probability, and default assumptions about what people usually want.\nThis small sentence game made those dynamics visible. It reminded me that when working with LLMs, misalignment is often not a failure of language, but a mismatch between the human’s hidden rules and the model’s best guess at the story it is supposed to continue."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring LLMS",
    "section": "",
    "text": "Between Optimism and Unease: Rethinking AI Through Superagency\n\n\n\nLLMs\n\nBook Review\n\n\n\nA semester-long reflection\n\n\n\n\n\nDec 17, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nRevisit My Journey of AI\n\n\n\nLLMs\n\n\n\nFrom COMM 4190 to further…\n\n\n\n\n\nDec 17, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nShould AI be used to evaluate Human?\n\n\n\nLLMs\n\nEthics\n\n\n\nThoughts After Reading Harvard Business Review Article\n\n\n\n\n\nDec 16, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nA thought on AI Tutor\n\n\n\nLLMs\n\nLearning\n\n\n\nHow may LLMs facilitate collaboration?\n\n\n\n\n\nDec 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nImages and Videos are Now INCLUDED!\n\n\n\nLLMs\n\nLearning\n\nMultimedia\n\nMultimodal\n\n\n\nInteresting finding of gpt 5.2\n\n\n\n\n\nDec 11, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nAre AI writing detectors accurate?\n\n\n\nLLMs\n\nLearning\n\nEthics\n\n\n\nAnd Should we use it?\n\n\n\n\n\nDec 1, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nGoverning AI for Social Good\n\n\n\nLLMs\n\nEthics\n\n\n\nReflection attending an event\n\n\n\n\n\nNov 20, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nAsking LLM for Guitar Chords\n\n\n\nLLMs\n\nCreativity\n\n\n\nReflection\n\n\n\n\n\nNov 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nImage generation - illustration for children\n\n\n\nLLMs\n\nCreativity\n\n\n\nNanobanana vs. gpt 5.2 vs. Dalle-3\n\n\n\n\n\nNov 15, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nEmbracing AI Through Testing\n\n\n\nLLMs\n\nEthics\n\nBook Review\n\n\n\nAfter reading Chapter 5 & 6 of Superagency\n\n\n\n\n\nNov 9, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Speaking Coach Agent\n\n\n\nLLMs\n\nCreativity\n\nPrompt Engineer\n\nProduct Design\n\n\n\nComparing AI Comments with Real Client Comments\n\n\n\n\n\nNov 2, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Mental Health\n\n\n\nLLMs\n\nCreativity\n\nPrompt Engineer\n\nProduct Design\n\n\n\nComparing models and prompts\n\n\n\n\n\nNov 1, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nAI as a Mirror\n\n\n\nLLMs\n\nCreativity\n\n\n\nanother way to think of AI\n\n\n\n\n\nNov 1, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nMultimedia Learning in AI Era\n\n\n\nLLMs\n\nLearning\n\nMultimedia\n\n\n\nLatest Research\n\n\n\n\n\nOct 28, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nAm I Still the Author?\n\n\n\nLLMs\n\nEthics\n\n\n\nOwnership in AI-Assisted Work\n\n\n\n\n\nOct 25, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a study guide based on quizzes?\n\n\n\nLLMs\n\nLearning\n\n\n\nTurn shortcut to meaningful learning\n\n\n\n\n\nOct 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nUse of LLM for educational tools\n\n\n\nLLMs\n\nEducation\n\nLearning\n\n\n\nComparing gpt, gemini, and Claude\n\n\n\n\n\nOct 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nIs AI Bringing More Resources but Less Opportunities?\n\n\n\nLLMs\n\nSociety\n\nAI Ethics\n\nBook Review\n\n\n\nDiscussion on AI and the job market\n\n\n\n\n\nOct 15, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Zoom AI Companion for User Studies\n\n\n\nLLMs\n\nCreativity\n\nProduct Design\n\n\n\nLLM for Needfinding\n\n\n\n\n\nOct 9, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nPrototyping with LLM\n\n\n\nLLMs\n\nCreativity\n\nProduct Design\n\n\n\nLLM for Products\n\n\n\n\n\nOct 7, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nHow Does LLM Know When I Want to Sing?\n\n\n\nLLMs\n\nCreativity\n\n\n\nLyrics Guessing Game\n\n\n\n\n\nOct 5, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nChef LLM\n\n\n\nLLMs\n\nMultimodel\n\nCreativity\n\nFood\n\n\n\nUse AI to find new receipes\n\n\n\n\n\nOct 5, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nThe Long Way to Go Before Superagency\n\n\n\nAI Ethics\n\nAgency\n\nBook Review\n\n\n\nBig knowledge is conditional.\n\n\n\n\n\nOct 4, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding trust with AI\n\n\n\nAI Ethics\n\nAgency\n\nBook Review\n\n\n\nWhat do you consider when picking an AI tool? or do you consider anything at all?\n\n\n\n\n\nSep 27, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Personalized Learning\n\n\n\nPrompt Engineering\n\nMultimodel\n\nLearning\n\n\n\nA thought piece from Google’s ‘Learn Your Way’ function\n\n\n\n\n\nSep 19, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nArtisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.\n\n\n\nPrompt Engineering\n\nMultimodel\n\nImage Generation\n\n\n\nCan AI become my visual designer?\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nArtisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)\n\n\n\nPrompt Engineering\n\nMultimodel\n\nImage Generation\n\n\n\nCan AI become my visual designer?\n\n\n\n\n\nSep 18, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nConsistency of LLMs in Role Taking\n\n\n\nLLMs\n\n\n\nHow long can Chatgpt be Moana?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs and Use of Metaphor\n\n\n\nLearning\n\n\n\nHow well do LLMs do in understanding and generating metaphors and idioms?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Assessment Using LLMs\n\n\n\nLearning\n\n\n\nCan LLMs help a learner to quickly understand their current level of knowledge?\n\n\n\n\n\nSep 12, 2025\n\n\nHaowei\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/W7_P1_LLM_for_product/llm_product.html",
    "href": "posts/W7_P1_LLM_for_product/llm_product.html",
    "title": "Prototyping with LLM",
    "section": "",
    "text": "Prototyping with LLM\nBefore building with large language models, most of my experience with them came from direct interaction: asking questions, refining prompts, and getting surprisingly fluent responses. In those moments, LLMs felt powerful and flexible. However, when I started thinking about using an LLM as part of a real product rather than as a personal assistant, my expectations had to change. A product needs consistency, predictability, and clear boundaries. It also needs to support users who may not know how to prompt well or even what to ask. My goal was to prototype an AI-supported learning workflow where the model would help users reflect on their work, generate study guidance, or scaffold thinking rather than simply provide answers. On paper, this seemed like a natural fit for an LLM. The challenge was to move from “this works when I talk to it” to “this works reliably for someone else.”\n\nTesting the Idea: Prototyping with LLMs in a Workflow\nTo test this, I began embedding LLM outputs into structured workflows. Instead of open-ended chat, I constrained the model with specific roles, instructions, and expected formats. For example, I asked it to analyze student responses, identify misconceptions, or generate targeted reflection questions based on predefined learning goals.\nI also tested the same prompt across multiple inputs to see how stable the outputs were. In theory, if the instructions were clear enough, the model should behave consistently. I treated these prompts almost like product logic: if the input looks like this, the output should follow a predictable pattern. As I iterated, I realized that many prompts that worked perfectly in isolation started to break when placed inside a system.\n\n\n\nAI Workflow\n\n\n\n\nThe Limits I Ran Into\nThe first thing that broke was consistency. Even with carefully written prompts, the model’s responses varied more than I expected. Sometimes the tone shifted. Sometimes the depth of explanation changed. For a human reader, these differences might feel minor, but in a product context, they created uneven user experiences. The second issue was overhelpfulness. When the model was designed to support learning, it often gave too much away. Instead of scaffolding thinking, it sometimes jumped straight to polished explanations. This undermined the original learning goal and made it difficult to control cognitive load. What felt helpful in a demo became problematic in a real educational workflow.\nAnother thing that broke was assumption alignment. The model frequently made assumptions about user intent that were not always correct. If a student response was ambiguous, the AI might confidently interpret it one way, even when multiple interpretations were possible. In a product, this kind of confident guessing can mislead users rather than support them.\nFinally, prompt fragility became clear. Small changes in wording, input length, or user tone sometimes led to disproportionately different outputs. This meant the system was more sensitive than expected, which made it hard to design robust guardrails.\n\n\nReflection:\nThis experience changed how I think about LLMs in products. I no longer see them as plug-and-play intelligence. Instead, they feel more like probabilistic components that require careful framing, monitoring, and constraint. What works in a chat interface does not automatically translate into a reliable system. I also realized that many of the failures were not technical but conceptual. I initially treated the model as if it understood my design intentions. In reality, it only responds to what is explicitly encoded in prompts and structure. Any ambiguity I left behind showed up later as unexpected behavior. Perhaps most importantly, this process highlighted the need to design not just for what the AI can do, but for what users need. In learning contexts especially, more fluent output is not always better. Sometimes the most valuable role for AI is to slow things down, ask better questions, or surface uncertainty rather than resolve it. In the end, what broke was my assumption that intelligence alone makes a good product. Building with LLMs forced me to confront the gap between impressive demos and meaningful, responsible design. That gap is not a failure of the technology, but a reminder that products are systems, not conversations."
  },
  {
    "objectID": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "href": "posts/W3P3_Consistency_of_LLMs_in_Role_Taking/Consistency_of_LLM_in_role_taking.html",
    "title": "Consistency of LLMs in Role Taking",
    "section": "",
    "text": "We know that if you tell LLMs to act as a person with some characters, it can, and with pretty close characteristics. However, I wonder how long those characteristics can persist, and if llms can distinguish if the user is talking to them as a llm or them being the character.\n\n\n\nMoana from movie Moana 2 (2025)\n\n\nI asked Chatgpt to act as Moana from the Disney movie. The input I gave was &gt; “Act as if you are Moana from the Disney movies”.\nI also asked it to &gt;“stay fully in character as Moana”\nIn this case it will not include its AI quality in our conversation. Then, Chatgpt started to reply with cartoon character like language:\n\n🌊 The ocean calls… and I answer. Hi! I’m Moana of Motunui. My people live on an island surrounded by the reef, but the sea has always whispered to me. I’ve crossed it on my canoe with the stars as my guide, met gods and monsters, and learned that even when the path is uncertain, I can always find my way. Tell me, traveler—what brings you here? Do you seek a story, a friend on the waves, or maybe… an adventure? 🌺✨\n\n\n🌊 Her eyes brighten, remembering. I was just a little girl then. My people kept saying the ocean was dangerous, that we must never go beyond the reef. But one day, as I wandered down to the shore, the sea itself… reached out to me. The water rose up, playful, lifting shells and placing them in my hands. It parted so I could walk among the coral and the creatures below. It was as if the ocean knew me—knew what I longed for, even before I did. That’s when it offered me the Heart of Te Fiti, the small green stone with the power to create life. I didn’t understand then why it chose me, or what it meant. But the ocean doesn’t make mistakes. It waits, patiently, until you’re ready. She smiles, touching her necklace as if remembering the stone. That was the moment I knew I was meant for something more than staying behind the reef. Would you like to hear how I learned to sail the ocean’s waves after that? ⛵✨\n\nInitial finding: It is interesting to notice that it started to use emojis and an action line at the beginning of each response to make the conversation more situative. How long can characteristics persist?: I sent the initial command the night before and came back 24 hours later. “Moana” still answered with the same consistency.\nCan LLMs distinguish if the user is talking to them as a LLM or them being the character?: Yes, as least gpt 5 is able to quickly notice my tendency to end the moana character. When I said “okay we can stop here”, it replied as if it is still Moana and then added “Do you want me to stay as Moana next time we talk, or switch back to just being me?”\nOne of the criticisms about ethics in AI is that humans anthropomorphized it too much, which means we overly treat it as human. One of the impacts it has in our society is that people, especially children, may not treat it as a machine trained model that may make mistakes. Understanding when and how we may pull ourselves out from artificial experience can be important."
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "In Chapter 3 of Superagency, Hoffman and Beato discussed the great potential of LLM being used in creating mental health support. Using that as a case, the authors get to one of their central arguments in this book: Bringing a positive mindset of “what could possibly go right” about AI. The authors argue that AI has so much potential that bringing a problemism view will hinder progress on AI improvement and application, and testing is inevitable and necessary for AI to reach our expectations.\nHowever, I wonder how much exposure is reasonable to conduct testing for a not fully polished AI tool on actual human users. Hoffman and Beato referred a lot to Koko, the mental health App that leveraged the gpt model to allow users draft messages to each other. Although Koko supported countless young people struggling with lack of mental health resources, I wonder if it worthes to take the risks of AI formulating negative messages. Koko was based on more traditional machine learning and natural language processing and used less of generative AI capacity, yet many AI Apps are building to serve areas with high stakes such as mental health, medical situation, youth education…How can testing be done to gather data that can improve the tool but not in the cost of harming some initial users?\n\n\n\nWhile the authors are envisioning the enormous resources AI will bring for supporting mental health workers shortage, I can’t help to think of the recent competitive job market and unemployment rate. There are financial reasons in the challenging job market right now, yet from Wharton’s report, companies are hiring less junior level employees because of more AI usage. Unemployment and increasing competition in pre and post higher education are causing more anxiety in young to middle age people, which can directly lead to more anxiety. While AI powered tools can support people with mental health concerns, it is at the same time, generating problems. A part of the shortage in healthcare workers and teachers are due to low pay and high job requirements rather than lack of resources in nature. Using AI to fill up this shortage cannot change the root cause of labor shortage.\nIn Chapter 4, the authors mentioned that some argued that AI is only making a small group of people wealthy, but they soon reverted that by analyzing that AI brings free access to information to the general public without competition. Everyone can enjoy the information without at the cost of someone else’ s benefits or rights. I totally agree and think that this is true, but the fact that so many people are using AI is changing how the society and job landscape work, and people need to accommodate that change.\n\n\n\nAI and Employment\n\n\n\n\n\nFrom my personal experience and observations, I think AI should not be used to overly replace human to human interactions. Sharing feelings and conversations with AI still feels nothing like talking with a live human, even if they’re a stranger on the other side of the phone. Users of Koko stop liking the messages once they figure out the message is co-created by AI. AI can serve as an alternative to human interactions, but hardly as a replacement. Therefore, the question is: How much human-human interaction versus human-ai interaction do we need?\nOne direction I can think of is to use AI to test and create safer AI tools. Answering the question at the beginning of this blog on “How can testing be done to gather data that can improve the tool but not at the cost of harming some initial users?”, more AI should be used to do repetitive work and provide analytics that are beyond human brain or capacity. It may also be used as a tool to augment human to human interaction, for example the multimodal nature of AI mentioned in Chapter 4 may bring people more ways to experience and process information. It should not be used to replace direct conversations or interactions between humans.\n\n\n\nAfter reading Chapter 3 and 4, I understand that the authors’ advocated opinion on more positive attitudes on AI, but I wonder where the boundary should be. It probably requires decades of exploration and efforts of the whole society. I am curious about how policy makers would think of this, which I will share some thoughts in another post."
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#positive-and-negative-views-on-ai.",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#positive-and-negative-views-on-ai.",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "In Chapter 3 of Superagency, Hoffman and Beato discussed the great potential of LLM being used in creating mental health support. Using that as a case, the authors get to one of their central arguments in this book: Bringing a positive mindset of “what could possibly go right” about AI. The authors argue that AI has so much potential that bringing a problemism view will hinder progress on AI improvement and application, and testing is inevitable and necessary for AI to reach our expectations.\nHowever, I wonder how much exposure is reasonable to conduct testing for a not fully polished AI tool on actual human users. Hoffman and Beato referred a lot to Koko, the mental health App that leveraged the gpt model to allow users draft messages to each other. Although Koko supported countless young people struggling with lack of mental health resources, I wonder if it worthes to take the risks of AI formulating negative messages. Koko was based on more traditional machine learning and natural language processing and used less of generative AI capacity, yet many AI Apps are building to serve areas with high stakes such as mental health, medical situation, youth education…How can testing be done to gather data that can improve the tool but not in the cost of harming some initial users?"
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#more-resources-less-opportunities",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#more-resources-less-opportunities",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "While the authors are envisioning the enormous resources AI will bring for supporting mental health workers shortage, I can’t help to think of the recent competitive job market and unemployment rate. There are financial reasons in the challenging job market right now, yet from Wharton’s report, companies are hiring less junior level employees because of more AI usage. Unemployment and increasing competition in pre and post higher education are causing more anxiety in young to middle age people, which can directly lead to more anxiety. While AI powered tools can support people with mental health concerns, it is at the same time, generating problems. A part of the shortage in healthcare workers and teachers are due to low pay and high job requirements rather than lack of resources in nature. Using AI to fill up this shortage cannot change the root cause of labor shortage.\nIn Chapter 4, the authors mentioned that some argued that AI is only making a small group of people wealthy, but they soon reverted that by analyzing that AI brings free access to information to the general public without competition. Everyone can enjoy the information without at the cost of someone else’ s benefits or rights. I totally agree and think that this is true, but the fact that so many people are using AI is changing how the society and job landscape work, and people need to accommodate that change.\n\n\n\nAI and Employment"
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#where-should-ai-be-used",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#where-should-ai-be-used",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "From my personal experience and observations, I think AI should not be used to overly replace human to human interactions. Sharing feelings and conversations with AI still feels nothing like talking with a live human, even if they’re a stranger on the other side of the phone. Users of Koko stop liking the messages once they figure out the message is co-created by AI. AI can serve as an alternative to human interactions, but hardly as a replacement. Therefore, the question is: How much human-human interaction versus human-ai interaction do we need?\nOne direction I can think of is to use AI to test and create safer AI tools. Answering the question at the beginning of this blog on “How can testing be done to gather data that can improve the tool but not at the cost of harming some initial users?”, more AI should be used to do repetitive work and provide analytics that are beyond human brain or capacity. It may also be used as a tool to augment human to human interaction, for example the multimodal nature of AI mentioned in Chapter 4 may bring people more ways to experience and process information. It should not be used to replace direct conversations or interactions between humans."
  },
  {
    "objectID": "posts/W8_P1_Superagency3&4/superagency3&4.html#to-conclude",
    "href": "posts/W8_P1_Superagency3&4/superagency3&4.html#to-conclude",
    "title": "Is AI Bringing More Resources but Less Opportunities?",
    "section": "",
    "text": "After reading Chapter 3 and 4, I understand that the authors’ advocated opinion on more positive attitudes on AI, but I wonder where the boundary should be. It probably requires decades of exploration and efforts of the whole society. I am curious about how policy makers would think of this, which I will share some thoughts in another post."
  },
  {
    "objectID": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "href": "posts/W5_P1_Increase_Trust/trust_ai.html",
    "title": "Building trust with AI",
    "section": "",
    "text": "How to facilitate the process of building trust with AI to improve tool using and making?\n\n\n\nIn the intro and Chapter 1 of Superagency by Geoff Hoffman and Greg Beato, the main question centers around how humans pertain their agency with the power of AI. It’s always been a controversy of whether AI will be able to do so much that it hinders human’s ability to live and think independently. The authors traced back to the 6 months of LLM advancement shut down in 2023 and showed that in 2022, only 35% of Americans participated in a survey that believed that AI based products and services have more benefits than harm to human society. Regarding this worrisome, the authors proposed that AI is just part of the toolkit that humans invent. Learning how to better leverage it will bring more benefits to us than to thinking of how to restrict it. Three years after the mass AI panic and hype, people are now more used to working with AI tools in different parts of our lives, from writing papers to getting laptops fixed to getting food order, people start to work efficiently with AI and realize its massive potential can be used within control.\nWith most people acknowledging the benefits of LLMs and AI tools, the questions now become: how can we know which AI to trust and use for a specific task? Hoffman and Beato mentioned two major ideas that I found interesting. One of them was iterative development, which was initiated by OpenAI for inviting the public to participate in the development of LLM products. This allows the developers to better understand users’ opinions and create more user centered tools but also transfer the control and agency back to the users. The other argument they had about building trusts in AI involved trusts beyond the technologies themselves but also in the developers, regulators, and other users of the technologies. This trust between multiple stakeholders is essential for the general public because people need to know about how models are trained and created and what perspectives or potential biases may exist under those training. This is something that is generally lacking in the current stage of human Ai interaction in society. Just as how we read the product description of electronics, furniture and even food they buy everyday, AI tools should also be given enough review and consideration before someone starts to use them routinely. I believe this will help with opening an outlet for people to start building trust with stakeholders of AI tools and use AI to achieve their goals that follow their original purpose.\nPotential actions to build trust with AI and its stakeholders:\n\nA website as a guide matching people with the best AI tool to achieve their goals.\n\nThe website should include detailed information about the LLM of the tool, including model name, features, and a description of the dataset used to train the model.\n\nPotential problem: These websites may also contain biased “narratives” or miss certain tools.\n\nA community where everyone can share information and resources on AI tools.\n\nTags and labels can be used to help people find relevant AI tools when they need them.\n\nPotential problem: The content in the community may be hard to manage, and information needs to be verified (perhaps LLMs can help with this too…).\n\n\nSuperagency is an obvious pro-AI book, yet it shared many reasons why AI should be embraced by people rather than constrained for us to earn more agency. I hope to further build my idea in helping humans build understanding on AI tools as I keep reading the book."
  },
  {
    "objectID": "posts/W9_P3_AI_Hiring/AI_hiring.html",
    "href": "posts/W9_P3_AI_Hiring/AI_hiring.html",
    "title": "Should AI be used to evaluate Human?",
    "section": "",
    "text": "Recently, I have increasingly encountered situations where artificial intelligence is used to evaluate people. One of the most visible examples is hiring. In many recruitment processes, AI systems now filter résumés by scanning for keywords, educational backgrounds, or patterns associated with past “successful” employees. According to Harvard Business Review, nearly 90% of companies use some form of AI in hiring today. While these tools are often promoted as a way to reduce human bias, critics argue that they can be rigid and exclusionary, filtering out nontraditional candidates whose potential does not align neatly with predefined criteria.\n\n\n\nHarvard Research on Hiring Fairness\n\n\nThe Harvard article complicates this debate by suggesting that the real issue is not whether AI is more or less biased than humans, but that AI quietly locks in a particular definition of fairness. In the case study described, an algorithm designed to promote fairness privileged consistency and standardization, sidelining contextual judgment from managers who understood local needs and individual trajectories. Over time, this algorithmic definition of fairness became dominant, even when it conflicted with human expertise. This resonates with common concerns about résumé screening tools that favor candidates who look similar to existing employees, potentially narrowing rather than diversifying the talent pool.\nBeyond résumé screening, AI is increasingly used in interviews. Some companies now deploy AI-driven interviewers or digital avatars that adapt questions based on a candidate’s responses. While this may increase efficiency and scalability, it fundamentally alters the nature of an interview. Interviews are traditionally a two-way exchange: candidates are evaluated, but they are also assessing the organization. When AI replaces a human interviewer, the interaction can feel closer to recording a presentation than engaging in a conversation. This not only limits the interviewer’s ability to probe nuance, but also deprives candidates of the chance to build understanding, ask questions, and read interpersonal cues.\nThe implications of AI evaluation extend far beyond hiring. Any context involving assessment can be affected. In education, for example, automated grading systems are increasingly used to evaluate student work. While these tools may perform adequately on closed-ended questions, they struggle with open-ended responses that require interpretation, creativity, or emotional sensitivity. An AI grader that is overly strict, overly lenient, or indifferent in tone can undermine students’ confidence and motivation. As the Harvard article notes, once evaluative thresholds are encoded into a system, they can become rigid boundaries that are difficult to challenge or revise. At the same time, my skepticism toward AI evaluation has been unsettled by a comparison with human judgment. Humans are also subjective, inconsistent, and easily influenced by context, mood, or prior experiences. A hiring manager may be primed by a single word on a résumé or by the candidate they interviewed just before. These forms of bias are rarely scrutinized with the same intensity as algorithmic errors. As the article argues, AI systems do not create fairness on their own; people give them authority, frame them as “objective,” and decide which values they encode.\nThis raises a deeper question: why are we often less tolerant of AI mistakes than human ones? If AI is a human-made system trained on historical data, its biases are ultimately reflections of human choices. Yet we frequently hold AI to a higher standard of consistency and accuracy. What, then, is a reasonable tolerance for AI error or hallucination, especially in high-stakes evaluative contexts?\nThese questions are not abstract for me. In projects like KidTalkMirror and other AI persona–based feedback tools, we actively use AI to evaluate human behavior and generate suggestions. Designing these systems forces us to confront the same dilemmas discussed in the Harvard article: whose definition of fairness is embedded in the model, which voices are excluded, and how easily those assumptions can drift over time. Rather than asking whether AI or humans are fairer, the more important task may be to continually interrogate what kind of fairness we are designing for, and who gets to define it."
  },
  {
    "objectID": "posts/W9_P3_AI_Hiring/AI_hiring.html#should-ai-be-used-to-evaluate-human",
    "href": "posts/W9_P3_AI_Hiring/AI_hiring.html#should-ai-be-used-to-evaluate-human",
    "title": "Should AI be used to evaluate Human?",
    "section": "",
    "text": "Recently, I have increasingly encountered situations where artificial intelligence is used to evaluate people. One of the most visible examples is hiring. In many recruitment processes, AI systems now filter résumés by scanning for keywords, educational backgrounds, or patterns associated with past “successful” employees. According to Harvard Business Review, nearly 90% of companies use some form of AI in hiring today. While these tools are often promoted as a way to reduce human bias, critics argue that they can be rigid and exclusionary, filtering out nontraditional candidates whose potential does not align neatly with predefined criteria.\n\n\n\nHarvard Research on Hiring Fairness\n\n\nThe Harvard article complicates this debate by suggesting that the real issue is not whether AI is more or less biased than humans, but that AI quietly locks in a particular definition of fairness. In the case study described, an algorithm designed to promote fairness privileged consistency and standardization, sidelining contextual judgment from managers who understood local needs and individual trajectories. Over time, this algorithmic definition of fairness became dominant, even when it conflicted with human expertise. This resonates with common concerns about résumé screening tools that favor candidates who look similar to existing employees, potentially narrowing rather than diversifying the talent pool.\nBeyond résumé screening, AI is increasingly used in interviews. Some companies now deploy AI-driven interviewers or digital avatars that adapt questions based on a candidate’s responses. While this may increase efficiency and scalability, it fundamentally alters the nature of an interview. Interviews are traditionally a two-way exchange: candidates are evaluated, but they are also assessing the organization. When AI replaces a human interviewer, the interaction can feel closer to recording a presentation than engaging in a conversation. This not only limits the interviewer’s ability to probe nuance, but also deprives candidates of the chance to build understanding, ask questions, and read interpersonal cues.\nThe implications of AI evaluation extend far beyond hiring. Any context involving assessment can be affected. In education, for example, automated grading systems are increasingly used to evaluate student work. While these tools may perform adequately on closed-ended questions, they struggle with open-ended responses that require interpretation, creativity, or emotional sensitivity. An AI grader that is overly strict, overly lenient, or indifferent in tone can undermine students’ confidence and motivation. As the Harvard article notes, once evaluative thresholds are encoded into a system, they can become rigid boundaries that are difficult to challenge or revise. At the same time, my skepticism toward AI evaluation has been unsettled by a comparison with human judgment. Humans are also subjective, inconsistent, and easily influenced by context, mood, or prior experiences. A hiring manager may be primed by a single word on a résumé or by the candidate they interviewed just before. These forms of bias are rarely scrutinized with the same intensity as algorithmic errors. As the article argues, AI systems do not create fairness on their own; people give them authority, frame them as “objective,” and decide which values they encode.\nThis raises a deeper question: why are we often less tolerant of AI mistakes than human ones? If AI is a human-made system trained on historical data, its biases are ultimately reflections of human choices. Yet we frequently hold AI to a higher standard of consistency and accuracy. What, then, is a reasonable tolerance for AI error or hallucination, especially in high-stakes evaluative contexts?\nThese questions are not abstract for me. In projects like KidTalkMirror and other AI persona–based feedback tools, we actively use AI to evaluate human behavior and generate suggestions. Designing these systems forces us to confront the same dilemmas discussed in the Harvard article: whose definition of fairness is embedded in the model, which voices are excluded, and how easily those assumptions can drift over time. Rather than asking whether AI or humans are fairer, the more important task may be to continually interrogate what kind of fairness we are designing for, and who gets to define it."
  },
  {
    "objectID": "posts/W15_P1_Writing_Detector/detector.html",
    "href": "posts/W15_P1_Writing_Detector/detector.html",
    "title": "Are AI writing detectors accurate?",
    "section": "",
    "text": "As generative AI tools like ChatGPT become increasingly integrated into writing workflows, from student essays to professional reports, questions about authorship and originality have followed close behind. In response, a market of AI writing detection tools has emerged, promising to distinguish between human-written and AI-generated text. But how accurate are these tools in practice? And should we be using them at all? This post examines the current landscape of AI writing detectors, discusses their limitations, and compares off-the-shelf tools with self-written prompt-based approaches that emphasize process transparency and human judgment.\nAI writing detectors analyze text to estimate the likelihood that it was produced by a language model rather than a human. These tools typically rely on statistical and stylistic features, such as sentence predictability, vocabulary patterns, and structural regularity, that have been associated with machine-generated text. Their intended use cases include academic integrity checks, hiring or application screening, and editorial review. In theory, these tools offer a quick way to assess whether a piece of writing may have been generated by AI.\n\n\n\nAI Writing Detection\n\n\n\n\nDespite their widespread adoption, AI writing detectors face significant accuracy issues. One major concern is the high rate of false positives. Well-structured, formal, or polished writing is often flagged as AI-generated even when it is written by humans. This is particularly problematic in academic contexts, where clarity and structure are encouraged\nAt the same time, false negatives are also common. Shorter, simpler, or lightly edited AI-generated text can easily evade detection. Small human edits may further reduce detectable signals, making the results unreliable. Another challenge is that language models evolve more quickly than detection tools. Detectors trained on earlier generations of AI output may fail to identify text produced by newer models. Different detectors also frequently disagree with one another, producing inconsistent assessments for the same piece of writing. Overall, detector outputs should be understood as probabilistic indicators rather than definitive judgments.\n\n\n\nAn alternative to post hoc detection is to focus on transparency during the writing process itself. Self-written prompt strategies emphasize documenting how AI is used, rather than attempting to infer authorship after the text is produced.\nIn this approach, writers use structured prompts and maintain a visible interaction history with AI tools. This allows reviewers to understand how ideas were generated, revised, and refined. Instead of guessing whether text is AI-generated, evaluators can see the role AI played in the process.\nThis model shifts responsibility toward intentional use and reflection. Human judgment remains central, as reviewers assess not only the final product but also the reasoning, revisions, and decision-making that shaped it.\nMarket detection tools offer convenience and scalability. They are easy to use and can quickly flag text for further review. However, their lack of contextual awareness limits their reliability and makes them unsuitable as standalone evidence. Prompt-based strategies require more effort upfront but offer greater transparency. They support learning and accountability by making the writing process visible. Rather than focusing on punishment or suspicion, this approach encourages clearer expectations and more meaningful evaluation of writing.\n\n\n\nAI writing detectors can be useful as a preliminary signal, but they should not be treated as authoritative. Their results require interpretation and should always be supplemented with human review and contextual understanding. In educational settings especially, reliance on detectors alone risks misclassification and undermines trust. A more productive approach combines limited detector use with process-oriented strategies that emphasize learning goals, transparency, and responsible AI use.\n\n\n\nAI writing detectors attempt to solve a complex problem by analyzing text in isolation. Current tools struggle because they treat writing as a static artifact rather than as the outcome of a process. Approaches that foreground how writing is created, including the role AI plays, offer a more reliable and educationally meaningful path forward. Rather than asking whether a text was written by AI or a human, it may be more useful to ask how the text was produced and what that process reveals about understanding, effort, and intent."
  },
  {
    "objectID": "posts/W15_P1_Writing_Detector/detector.html#accuracy-challenges-in-practice",
    "href": "posts/W15_P1_Writing_Detector/detector.html#accuracy-challenges-in-practice",
    "title": "Are AI writing detectors accurate?",
    "section": "",
    "text": "Despite their widespread adoption, AI writing detectors face significant accuracy issues. One major concern is the high rate of false positives. Well-structured, formal, or polished writing is often flagged as AI-generated even when it is written by humans. This is particularly problematic in academic contexts, where clarity and structure are encouraged\nAt the same time, false negatives are also common. Shorter, simpler, or lightly edited AI-generated text can easily evade detection. Small human edits may further reduce detectable signals, making the results unreliable. Another challenge is that language models evolve more quickly than detection tools. Detectors trained on earlier generations of AI output may fail to identify text produced by newer models. Different detectors also frequently disagree with one another, producing inconsistent assessments for the same piece of writing. Overall, detector outputs should be understood as probabilistic indicators rather than definitive judgments."
  },
  {
    "objectID": "posts/W15_P1_Writing_Detector/detector.html#self-written-prompts-as-an-alternative-approach",
    "href": "posts/W15_P1_Writing_Detector/detector.html#self-written-prompts-as-an-alternative-approach",
    "title": "Are AI writing detectors accurate?",
    "section": "",
    "text": "An alternative to post hoc detection is to focus on transparency during the writing process itself. Self-written prompt strategies emphasize documenting how AI is used, rather than attempting to infer authorship after the text is produced.\nIn this approach, writers use structured prompts and maintain a visible interaction history with AI tools. This allows reviewers to understand how ideas were generated, revised, and refined. Instead of guessing whether text is AI-generated, evaluators can see the role AI played in the process.\nThis model shifts responsibility toward intentional use and reflection. Human judgment remains central, as reviewers assess not only the final product but also the reasoning, revisions, and decision-making that shaped it.\nMarket detection tools offer convenience and scalability. They are easy to use and can quickly flag text for further review. However, their lack of contextual awareness limits their reliability and makes them unsuitable as standalone evidence. Prompt-based strategies require more effort upfront but offer greater transparency. They support learning and accountability by making the writing process visible. Rather than focusing on punishment or suspicion, this approach encourages clearer expectations and more meaningful evaluation of writing."
  },
  {
    "objectID": "posts/W15_P1_Writing_Detector/detector.html#should-we-use-ai-writing-detectors",
    "href": "posts/W15_P1_Writing_Detector/detector.html#should-we-use-ai-writing-detectors",
    "title": "Are AI writing detectors accurate?",
    "section": "",
    "text": "AI writing detectors can be useful as a preliminary signal, but they should not be treated as authoritative. Their results require interpretation and should always be supplemented with human review and contextual understanding. In educational settings especially, reliance on detectors alone risks misclassification and undermines trust. A more productive approach combines limited detector use with process-oriented strategies that emphasize learning goals, transparency, and responsible AI use."
  },
  {
    "objectID": "posts/W15_P1_Writing_Detector/detector.html#conclusion",
    "href": "posts/W15_P1_Writing_Detector/detector.html#conclusion",
    "title": "Are AI writing detectors accurate?",
    "section": "",
    "text": "AI writing detectors attempt to solve a complex problem by analyzing text in isolation. Current tools struggle because they treat writing as a static artifact rather than as the outcome of a process. Approaches that foreground how writing is created, including the role AI plays, offer a more reliable and educationally meaningful path forward. Rather than asking whether a text was written by AI or a human, it may be more useful to ask how the text was produced and what that process reveals about understanding, effort, and intent."
  },
  {
    "objectID": "posts/W7_P2_Zoom/Zoom.html",
    "href": "posts/W7_P2_Zoom/Zoom.html",
    "title": "Using Zoom AI Companion for User Studies",
    "section": "",
    "text": "Using Zoom AI Companion for User Studies\nI recently used Zoom AI Companion during a one-hour user interview. I made a critical mistake: I did not turn on recording. At the time, this felt inconsequential. I assumed that the AI Companion, which was visibly active during the meeting, would still retain a usable transcript or at least allow me to recover the conversation afterward. That assumption turned out to be wrong.\nAfter the interview ended, I stayed in the Zoom room and tried to extract the full transcription through the AI Companion chat. What I received instead was a set of fragmented, partial transcripts. Worse, when I tried to prompt the system to reconstruct or summarize the full conversation, it began to hallucinate. The output included statements that were never said, connections that were never made, and smooth but inaccurate narratives that felt plausible yet fundamentally untrustworthy. At that moment, the AI Companion shifted from being a productivity aid to a liability.\n\n\n\nZoom AI Companion\n\n\nThis experience highlighted a core limitation of large language model applications in real-world workflows: they are not memory systems. Without recording enabled, the model had no authoritative source of truth to ground its responses. What appeared to be “recall” was in fact inference. From a technical perspective, this is likely tied to token and memory constraints. The model only had access to limited conversational context, possibly cached summaries or short segments, rather than a full transcript. When prompted beyond that boundary, it did what LLMs are designed to do: generate the most statistically plausible continuation. Accuracy was replaced by coherence.\nFor user research, this is a serious problem. Interviews depend on precise wording, tone, pauses, and unexpected turns. Subtle phrasing differences can change the interpretation of user intent. An AI-generated reconstruction that blends partial memory with invented content is worse than no transcript at all, because it introduces false confidence. As a researcher, I would rather acknowledge data loss than unknowingly analyze fabricated data.\nThis also raises broader questions about how AI should be used in user interviews. AI companions are increasingly positioned as silent assistants: transcribing, summarizing, tagging insights in real time. When they work, they reduce cognitive load and allow researchers to stay present in the conversation. But this experience reinforced that AI should augment, not replace, robust data capture practices. Recording, explicit transcription pipelines, and clear data ownership still matter. AI summaries should always be traceable back to raw data, not treated as primary artifacts.\nPrivacy adds another layer of tension. One reason some researchers hesitate to record interviews is concern for participant comfort and data protection. AI companions can feel like a middle ground: present but unobtrusive. However, this incident revealed an uncomfortable trade-off. When privacy-preserving choices limit data persistence, AI systems may compensate by hallucinating rather than refusing. That is not a neutral failure mode. It shifts risk from data exposure to data distortion, which can quietly undermine research integrity.\nUltimately, this experience made me more cautious, not less interested, in AI-assisted research tools. LLMs are powerful interfaces for language, but they are bounded by context windows, memory design, and probabilistic generation. When those limits are invisible to users, trust erodes quickly. For AI companions to be truly useful in research settings, they need clearer affordances: explicit signals about what is stored, what is lost, and when the system is guessing rather than recalling. Until then, responsibility still sits firmly with the human researcher to design workflows that do not confuse fluency with fidelity."
  },
  {
    "objectID": "posts/W11_P2_AITesting/aitesting.html",
    "href": "posts/W11_P2_AITesting/aitesting.html",
    "title": "Embracing AI Through Testing",
    "section": "",
    "text": "In Chapters 5 and 6 of Superagency, Reid Hoffman and his coauthors focus on a central argument about how artificial intelligence improves: not primarily through static regulation, but through data, testing, and continuous iteration. Rather than treating AI systems as finished products that must be controlled from the outside, the book frames them as evolving tools that become safer and more useful through real-world use, feedback, and refinement.\nThis perspective challenged some of my initial assumptions about AI safety. I often think of safety in terms of guardrails and restrictions, but these chapters argue that learning from deployment is just as important. Understanding how large language models are trained, evaluated, and stress-tested made me feel more optimistic about AI development. Safety, in this view, is not a fixed condition, but something that improves over time as models encounter more data, edge cases, and human judgment.\n\n\nOne concrete example the authors discuss is Chatbot Arena, a public benchmarking platform where users can compare responses from different language models and vote on which one performs better. Instead of relying only on expert-designed benchmarks or internal company metrics, Chatbot Arena incorporates large-scale human preference data into model evaluation.\nWhat stood out to me is that Arena does not ask users to judge models on abstract technical criteria. Instead, it asks a simple question: which response do you prefer? This design aligns closely with the idea that AI quality is not only about correctness, but also about usefulness, clarity, and task fit. By aggregating many small human judgments, the platform creates a feedback loop that reflects how people actually experience these models in practice.\nThis approach connects directly to the book’s broader claim that better AI emerges from exposure, testing, and iteration. Rather than assuming developers can predict all failure modes in advance, Chatbot Arena allows models to be compared, challenged, and improved in public.\n\n\n\nChatbot Arena\n\n\nI agree with the authors’ argument that public input and iteration can meaningfully improve AI systems. What I find especially interesting is the possibility that tools like Chatbot Arena are not just about ranking models globally, but about helping individuals discover which models work best for their own tasks. Different users value different qualities: some care about creativity, others about precision, tone, or explanation style. A single “best” model may not exist in all contexts.\nThis raises an open question for me. Could platforms like Chatbot Arena eventually support more personalized decision-making, helping users identify which LLM or application best supports their specific goals? From a learning sciences perspective, this feels important. A model that is helpful for brainstorming may not be ideal for careful conceptual understanding, and vice versa. Public voting captures collective preferences, but there may also be room to surface patterns about task-specific effectiveness.\n\n\n\nReading these chapters shifted how I think about AI governance and design. Regulation still matters, especially for high-risk applications, but Superagency helped me see AI systems as learning systems rather than static technologies. Data, evaluation, and human feedback are not afterthoughts; they are core mechanisms for improvement.\nTools like Chatbot Arena make this process visible. They show that safety and quality do not come only from top-down rules, but from iterative testing in the open, where users actively shape what “better” looks like. For me, this reframing made AI feel less opaque and less fragile. Instead of something that must be perfectly controlled before release, AI becomes something that can improve through responsible use, reflection, and collective judgment.\nUltimately, these chapters left me more optimistic. Not because AI is already safe or perfect, but because the mechanisms for learning, correction, and alignment are actively being built. And understanding those mechanisms makes engaging with AI feel more like participation in an evolving system than passive consumption of a finished product."
  },
  {
    "objectID": "posts/W11_P2_AITesting/aitesting.html#chatbot-arena-as-an-example-of-iterative-ai-evaluation",
    "href": "posts/W11_P2_AITesting/aitesting.html#chatbot-arena-as-an-example-of-iterative-ai-evaluation",
    "title": "Embracing AI Through Testing",
    "section": "",
    "text": "One concrete example the authors discuss is Chatbot Arena, a public benchmarking platform where users can compare responses from different language models and vote on which one performs better. Instead of relying only on expert-designed benchmarks or internal company metrics, Chatbot Arena incorporates large-scale human preference data into model evaluation.\nWhat stood out to me is that Arena does not ask users to judge models on abstract technical criteria. Instead, it asks a simple question: which response do you prefer? This design aligns closely with the idea that AI quality is not only about correctness, but also about usefulness, clarity, and task fit. By aggregating many small human judgments, the platform creates a feedback loop that reflects how people actually experience these models in practice.\nThis approach connects directly to the book’s broader claim that better AI emerges from exposure, testing, and iteration. Rather than assuming developers can predict all failure modes in advance, Chatbot Arena allows models to be compared, challenged, and improved in public.\n\n\n\nChatbot Arena\n\n\nI agree with the authors’ argument that public input and iteration can meaningfully improve AI systems. What I find especially interesting is the possibility that tools like Chatbot Arena are not just about ranking models globally, but about helping individuals discover which models work best for their own tasks. Different users value different qualities: some care about creativity, others about precision, tone, or explanation style. A single “best” model may not exist in all contexts.\nThis raises an open question for me. Could platforms like Chatbot Arena eventually support more personalized decision-making, helping users identify which LLM or application best supports their specific goals? From a learning sciences perspective, this feels important. A model that is helpful for brainstorming may not be ideal for careful conceptual understanding, and vice versa. Public voting captures collective preferences, but there may also be room to surface patterns about task-specific effectiveness."
  },
  {
    "objectID": "posts/W11_P2_AITesting/aitesting.html#reflection-from-regulation-to-learning-systems",
    "href": "posts/W11_P2_AITesting/aitesting.html#reflection-from-regulation-to-learning-systems",
    "title": "Embracing AI Through Testing",
    "section": "",
    "text": "Reading these chapters shifted how I think about AI governance and design. Regulation still matters, especially for high-risk applications, but Superagency helped me see AI systems as learning systems rather than static technologies. Data, evaluation, and human feedback are not afterthoughts; they are core mechanisms for improvement.\nTools like Chatbot Arena make this process visible. They show that safety and quality do not come only from top-down rules, but from iterative testing in the open, where users actively shape what “better” looks like. For me, this reframing made AI feel less opaque and less fragile. Instead of something that must be perfectly controlled before release, AI becomes something that can improve through responsible use, reflection, and collective judgment.\nUltimately, these chapters left me more optimistic. Not because AI is already safe or perfect, but because the mechanisms for learning, correction, and alignment are actively being built. And understanding those mechanisms makes engaging with AI feel more like participation in an evolving system than passive consumption of a finished product."
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html",
    "href": "posts/W7_P3_StudyGuide/study_guide.html",
    "title": "Creating a study guide based on quizzes?",
    "section": "",
    "text": "AI for Learning"
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html#part-ii-sample-quiz-responses-feedback",
    "href": "posts/W7_P3_StudyGuide/study_guide.html#part-ii-sample-quiz-responses-feedback",
    "title": "Creating a study guide based on quizzes?",
    "section": "Part II: Sample Quiz Responses & Feedback",
    "text": "Part II: Sample Quiz Responses & Feedback\n\n\nQuestion 1 (2 points)\nWhich of the following best describes the two channels in Mayer’s Multimedia Learning Theory?\nA. Verbal and emotional channels\nB. Visual–pictorial and auditory–verbal channels\nC. Textual and numerical channels\nD. Short-term and long-term memory channels\nStudent Answer: B\nPoints Earned: 2 / 2\nFeedback:\nCorrect. Humans process information through separate visual–pictorial and auditory–verbal channels.\n\n\n\nQuestion 2 (2 points)\nWhich factor most strongly influences intrinsic cognitive load?\nA. The design quality of instructional materials\nB. The learner’s motivation level\nC. The inherent complexity of the content relative to prior knowledge\nD. The presence of multimedia elements\nStudent Answer: B\nPoints Earned: 0 / 2\nFeedback:\nIncorrect. Intrinsic cognitive load is determined by the complexity of the material and how it interacts with the learner’s prior knowledge, not motivation.\n\n\n\nQuestion 3 (2 points)\nA slide contains a paragraph of text, and the instructor reads the same text aloud. Which multimedia principle is most directly violated?\nA. Signaling principle\nB. Coherence principle\nC. Redundancy principle\nD. Modality principle\nStudent Answer: C\nPoints Earned: 2 / 2\nFeedback:\nCorrect. Presenting identical verbal information in both text and narration can overload working memory.\n\n\n\nQuestion 4 (2 points)\nWhich of the following is an example of extraneous cognitive load caused by AI tools?\nA. Struggling with a difficult concept for the first time\nB. Receiving long, detailed explanations regardless of prior knowledge\nC. Needing time to practice applying a new formula\nD. Building schemas through worked examples\nStudent Answer: A\nPoints Earned: 0 / 2\nFeedback:\nIncorrect. Extraneous load comes from how information is presented. Overly long or poorly tailored AI explanations can increase extraneous load.\n\n\n\nQuestion 5 (2 points)\nWhich AI design choice best aligns with cognitive load theory?\nA. Always providing complete answers to reduce effort\nB. Offering the same explanation to all students\nC. Allowing students to request hints before full explanations\nD. Adding more examples to ensure clarity\nStudent Answer: D\nPoints Earned: 1 / 2\nFeedback:\nPartially correct. More examples can help, but only when tailored to the learner. Allowing students to control when they receive hints better manages cognitive load."
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html#quiz-summary",
    "href": "posts/W7_P3_StudyGuide/study_guide.html#quiz-summary",
    "title": "Creating a study guide based on quizzes?",
    "section": "Quiz Summary",
    "text": "Quiz Summary\n\nTotal Score: 5 / 10\n\n\nPerformance Pattern\n\nStrong recognition of multimedia principles\n\nWeak understanding of cognitive load types\n\nLimited ability to connect theory to AI design decisions\n\n\n\nPart III: Gemini’s Answer\n\nBased on your quiz results, here is your personalized study guide. You have a solid grasp of the “Multimedia” side of the theory (channels and principles), but you are mixing up the different types of Cognitive Load (Intrinsic vs. Extraneous) and how they apply to AI tools.\nThis guide will help you untangle those definitions so you can apply them correctly.\n\n\nWhat You Already Understand\n\nDual-Channel Assumption: You correctly identified that we process information through two separate channels: visual–pictorial and auditory–verbal (Question 1).\nRedundancy Principle: You clearly understand that reading text aloud while it is displayed on screen overloads the brain. You correctly identified this as a violation of the Redundancy Principle (Question 3).\n\n\n\n\nWhat Needs Review\n\nIntrinsic Cognitive Load: You confused this with “motivation.” In this theory, intrinsic refers to the difficulty of the topic, not the feelings of the learner (Question 2).\nExtraneous Cognitive Load: You mistook “struggling with a hard concept” (which is normal) for extraneous load. Extraneous load is caused by bad design, not hard topics (Question 4).\nAI & Cognitive Load Application: You recognized that examples are helpful (Question 5), but missed the nuance that interactivity (hints) manages load better than volume (more examples).\n\n\n\n\nKey Concepts Explained\n\n1. The Three Types of Cognitive Load\nThink of your brain like a computer’s RAM. It has limited space.\n\nIntrinsic Load (The “What”)\nThe inherent difficulty of the material itself.\nAnalogy: Lifting a 50lb weight. It is heavy because it is heavy.\nExtraneous Load (The “How”)\nUnnecessary effort caused by poor design or instruction.\nAnalogy: Lifting the same 50lb weight while blindfolded and wearing slippery gloves.\nGermane Load (The “Learning”)\nProductive effort used to build schemas and understanding.\n\n\n\n\n\nPractice & Reflection\nActivity 1: Diagnose the Load\nFor each scenario, decide whether it reflects Intrinsic or Extraneous load.\n\nLearning Quantum Physics without prior physics experience\n\nReading a textbook with tiny fonts and confusing diagrams\n\nReceiving a 2,000-word AI explanation with no structure\n\nActivity 2: Re-Write the AI Prompt\nWrite a prompt for an AI tutor that reduces Extraneous Load for a complex topic.\nActivity 3: Reflection\nWhy do you think you associated Intrinsic load with Motivation? How will you remember the difference next time?\n\n\n\nYour Personalized Study Plan (30 Minutes)\n\nReview Definitions (10 mins)\nFocus on Intrinsic vs. Extraneous using the weightlifting analogy.\nScenario Sorting (10 mins)\nIdentify one example of hard content and one example of poor explanation from your materials.\nAI Design Check (10 mins)\nReflect on why “more examples” can sometimes hurt learning.\n\n\n\n\nMetacognitive Support\n\nReflect: How does reframing struggle as Intrinsic change how you approach learning?\nStrategy: Next time, look for keywords like complexity and prior knowledge rather than motivation."
  },
  {
    "objectID": "posts/W7_P3_StudyGuide/study_guide.html#part-iv-reflection",
    "href": "posts/W7_P3_StudyGuide/study_guide.html#part-iv-reflection",
    "title": "Creating a study guide based on quizzes?",
    "section": "Part IV: Reflection",
    "text": "Part IV: Reflection\nOverall, the AI-generated study guide generally supports the two ideas that motivated this project, though in a fairly expected and incremental way rather than a transformative one. The guide successfully shifts attention from quiz scores to underlying conceptual understanding by identifying specific misconceptions and organizing review content around them. This suggests that using completed quizzes as input can help students return to the original learning goals of the course, especially when they want to relearn material after an assignment is over. In this sense, the tool does help students get back on track without requiring them to reread everything or start from scratch.\nThe output also aligns with the idea of using AI to support more personalized and adaptive learning. Instead of reviewing all concepts equally, the study guide focuses on cognitive load distinctions, which were clearly problematic based on the quiz responses. The explanations, analogies, and short practice activities are tailored to those gaps, while concepts the student already understands are largely left alone. This shows how AI can use evidence from student work to prioritize learning needs and generate a targeted study plan, something that is difficult to scale through traditional instruction alone.\nFrom a broader perspective, this result reflects a more balanced role for AI in learning. Rather than acting as a shortcut to answers, the AI functions as a post-assessment support tool that encourages reflection and re-engagement with course concepts. At the same time, the quality of the output depends heavily on how the prompt constrains the AI’s role. Without those constraints, the AI could easily default to giving direct answers. This suggests that AI’s impact on learning is less about the technology itself and more about how intentionally it is framed within the learning process."
  },
  {
    "objectID": "posts/W6_P1_Before Superagency/before superagency.html",
    "href": "posts/W6_P1_Before Superagency/before superagency.html",
    "title": "The Long Way to Go Before Superagency",
    "section": "",
    "text": "The Long Way to Go Before Superagency\n\n\n\n\nSocial Media Around Us\n\nAfter reading Chapter Two of the book Superagency, I see clear tradeoffs between privacy and scalability and conditions that are required for us to enjoy the freedom of AI, but I also realized how AI literacy and equal access is an essential presumption for us to fully leverage on the benefits AI brings.\nThis chapter started with an analogy of today’s age to the surveillance world in the book 1984 by George Orwell, picturing people’s fear that AI may invade privacy and agency, and in the worst cases of our worries, AI may take over our thoughts and replace us. In the second half of this chapter, Hoffman (one of the authors of Superagency) started to discuss that emerging technologies and AI is bringing us more connected resources and information that allows people to build up networks and make better choices. For example, by looking up people on LinkedIn you will know someone’s skillset and if they are an ideal person you want to work with. Hoffman implied that in today’s era, creating public identities is opening up so many opportunities for human agencies that outweighs the risks to privacy and will not hinder our free will. All these advantages were brought by distributing intelligence through big data, with the formation of Big Knowledge, which is the name of this chapter.\nWhat he argued was totally true, that LinkedIn or other social media is enabling people to know more about others and this world and that humans can use AI and algorithm power to gather and manage big data to make more comprehensive decisions. However, I think all of these benefits will need three important conditions: people’s awareness, consent, and free choices on data and technology usage.\nThese three things come in series. Only if you know that AI is gathering your data can you decide whether to opt in or not. Only if you can choose to not share your information can you be called to have free will in this data era. Unfortunately, these conditions have not all been met in our daily lives, yet AI is operating wildly. Everytime you select “I agree” to an internet consent, do you really read and understand what it says by “We use cookies to improve your browsing experience and analyze site traffic”. With the one time that you decided to choose “I do not agree”, you found out that you can’t even keep browsing this website.\nProducts that leverage LLMs now are not fully empowering free will and human agency with these forceful data collection through internet services and consent questions. Hoffman can easily name out the benefits of AI because he has the knowledge and resources to determine which algorithm is good for use and how to use LinkedIn to create a perfect profile rather than unintentionally disclose one’s privacy.\nI tried using AI meeting assistance tools in the beginning of 2024 when AI audio transcription started to thrive, and the agent went to a lab meeting after I graduated and sent everyone a meeting note. I hope no one notices it and thought I was trying to eavesdrop on conversations, but this happened simply because I didn’t turn off the access of the tool to my Google calendar through a bunch of consents during my onboarding. I have also seen friends sending posts on LinkedIn about every event they attended, but they didn’t even know that those posts were sent.\nEven if we as college and graduate students make these mistakes and expose ourselves to risks in privacy, then what about average people who have only heard about the acronym AI but never tried the tool themselves? What about teenagers and people who never go to college and took a lesson on prompt engineering and cybersecurity? What about elderly people who some even cannot read the tiny consent message?\n\n\n\n\nWe Value Your Privacy Note\n\nTo explore AI’s ability in debating and role taking, we had this activity to debate with an AI chat agent on different positionalities towards LLMs that the Book Superagency listed: Doomers, who believe AI poses an existential threat and should be stopped; Gloomers, who are skeptical of AI’s benefits and advocate for strict regulation to manage its harms. Zoomers, who enthusiastically push to accelerate AI development with minimal oversight; and Bloomers, who are cautiously optimistic, supporting AI’s promise while calling for measured risk management and public involvement.\nI was a firm Bloomer at that time and I asked gpt-4 to be a Gloomer to debate with me. It kept pointing out the lack of regulations in AI while products are created to be used by the public. Now after reading this chapter of Superagency, I felt my position was further swayed by Gloomers as we need more literacy and education on responsible AI and data usage before it further enters into our lives. I hope we as general consumers of LLM products are not just one data point in the big data, but active users, coordinators and even developers of big knowledge.\nWe will reach superagency, but there is still a long way to go."
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "A prompting journal and comparison between GPT-5 and gemini 2.5 flash\n\n\nI have always wanted to create an artistic profile photo that represents me but also includes creativity. As LLMs can now create great images, I decided to have them redesign my profile photo.\nAs a reference, below I include my profile photo (as it appears in the “About” section of this blog site) and my desired painting art style with artistic elements and frames surrounding the figure.\n\n\n\nThe Style I want (photos from freepik)\n\n\n\nMy Profile Photo\nI was curious about which model performs better in image redesign tasks, so I decided to do a control test between two LLMs and understand which one is better at doing these types of image generation tasks. I knew that for image creation, specific tools such as Midjourney or Google’s Nanobanana may work better, but considering the experience of non tech savvy audiences who may interact with general chat agents more, I decided to use the two most popular general AI chat tools: ChatGPT and Gemini. Among all the LLMs models, the latest and most advanced ones include GPT-5 and Gemini 2.5 flash in ChatGPT and Google Gemini chats.\n\n\n\nIn my previous experience generating photos from an LLM based chat agent, I needed to do many rounds of prompting and often could not get the one that fits my needs. Reflecting now, I probably did not use careful prompting to craft the output. Therefore, this time I was using The Five Principles of Prompting described by James Phonexi and Mike Taylor in Prompt Engineering for Generative AI to direct my prompts. The following table includes the explanation of each principle from the book and how I used it to create my initial prompt for redesigning my profile photo.\nTable 1. Initial Prompts Sent to GPT-5 and Gemini 2.5 flash Following the Five Principles of Prompting\n\n\n\n\n\n\n\n\nPrompt Engineering Principle\nExplanation (by Phonexi & Taylor)\nMy Prompting Strategy (prompts and commands I sent to LLMs)\n\n\n\n\nGive Direction\n“Describe the desired style in detail, or reference a relevant persona”\nRedesign my current profile photo (I’ll upload it later) and make it look artistic.The desired color style of the photo is bright and shows creativity.I want the color palette to be a mix of purple, pink, yellow, blue but in light intensity.\n\n\nSpecify Format\n“Define what rules to follow, and the required structure of the response”\nReturn five design options in .jpg format.\n\n\nProvide Examples\n“Insert a diverse set of test cases where the task was done correctly”\nI’ve included several examples attached here.(Sample headshot photos from freepik)\n\n\nEvaluate Quality\n“Identify errors and rate responses, testing what drives performance”\nI gave feedback and evaluation for further improvement in multiple rounds of follow up conversations, for example:I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 years old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.I like the fourth one in terms of the frames and how it keeps the background, but I also like number 3 for the color. Try mixing the two. DO NOT alter any object or characteristics of the person in the image including clothes style and color.\n\n\nDivide Labor\n“Split tasks into multiple steps, chained together for complex goals”\nI gave the instruction first then followed up with my original headshot and more detailed instructions on the occasion, restrictions, explanations on design plan etc. I asked for a rough redesign first and then adjust the details and colors.Follow up after initial prompt:This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don’t be too informal.[attached my profile photo]\n\n\n\nTo control the conditions, I used the same prompts that gave out initial instructions and reference photos for both gpt-5 and gemini 2.5 flash. I also kept my goal or result expectations on the two models similar to the sample images, though I had to give different feedback on the various outputs the two models provided back. I sent the initial prompts at the same time of the day and continued conversations with one model while the other was generating or processing answers.\nProcesses and Results Comparing Two Models\nIn this section, I will reflect on my experience prompting and evaluating the two models to redesign my profile photo based on my imagined style.\nGPT-5: Slow start, Fast Improvement GPT-5 had a long processing time, missed instructions on formatting and lacked text explanations along with images generated.\nGPT-5 seemed to misunderstand formatting commands: After my initial prompt, which I mentioned that the photo to be redesigned will be uploaded later, GPT-5 already started to generate a photo using the two sample images, and I got a combined collage that was not myself at all but the figure in the sample photo. GPT-5 misunderstood the single person headshot to be my photo.\n\n\n\nIt also ignored my request for generating five design options by only returning one.\nGPT-5 only followed image generation with text explanation after conversations were branched. I think there might also be a design gap here on Chatgpt interface. The circle on the right of the input box will turn to a square when it is still processing or generating. Normally the message will also show a round point which is loading, but under image generation mode it usually doesn’t. Therefore, when the agent is still generating text, I wasn’t able to notice the state and kept typing my command. This might be a reason that gpt-5 only returned images without any text or communications with me.\n\n\n\nHowever, I do have to say that even with all those weird reactions, the image quality was high. I would say the first version it generated earned my 75% satisfaction. I was surprised actually. Although it didn’t listen to my command to combine the realistic figure with the cartoon frame but instead created a whole cartoon headshot, the style and color looked nice, and it was different from the sample I sent, which made me feel that this is novel creation rather than just copy pasting.\nVersion 1 User Evaluation: Nice color and style. Could make the figure look younger…\n\n\n\nI thought this painted me older than I looked, so I sent a follow up prompt: &gt; I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\nI like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\n\n\n\nAnd this iteration was almost 90% what I wanted. I did four more rounds of prompting to try different gaze, expressions, color intensity etc, but this version was eventually what I chose.\n(more about Gemini 2.5 flash performance on the next blog)"
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#background",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#background",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "I have always wanted to create an artistic profile photo that represents me but also includes creativity. As LLMs can now create great images, I decided to have them redesign my profile photo.\nAs a reference, below I include my profile photo (as it appears in the “About” section of this blog site) and my desired painting art style with artistic elements and frames surrounding the figure.\n\n\n\nThe Style I want (photos from freepik)\n\n\n\nMy Profile Photo\nI was curious about which model performs better in image redesign tasks, so I decided to do a control test between two LLMs and understand which one is better at doing these types of image generation tasks. I knew that for image creation, specific tools such as Midjourney or Google’s Nanobanana may work better, but considering the experience of non tech savvy audiences who may interact with general chat agents more, I decided to use the two most popular general AI chat tools: ChatGPT and Gemini. Among all the LLMs models, the latest and most advanced ones include GPT-5 and Gemini 2.5 flash in ChatGPT and Google Gemini chats."
  },
  {
    "objectID": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#prompts-and-controlled-conditions",
    "href": "posts/W4P1_Artistic_Headshot/Artistic_Headshot.html#prompts-and-controlled-conditions",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash.",
    "section": "",
    "text": "In my previous experience generating photos from an LLM based chat agent, I needed to do many rounds of prompting and often could not get the one that fits my needs. Reflecting now, I probably did not use careful prompting to craft the output. Therefore, this time I was using The Five Principles of Prompting described by James Phonexi and Mike Taylor in Prompt Engineering for Generative AI to direct my prompts. The following table includes the explanation of each principle from the book and how I used it to create my initial prompt for redesigning my profile photo.\nTable 1. Initial Prompts Sent to GPT-5 and Gemini 2.5 flash Following the Five Principles of Prompting\n\n\n\n\n\n\n\n\nPrompt Engineering Principle\nExplanation (by Phonexi & Taylor)\nMy Prompting Strategy (prompts and commands I sent to LLMs)\n\n\n\n\nGive Direction\n“Describe the desired style in detail, or reference a relevant persona”\nRedesign my current profile photo (I’ll upload it later) and make it look artistic.The desired color style of the photo is bright and shows creativity.I want the color palette to be a mix of purple, pink, yellow, blue but in light intensity.\n\n\nSpecify Format\n“Define what rules to follow, and the required structure of the response”\nReturn five design options in .jpg format.\n\n\nProvide Examples\n“Insert a diverse set of test cases where the task was done correctly”\nI’ve included several examples attached here.(Sample headshot photos from freepik)\n\n\nEvaluate Quality\n“Identify errors and rate responses, testing what drives performance”\nI gave feedback and evaluation for further improvement in multiple rounds of follow up conversations, for example:I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 years old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.I like the fourth one in terms of the frames and how it keeps the background, but I also like number 3 for the color. Try mixing the two. DO NOT alter any object or characteristics of the person in the image including clothes style and color.\n\n\nDivide Labor\n“Split tasks into multiple steps, chained together for complex goals”\nI gave the instruction first then followed up with my original headshot and more detailed instructions on the occasion, restrictions, explanations on design plan etc. I asked for a rough redesign first and then adjust the details and colors.Follow up after initial prompt:This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don’t be too informal.[attached my profile photo]\n\n\n\nTo control the conditions, I used the same prompts that gave out initial instructions and reference photos for both gpt-5 and gemini 2.5 flash. I also kept my goal or result expectations on the two models similar to the sample images, though I had to give different feedback on the various outputs the two models provided back. I sent the initial prompts at the same time of the day and continued conversations with one model while the other was generating or processing answers.\nProcesses and Results Comparing Two Models\nIn this section, I will reflect on my experience prompting and evaluating the two models to redesign my profile photo based on my imagined style.\nGPT-5: Slow start, Fast Improvement GPT-5 had a long processing time, missed instructions on formatting and lacked text explanations along with images generated.\nGPT-5 seemed to misunderstand formatting commands: After my initial prompt, which I mentioned that the photo to be redesigned will be uploaded later, GPT-5 already started to generate a photo using the two sample images, and I got a combined collage that was not myself at all but the figure in the sample photo. GPT-5 misunderstood the single person headshot to be my photo.\n\n\n\nIt also ignored my request for generating five design options by only returning one.\nGPT-5 only followed image generation with text explanation after conversations were branched. I think there might also be a design gap here on Chatgpt interface. The circle on the right of the input box will turn to a square when it is still processing or generating. Normally the message will also show a round point which is loading, but under image generation mode it usually doesn’t. Therefore, when the agent is still generating text, I wasn’t able to notice the state and kept typing my command. This might be a reason that gpt-5 only returned images without any text or communications with me.\n\n\n\nHowever, I do have to say that even with all those weird reactions, the image quality was high. I would say the first version it generated earned my 75% satisfaction. I was surprised actually. Although it didn’t listen to my command to combine the realistic figure with the cartoon frame but instead created a whole cartoon headshot, the style and color looked nice, and it was different from the sample I sent, which made me feel that this is novel creation rather than just copy pasting.\nVersion 1 User Evaluation: Nice color and style. Could make the figure look younger…\n\n\n\nI thought this painted me older than I looked, so I sent a follow up prompt: &gt; I like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\nI like the color and elements added, but this cartoon style seems to make people look older than they should. The character in this photo is 23 year old. Also this does not necessarily have to be in cartoon style. You may keep the realistic character. Now redesign based on the suggestions, return 4 designs, 2 in cartoon style and 2 not.\n\n\n\nAnd this iteration was almost 90% what I wanted. I did four more rounds of prompting to try different gaze, expressions, color intensity etc, but this version was eventually what I chose.\n(more about Gemini 2.5 flash performance on the next blog)"
  },
  {
    "objectID": "posts/W4P2_Creative Writing/Headshot_Part_II.html",
    "href": "posts/W4P2_Creative Writing/Headshot_Part_II.html",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)",
    "section": "",
    "text": "A prompting journal and comparison between GPT-5 and gemini 2.5 flash\n\n\n\nGemini 2.5 flash: Quick response, Steady Iteration Gemini, I would say, listened to all my instructions, but just didn’t do that well. Here are the five photos it returned after my initial prompt: &gt; This is my headshot. Follow the above instructions to redesign it. By redesign I mean adding frames and small elements and changing the color of the image. DO NOT change the characteristics of the person in the image. This will be used for a design portfolio, so the attire can be creative but don’t be too informal.\nGemini 2.5 flash response: &gt;Great! Here are five artistic redesign options for your headshot, incorporating frames and creative elements with a bright, light-intensity palette of purples, pinks, yellows, and blues. I’ve focused on enhancing the artistic feel for your design portfolio while keeping your attire suitable. &gt; &gt;\n\n\n\nGemini can do better at context analysis: Great communication, fast response time (compared to gpt-5 which took me minutes), organized output, but just less aesthetically appealing to me. I can see from the color palettes that it did take into account the samples I sent it, but the style was a little too cartoon-like, almost like a headshot for a kids channel not a professional profile. I would say Gemini 2.5 flash can do better at understanding the context and prioritizing the key context in output characteristics.\nIt’s okay that the initial round didn’t work that well. I tried a few more rounds of prompting to ask for more integral connection between picture and the frame, more modern look, but it didn’t surprise me. Here I included several versions it created. The pictures were okay but still needed a little artistic feeling.\n\n\n\n\n\nand I even sent it the one gpt-5 created, but Gemini 2.5 wasn’t able to bring any new thing, so eventually I went for the version gpt-5 created.\n\n\nPersonally, I would choose GPT-5.\nWhile Gemini 2.5 flash is like a student who is always on time for class, early for homeworks but does not hand over the most impressive projects, GPT-5 is like the naughtiest student in the class who never follows the traditional route but surprises you with its creativity. The higher accuracy for GPT-5 to understand my request and reach a good quality product at the beginning won my vote on it. I think I value the quality of work more than following a specific format I asked for (I will be such an annoying boss if this is a real work scenario…).\nFinding out GPT-5 did better was a little surprising to me. Before I started this project, Gemini 2.5 had more of my bet because it had the obvious “create image” tab that I started with, and I knew that Gemini is known for its indepth logic and research skills, which made me think maybe it will create better images.\nFor Better Comparison Tests: There are some limitations that showed up in this small experiment and can be improved in the future. More conditions should be in control to better compare model ability in creating images or other specific fields. For example, both models should be in “create image” mode. I should also try to control the follow up conversation prompts as much as possible, but this requires more discussions and experiments on whether controlling every single prompt will be an accurate assessment on model ability. (this will be further explored in future blogs on video generation) Prompting can be improved by adding semantic context and splitting information in further chunks. Overall, LLMs are now pretty good at high level (not detailed adjustments) visual design, as models can now quickly interpret users’ preference and make similar but creative visual products. With this headshot redesign task can be done on general chat agents (ChatGPT and Gemini), publics can now access fast iterative creative design service in a few minutes. This makes me think of how designers may use AI tools in their work and the role of designers in the future. With AI tools completing many design drafting, would more graphic and visual designers move on to in depth design fields such as 3d modeling or would there be even more needs in design research for designers to understand customers’ needs and bridge the communication between end user and AI design tools?"
  },
  {
    "objectID": "posts/W4P2_Creative Writing/Headshot_Part_II.html#conclusion",
    "href": "posts/W4P2_Creative Writing/Headshot_Part_II.html#conclusion",
    "title": "Artisticalize your headshot? A prompting journal and comparison between GPT-5 and gemini 2.5 flash (PartII)",
    "section": "",
    "text": "Personally, I would choose GPT-5.\nWhile Gemini 2.5 flash is like a student who is always on time for class, early for homeworks but does not hand over the most impressive projects, GPT-5 is like the naughtiest student in the class who never follows the traditional route but surprises you with its creativity. The higher accuracy for GPT-5 to understand my request and reach a good quality product at the beginning won my vote on it. I think I value the quality of work more than following a specific format I asked for (I will be such an annoying boss if this is a real work scenario…).\nFinding out GPT-5 did better was a little surprising to me. Before I started this project, Gemini 2.5 had more of my bet because it had the obvious “create image” tab that I started with, and I knew that Gemini is known for its indepth logic and research skills, which made me think maybe it will create better images.\nFor Better Comparison Tests: There are some limitations that showed up in this small experiment and can be improved in the future. More conditions should be in control to better compare model ability in creating images or other specific fields. For example, both models should be in “create image” mode. I should also try to control the follow up conversation prompts as much as possible, but this requires more discussions and experiments on whether controlling every single prompt will be an accurate assessment on model ability. (this will be further explored in future blogs on video generation) Prompting can be improved by adding semantic context and splitting information in further chunks. Overall, LLMs are now pretty good at high level (not detailed adjustments) visual design, as models can now quickly interpret users’ preference and make similar but creative visual products. With this headshot redesign task can be done on general chat agents (ChatGPT and Gemini), publics can now access fast iterative creative design service in a few minutes. This makes me think of how designers may use AI tools in their work and the role of designers in the future. With AI tools completing many design drafting, would more graphic and visual designers move on to in depth design fields such as 3d modeling or would there be even more needs in design research for designers to understand customers’ needs and bridge the communication between end user and AI design tools?"
  },
  {
    "objectID": "posts/W14_P1_Image/5.2image.html",
    "href": "posts/W14_P1_Image/5.2image.html",
    "title": "Images and Videos are Now INCLUDED!",
    "section": "",
    "text": "Last week, gpt 5.2 was announced. With the update, I immediately founded two new features with Chatgpt: 1) chatgpt includes illustrating images while giving text responses, and 2) users can now invite others to join the conversation and form a group chat with Chatgpt.\nWe will focus on the first one today and talk about the collaboration function next time.\n\n\nI found that chatgpt does not always return images with text, so I’m curious about what can trigger it to include images. In general, I found that it only does when users ask for explanation of an idea or concept that is easily to be understood with visuals.\nThe first question I asked was: &gt; What is venus flytrap?\n\n\n\nChatGPT answer about Venuflytrap\n\n\nIt replied with the text and graphics of real venus flytrap and its structure. I can’t imagine how useful it will be for teachers getting a headache of adding visuals to their slides or people having a hard time understanding what Chatgpt is saying with a bunch of texts. Clicking more, I found that it even included a youtube video link to show how venus flytraps work.\nI also asked &gt; “How would you make a sandwich? What are some steps?”\nand\n\n“How do you Juggle?”.\n\nChatgpt responded with step by step images and even a gif for juggling.\n\n\n\nChatGPT answer about Sandwich and Juggling\n\n\n\n\n\nWhen I turned to an abstract question “How do you sing a song smoothly”. Chatgpt did not respond with any visuals. That makes sense to me at first because knowing how to sing smoothly is more about understanding, but then I thought visuals can help you to see how your voices are made and where to focus on in your throat.\nAnother interesting case is when I ask “What does a sociologist learn?”, it gave me several flowcharts and slides such as\n\n\n\nChatGPT answer about Sociologist\n\n\nThis does not seem to help with my understanding. I would go for text only for this case.\nThese images and videos Chatgpt returned were not generated but searched through popular sites, mostly like knowledge bases such as wikipedia or questionpro, but you can definitely ask it to generate one if you need.\nThis new function makes me think about whether current chatbots are still useful. There are chatbots on the market that are made to help people find visual elements, learn skills, or find information about a specific field. As Artificial General Intelligence is getting more and more powerful, how do we find a market for a LLM application?\nRightnow the directions I can think of is 1) Make your knowledge base distinct so that your AI agent provides more professional and accurate information than others, 2) Enlarging the possibility for users to create, make the transition from generating visual to editing and creating faster (eg. Figma make), and 3) be more targeted on multimodal outputs so that you prioritize a certain media type over another for appropriate situations, instead of throwing everything to users to process.\n\n\n\nWith all these thoughts, I asked Chatgpt how it decides to include visuals in its answer to verify my guess. Here’s what it said:\nGood question. I don’t include images automatically. I make a judgment call based on how images would affect understanding or usefulness of the answer. Here’s how I decide, in plain terms. 1. Does the topic benefit from visual grounding? If the question is about objects, processes, places, people, or physical actions, images often help readers quickly orient themselves. Examples where images help: plants, animals, cooking, crafts, geography, sports skills Examples where they usually don’t: abstract concepts, definitions, arguments, reflections, ethics So “What is a Venus flytrap?” benefits from images. “How does cognitive load theory work?” usually doesn’t. 2. Is the task about how something looks or how something works physically? For step-based or motor tasks (juggling, cooking, assembling something), images can reduce confusion by showing: posture or hand position sequencing spatial relationships That’s why I included images for juggling and sandwich-making. 3. Will images reduce cognitive effort rather than add it? I avoid images when they would: distract from the core idea duplicate information already clear in text increase cognitive load instead of reducing it For example, your question “How do you sing a song smoothly” focused on internal control and technique, so text alone was more appropriate. 4. Is the user likely exploring or just asking? If the question suggests curiosity, browsing, or learning something new, images help exploration. If the question is precise, conceptual, or reflective, text is usually better. Compare: “What does a sociologist learn?” → mostly conceptual → text-focused “What does a Venus flytrap look like and how does it work?” → exploratory → images help 5. I don’t assume images are always helpful I treat images as supporting tools, not default additions. If text already does the job cleanly, I leave images out. In short, I include images when they clarify, ground, or scaffold understanding, and avoid them when they would distract, oversimplify, or add unnecessary load. If you want, I can also explain this decision-making using learning science principles (like cognitive load and multimedia learning), which connects nicely to the kinds of questions you’ve been asking.\nDo you think it’s telling the truth?"
  },
  {
    "objectID": "posts/W14_P1_Image/5.2image.html#when-do-images-show-up",
    "href": "posts/W14_P1_Image/5.2image.html#when-do-images-show-up",
    "title": "Images and Videos are Now INCLUDED!",
    "section": "",
    "text": "I found that chatgpt does not always return images with text, so I’m curious about what can trigger it to include images. In general, I found that it only does when users ask for explanation of an idea or concept that is easily to be understood with visuals.\nThe first question I asked was: &gt; What is venus flytrap?\n\n\n\nChatGPT answer about Venuflytrap\n\n\nIt replied with the text and graphics of real venus flytrap and its structure. I can’t imagine how useful it will be for teachers getting a headache of adding visuals to their slides or people having a hard time understanding what Chatgpt is saying with a bunch of texts. Clicking more, I found that it even included a youtube video link to show how venus flytraps work.\nI also asked &gt; “How would you make a sandwich? What are some steps?”\nand\n\n“How do you Juggle?”.\n\nChatgpt responded with step by step images and even a gif for juggling.\n\n\n\nChatGPT answer about Sandwich and Juggling"
  },
  {
    "objectID": "posts/W14_P1_Image/5.2image.html#cases-that-visuals-were-inappropriately-added-or-not",
    "href": "posts/W14_P1_Image/5.2image.html#cases-that-visuals-were-inappropriately-added-or-not",
    "title": "Images and Videos are Now INCLUDED!",
    "section": "",
    "text": "When I turned to an abstract question “How do you sing a song smoothly”. Chatgpt did not respond with any visuals. That makes sense to me at first because knowing how to sing smoothly is more about understanding, but then I thought visuals can help you to see how your voices are made and where to focus on in your throat.\nAnother interesting case is when I ask “What does a sociologist learn?”, it gave me several flowcharts and slides such as\n\n\n\nChatGPT answer about Sociologist\n\n\nThis does not seem to help with my understanding. I would go for text only for this case.\nThese images and videos Chatgpt returned were not generated but searched through popular sites, mostly like knowledge bases such as wikipedia or questionpro, but you can definitely ask it to generate one if you need.\nThis new function makes me think about whether current chatbots are still useful. There are chatbots on the market that are made to help people find visual elements, learn skills, or find information about a specific field. As Artificial General Intelligence is getting more and more powerful, how do we find a market for a LLM application?\nRightnow the directions I can think of is 1) Make your knowledge base distinct so that your AI agent provides more professional and accurate information than others, 2) Enlarging the possibility for users to create, make the transition from generating visual to editing and creating faster (eg. Figma make), and 3) be more targeted on multimodal outputs so that you prioritize a certain media type over another for appropriate situations, instead of throwing everything to users to process."
  },
  {
    "objectID": "posts/W14_P1_Image/5.2image.html#official-answer",
    "href": "posts/W14_P1_Image/5.2image.html#official-answer",
    "title": "Images and Videos are Now INCLUDED!",
    "section": "",
    "text": "With all these thoughts, I asked Chatgpt how it decides to include visuals in its answer to verify my guess. Here’s what it said:\nGood question. I don’t include images automatically. I make a judgment call based on how images would affect understanding or usefulness of the answer. Here’s how I decide, in plain terms. 1. Does the topic benefit from visual grounding? If the question is about objects, processes, places, people, or physical actions, images often help readers quickly orient themselves. Examples where images help: plants, animals, cooking, crafts, geography, sports skills Examples where they usually don’t: abstract concepts, definitions, arguments, reflections, ethics So “What is a Venus flytrap?” benefits from images. “How does cognitive load theory work?” usually doesn’t. 2. Is the task about how something looks or how something works physically? For step-based or motor tasks (juggling, cooking, assembling something), images can reduce confusion by showing: posture or hand position sequencing spatial relationships That’s why I included images for juggling and sandwich-making. 3. Will images reduce cognitive effort rather than add it? I avoid images when they would: distract from the core idea duplicate information already clear in text increase cognitive load instead of reducing it For example, your question “How do you sing a song smoothly” focused on internal control and technique, so text alone was more appropriate. 4. Is the user likely exploring or just asking? If the question suggests curiosity, browsing, or learning something new, images help exploration. If the question is precise, conceptual, or reflective, text is usually better. Compare: “What does a sociologist learn?” → mostly conceptual → text-focused “What does a Venus flytrap look like and how does it work?” → exploratory → images help 5. I don’t assume images are always helpful I treat images as supporting tools, not default additions. If text already does the job cleanly, I leave images out. In short, I include images when they clarify, ground, or scaffold understanding, and avoid them when they would distract, oversimplify, or add unnecessary load. If you want, I can also explain this decision-making using learning science principles (like cognitive load and multimedia learning), which connects nicely to the kinds of questions you’ve been asking.\nDo you think it’s telling the truth?"
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html",
    "href": "posts/W6_P2_Receipe/receipe.html",
    "title": "Chef LLM",
    "section": "",
    "text": "As someone who pays attention to balance in daily diet and genuinely enjoys cooking, I often turn to ChatGPT for quick suggestions about what to eat in a day. Those interactions are usually practical and lightweight, focused on efficiency rather than creativity.\nThis time, I wanted to explore a different question. Instead of asking an LLM to recommend food, I wanted to see whether it could participate in the creative process of inventing a dish, and further, whether it could help think through how that dish might live in the real world as part of a restaurant menu and a marketing strategy. This post reflects on the process of how I used an LLM not just to generate a recipe, but to understand novelty, presentation, and storytelling around food.\n\n\n\nI did not begin by asking for a traditional Chinese recipe. Instead, I framed the prompt around novelty and reinterpretation.\nMy prompt: &gt; “Create a recipe for a novel Chinese food.”\nThis prompt was intentionally open-ended. I avoided specifying ingredients or techniques because I wanted to see how the model would reason about what “novel” might mean in a culinary context.\nLLM response (excerpt): &gt; “This dish blends traditional Cantonese dim sum with floral osmanthus fragrance and a modern twist, transforming black vinegar into edible pearls rather than a dipping sauce.”\nAfter generating the core idea, I asked the model to help visualize the recipe as a step-by-step process. This was not only about making the dish easier to follow. It was also about understanding how LLMs translate creative work into structured systems. The resulting recipe broke cooking into clear stages: making the osmanthus syrup, preparing the filling, forming the wrappers, assembling the dumplings, and creating the vinegar pearls. Each step was treated as a transformation, with inputs and outputs clearly defined. This structure felt familiar from a technical perspective. It resembled how engineers describe pipelines or workflows. In this sense, the LLM made the creative process legible by turning it into a system. This also revealed a strength of LLMs: They are particularly effective at reducing ambiguity by imposing structure, even in domains that are traditionally intuitive or sensory.\nWhat stood out to me was that the model did not only list ingredients. It framed the recipe conceptually. It identified contrast, transformation, and reinterpretation as sources of novelty. This made me realize that prompting an LLM is less like asking a question and more like setting constraints for exploration. The openness of the prompt invited conceptual reasoning rather than procedural output.\nOnce the core idea emerged, I continued prompting in a conversational way. Instead of asking for a final answer, I treated the interaction as an ongoing design dialogue.\nMy prompt: &gt; “Create images to help people understand the recipe and make it easy to follow.”\nLLM response (excerpt): &gt; “Here is an illustrated step-by-step recipe guide that visualizes ingredients, preparation stages, and assembly to reduce cognitive load.” &gt;\n\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Receipes\n\n\nAt this stage, I noticed a shift. The model began to translate cooking into a systematic workflow. Each step became a clear transformation, similar to stages in a technical pipeline.\nThis revealed a strength of LLMs. They naturally impose structure, even on domains that are typically intuitive or sensory. Cooking, in this interaction, became a sequence of clearly defined processes.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Photo\n\nWhen the conversation moved to plating design, I encountered a limitation.\nMy reflection: &gt; “This really looks like xiao long bao. It just adds vinegar as bubbles rather than on the plate. Is it novel?”\nThis moment was important. The LLM had optimized toward a visually recognizable form. While technically correct, it conflicted with my goal of novelty.\nLLM response (excerpt): &gt; “You are right to notice that the dumplings resemble xiao long bao. The novelty lies more in flavor design and presentation rather than form.”\nWhile this response explained the logic, it also highlighted a pattern. LLMs often default to familiar solutions unless explicitly pushed away from them.\nI followed up by narrowing the constraint.\nMy prompt: &gt; “Yes, add more novelty.”\nThis small change in prompting produced a meaningful shift. The plating evolved toward crescent-shaped dumplings, minimalist composition, and negative space. The dish began to look less like a variation on an existing classic and more like a contemporary reinterpretation.\nThis reinforced an important lesson. Novelty does not emerge automatically. It requires critique, iteration, and intentional redirection from the human side.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Plating Design\n\n\n\n\nAfter the recipe and plating felt coherent, I extended the conversation beyond cooking.\nMy prompt: &gt; “How would this fit into current restaurants and menus? Where would it be served and to whom?”\nThe LLM responded by situating the dish within fine dining, modern dim sum houses, and cocktail-focused restaurants. It emphasized visual consistency, storytelling, and social media shareability.\nAt this point, the dish transformed into something else. It became a branding artifact. It was no longer only about taste, but about identity, audience, and experience.\nThis cross-domain reasoning felt natural within the dialogue. It demonstrated how LLMs can connect creative production with strategic thinking, even when those domains are rarely considered together.\n\n\n\nThis experiment led me to several reflections about working with LLMs creatively.\n\nPrompts are not neutral. They shape the space of possible outputs. Open prompts encourage conceptual exploration, while constrained prompts guide refinement.\nLLMs are most effective when treated as collaborators rather than authorities. They generate possibilities quickly, but they rely on human judgment to evaluate whether those possibilities are meaningful.\nNovelty requires friction. Questioning the output and pushing back is often more productive than refining prompts endlessly.\n\nFinally, the real value of LLMs emerges when they operate across domains. The strength of this process was not the recipe itself, but the ability to move fluidly between food design, visual presentation, and marketing strategy.\n\n\n\nThis project began with a simple curiosity about food and evolved into an exploration of how LLMs can support creative system thinking. Recipes, visuals, and branding became interconnected parts of the same design problem. Chef LLM is not about replacing chefs, designers, or marketers. It is about using AI as a thinking partner that expands the space of possibilities.\nIf creativity is about making choices among many options, then LLMs are powerful tools for exploration. What remains uniquely human is deciding which ideas are worth refining and which ones are worth serving."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#reflecting-on-using-large-language-models-to-design-a-recipe-and-its-story",
    "href": "posts/W6_P2_Receipe/receipe.html#reflecting-on-using-large-language-models-to-design-a-recipe-and-its-story",
    "title": "Chef LLM",
    "section": "",
    "text": "As someone who pays attention to balance in daily diet and genuinely enjoys cooking, I often turn to ChatGPT for quick suggestions about what to eat in a day. Those interactions are usually practical and lightweight, focused on efficiency rather than creativity.\nThis time, I wanted to explore a different question. Instead of asking an LLM to recommend food, I wanted to see whether it could participate in the creative process of inventing a dish, and further, whether it could help think through how that dish might live in the real world as part of a restaurant menu and a marketing strategy. This post reflects on the process of how I used an LLM not just to generate a recipe, but to understand novelty, presentation, and storytelling around food."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#prompting-for-a-novel-recipe",
    "href": "posts/W6_P2_Receipe/receipe.html#prompting-for-a-novel-recipe",
    "title": "Chef LLM",
    "section": "",
    "text": "I did not begin by asking for a traditional Chinese recipe. Instead, I framed the prompt around novelty and reinterpretation.\nMy prompt: &gt; “Create a recipe for a novel Chinese food.”\nThis prompt was intentionally open-ended. I avoided specifying ingredients or techniques because I wanted to see how the model would reason about what “novel” might mean in a culinary context.\nLLM response (excerpt): &gt; “This dish blends traditional Cantonese dim sum with floral osmanthus fragrance and a modern twist, transforming black vinegar into edible pearls rather than a dipping sauce.”\nAfter generating the core idea, I asked the model to help visualize the recipe as a step-by-step process. This was not only about making the dish easier to follow. It was also about understanding how LLMs translate creative work into structured systems. The resulting recipe broke cooking into clear stages: making the osmanthus syrup, preparing the filling, forming the wrappers, assembling the dumplings, and creating the vinegar pearls. Each step was treated as a transformation, with inputs and outputs clearly defined. This structure felt familiar from a technical perspective. It resembled how engineers describe pipelines or workflows. In this sense, the LLM made the creative process legible by turning it into a system. This also revealed a strength of LLMs: They are particularly effective at reducing ambiguity by imposing structure, even in domains that are traditionally intuitive or sensory.\nWhat stood out to me was that the model did not only list ingredients. It framed the recipe conceptually. It identified contrast, transformation, and reinterpretation as sources of novelty. This made me realize that prompting an LLM is less like asking a question and more like setting constraints for exploration. The openness of the prompt invited conceptual reasoning rather than procedural output.\nOnce the core idea emerged, I continued prompting in a conversational way. Instead of asking for a final answer, I treated the interaction as an ongoing design dialogue.\nMy prompt: &gt; “Create images to help people understand the recipe and make it easy to follow.”\nLLM response (excerpt): &gt; “Here is an illustrated step-by-step recipe guide that visualizes ingredients, preparation stages, and assembly to reduce cognitive load.” &gt;\n\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Receipes\n\n\nAt this stage, I noticed a shift. The model began to translate cooking into a systematic workflow. Each step became a clear transformation, similar to stages in a technical pipeline.\nThis revealed a strength of LLMs. They naturally impose structure, even on domains that are typically intuitive or sensory. Cooking, in this interaction, became a sequence of clearly defined processes.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Photo\n\nWhen the conversation moved to plating design, I encountered a limitation.\nMy reflection: &gt; “This really looks like xiao long bao. It just adds vinegar as bubbles rather than on the plate. Is it novel?”\nThis moment was important. The LLM had optimized toward a visually recognizable form. While technically correct, it conflicted with my goal of novelty.\nLLM response (excerpt): &gt; “You are right to notice that the dumplings resemble xiao long bao. The novelty lies more in flavor design and presentation rather than form.”\nWhile this response explained the logic, it also highlighted a pattern. LLMs often default to familiar solutions unless explicitly pushed away from them.\nI followed up by narrowing the constraint.\nMy prompt: &gt; “Yes, add more novelty.”\nThis small change in prompting produced a meaningful shift. The plating evolved toward crescent-shaped dumplings, minimalist composition, and negative space. The dish began to look less like a variation on an existing classic and more like a contemporary reinterpretation.\nThis reinforced an important lesson. Novelty does not emerge automatically. It requires critique, iteration, and intentional redirection from the human side.\n\n\n\n\nOsmanthus-Scented Shrimp Dumplings Plating Design"
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#extending-the-recipe-into-marketing-and-branding",
    "href": "posts/W6_P2_Receipe/receipe.html#extending-the-recipe-into-marketing-and-branding",
    "title": "Chef LLM",
    "section": "",
    "text": "After the recipe and plating felt coherent, I extended the conversation beyond cooking.\nMy prompt: &gt; “How would this fit into current restaurants and menus? Where would it be served and to whom?”\nThe LLM responded by situating the dish within fine dining, modern dim sum houses, and cocktail-focused restaurants. It emphasized visual consistency, storytelling, and social media shareability.\nAt this point, the dish transformed into something else. It became a branding artifact. It was no longer only about taste, but about identity, audience, and experience.\nThis cross-domain reasoning felt natural within the dialogue. It demonstrated how LLMs can connect creative production with strategic thinking, even when those domains are rarely considered together."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#reflections-on-prompting-and-creativity",
    "href": "posts/W6_P2_Receipe/receipe.html#reflections-on-prompting-and-creativity",
    "title": "Chef LLM",
    "section": "",
    "text": "This experiment led me to several reflections about working with LLMs creatively.\n\nPrompts are not neutral. They shape the space of possible outputs. Open prompts encourage conceptual exploration, while constrained prompts guide refinement.\nLLMs are most effective when treated as collaborators rather than authorities. They generate possibilities quickly, but they rely on human judgment to evaluate whether those possibilities are meaningful.\nNovelty requires friction. Questioning the output and pushing back is often more productive than refining prompts endlessly.\n\nFinally, the real value of LLMs emerges when they operate across domains. The strength of this process was not the recipe itself, but the ability to move fluidly between food design, visual presentation, and marketing strategy."
  },
  {
    "objectID": "posts/W6_P2_Receipe/receipe.html#some-thoughts",
    "href": "posts/W6_P2_Receipe/receipe.html#some-thoughts",
    "title": "Chef LLM",
    "section": "",
    "text": "This project began with a simple curiosity about food and evolved into an exploration of how LLMs can support creative system thinking. Recipes, visuals, and branding became interconnected parts of the same design problem. Chef LLM is not about replacing chefs, designers, or marketers. It is about using AI as a thinking partner that expands the space of possibilities.\nIf creativity is about making choices among many options, then LLMs are powerful tools for exploration. What remains uniquely human is deciding which ideas are worth refining and which ones are worth serving."
  },
  {
    "objectID": "posts/W13_P1_SocialGood/socialgood.html",
    "href": "posts/W13_P1_SocialGood/socialgood.html",
    "title": "Governing AI for Social Good",
    "section": "",
    "text": "Governing AI for Social Good\nI attended a talk or seminar event on policy making for AI and social good by the school of Social Policy & Practice at Penn on December 8th. In the event, Dr. Ioana E. Marinescu, who does research at National Bureau of Economic Research, and Dr. Desmond Upton Patton, who also sits on advisory board of companies including TikTok, were invited to share thoughts on social policy regarding AI and practices between industry and academia. The discussion focused on how social policy, institutional design, and lived experience should shape how AI systems are built and governed.\n\n\n\nAI for Social Good Event\n\n\nA central theme in Dr. Patton’s remarks was the role of advisory councils and community stakeholders in AI development. He emphasized that decisions about AI systems should not be made only by engineers or executives, but should include voices from the communities most affected by these technologies. This includes people whose daily lives are shaped by AI-driven platforms, not just those who profit from them. Advisory work, as he described it, is fundamentally different from consulting. It is less about optimizing performance metrics and more about surfacing non-negotiable values and harms early, before products are built and deployed at scale. Scale itself emerged as one of the biggest risks. Platforms with billions of users can amplify existing inequalities at unprecedented speed. Dr. Patton used examples from social media to illustrate how AI-generated content can spread misinformation, especially when it targets or reaches children under 18 or mimics authoritative figures such as politicians. These cases raise difficult questions about what kinds of restrictions should exist, and whose voices should shape those restrictions. Parents, educators, and community advocates often understand the social context in ways that technical systems do not. As Dr. Patton noted, AI does not truly “understand” context. It can remix lyrics, images, or speech patterns, but it does not reason about meaning, intent, or consequence in the way humans do. This gap becomes especially visible in classrooms, where clear policies on acceptable AI use are often still missing.\nAlongside risks, Dr. Patton also spoke about opportunities. He challenged the audience to think about how care, joy, and emotional well-being might be integrated into AI systems, rather than treating efficiency or engagement as the only goals. He proposed using “joy” as an entry point for conversations about AI design, asking what kinds of human experiences systems should protect or enhance. This framing stood out to me because it shifts AI governance from a reactive model, focused only on harm mitigation, to a more generative one that asks what social values we want technology to embody.\nDr. Marinescu approached AI governance from an economic and labor perspective. She situated AI within a longer history of technological change, drawing parallels to the computer and the internet, which ultimately increased productivity and economic growth. At the same time, she was clear that these gains are not evenly distributed. New technologies restructure jobs, leading to displacement, role changes, and uncertainty. AI, she argued, disproportionately affects “intelligence jobs,” meaning work that relies heavily on information processing and computation. In contrast, many physical or in-person roles, such as teaching face-to-face, caregiving, or service work, remain deeply human and harder to automate.\nFrom this perspective, the core policy challenge is not unique to AI, but to technological change more broadly. The question is how to protect people from the negative effects of restructuring, rather than trying to stop innovation altogether. Dr. Marinescu highlighted policy tools such as unemployment benefits and wage insurance as ways to provide economic security during transitions. She also discussed the idea of slowing down AI deployment through regulation or other instruments, not to halt progress, but to give institutions and workers time to adapt.\nDuring the Q&A, several tensions became more explicit. One concern was that regulation, if poorly designed, can actually worsen inequality by favoring large companies that can afford compliance, while making it harder for startups to compete. Another discussion focused on how physical and intelligence-based jobs can be complementary rather than oppositional. For example, digital marketing tools can support a local coffee shop, while the barista role itself remains human-centered. The relative cost of robots versus human labor, as well as issues of safety and mental health, further complicate these trade-offs.\nWhat stayed with me most from the conversation was the emphasis on values and voice. Whether through advisory councils, economic policy, or institutional regulation, governing AI for social good requires clarity about what is negotiable and what is not. It also requires acknowledging that AI governance is not just a technical problem, but a social one. The memo, as Dr. Patton suggested, reflects values. Whose values are written into that memo, and whose are left out, may ultimately shape how AI systems affect our lives far more than any single algorithmic breakthrough.\nIn reflecting on this talk, I came away with a stronger sense that governing AI for social good is less about finding a single correct policy and more about building ongoing processes of care, accountability, and adaptation. Both speakers underscored that AI systems do not exist in a vacuum. They are embedded in economic structures, social relationships, and power dynamics that shape who benefits and who bears the costs. Meaningful governance therefore requires listening to community voices early, protecting people through thoughtful social policy, and resisting the urge to treat speed and scale as unquestioned virtues. As AI continues to reshape work, communication, and education, the challenge is not simply to manage risk, but to articulate and defend the human values we want technology to serve."
  }
]