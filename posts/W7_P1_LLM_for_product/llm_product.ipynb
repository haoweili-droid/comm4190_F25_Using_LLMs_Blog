{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d29df62-c99e-4430-9006-35d42f5af239",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prototyping with LLM\"\n",
    "description: \"LLM for Products\" \n",
    "author: \"Haowei\"\n",
    "date: \"10/7/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Creativity\n",
    "  - Product Design\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dab6f-19f0-4b65-980c-bd5975ef767d",
   "metadata": {},
   "source": [
    "# Prototyping with LLM\n",
    "Before building with large language models, most of my experience with them came from direct interaction: asking questions, refining prompts, and getting surprisingly fluent responses. In those moments, LLMs felt powerful and flexible. However, when I started thinking about using an LLM as part of a real product rather than as a personal assistant, my expectations had to change.\n",
    "A product needs consistency, predictability, and clear boundaries. It also needs to support users who may not know how to prompt well or even what to ask. My goal was to prototype an AI-supported learning workflow where the model would help users reflect on their work, generate study guidance, or scaffold thinking rather than simply provide answers. On paper, this seemed like a natural fit for an LLM.\n",
    "The challenge was to move from “this works when I talk to it” to “this works reliably for someone else.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c577fc-1aa0-4517-bf23-72ae483365ed",
   "metadata": {},
   "source": [
    "### Testing the Idea: Prototyping with LLMs in a Workflow\n",
    "\n",
    "To test this, I began embedding LLM outputs into structured workflows. Instead of open-ended chat, I constrained the model with specific roles, instructions, and expected formats. For example, I asked it to analyze student responses, identify misconceptions, or generate targeted reflection questions based on predefined learning goals.\n",
    "\n",
    "I also tested the same prompt across multiple inputs to see how stable the outputs were. In theory, if the instructions were clear enough, the model should behave consistently. I treated these prompts almost like product logic: if the input looks like this, the output should follow a predictable pattern.\n",
    "As I iterated, I realized that many prompts that worked perfectly in isolation started to break when placed inside a system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9994d20-832a-4cbb-b732-4f01634a196c",
   "metadata": {},
   "source": [
    "![AI Workflow](product.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23985c56-dac1-4b7b-a2b3-a7b091e38c44",
   "metadata": {},
   "source": [
    "### The Limits I Ran Into\n",
    "\n",
    "The first thing that broke was consistency. Even with carefully written prompts, the model’s responses varied more than I expected. Sometimes the tone shifted. Sometimes the depth of explanation changed. For a human reader, these differences might feel minor, but in a product context, they created uneven user experiences.\n",
    "The second issue was overhelpfulness. When the model was designed to support learning, it often gave too much away. Instead of scaffolding thinking, it sometimes jumped straight to polished explanations. This undermined the original learning goal and made it difficult to control cognitive load. What felt helpful in a demo became problematic in a real educational workflow.\n",
    "\n",
    "Another thing that broke was assumption alignment. The model frequently made assumptions about user intent that were not always correct. If a student response was ambiguous, the AI might confidently interpret it one way, even when multiple interpretations were possible. In a product, this kind of confident guessing can mislead users rather than support them.\n",
    "\n",
    "Finally, prompt fragility became clear. Small changes in wording, input length, or user tone sometimes led to disproportionately different outputs. This meant the system was more sensitive than expected, which made it hard to design robust guardrails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ae2a4-70b0-43b2-b85b-9767d415ab12",
   "metadata": {},
   "source": [
    "### Reflection:\n",
    "This experience changed how I think about LLMs in products. I no longer see them as plug-and-play intelligence. Instead, they feel more like probabilistic components that require careful framing, monitoring, and constraint. What works in a chat interface does not automatically translate into a reliable system.\n",
    "I also realized that many of the failures were not technical but conceptual. I initially treated the model as if it understood my design intentions. In reality, it only responds to what is explicitly encoded in prompts and structure. Any ambiguity I left behind showed up later as unexpected behavior.\n",
    "Perhaps most importantly, this process highlighted the need to design not just for what the AI can do, but for what users need. In learning contexts especially, more fluent output is not always better. Sometimes the most valuable role for AI is to slow things down, ask better questions, or surface uncertainty rather than resolve it.\n",
    "In the end, what broke was my assumption that intelligence alone makes a good product. Building with LLMs forced me to confront the gap between impressive demos and meaningful, responsible design. That gap is not a failure of the technology, but a reminder that products are systems, not conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0a782-db31-4c77-95d0-0f6f00681c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
