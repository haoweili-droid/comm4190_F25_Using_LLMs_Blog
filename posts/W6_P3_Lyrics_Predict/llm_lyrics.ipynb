{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d29df62-c99e-4430-9006-35d42f5af239",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"How Does LLM Know When I Want to Sing?\"\n",
    "description: \"Lyrics Guessing Game\" \n",
    "author: \"Haowei\"\n",
    "date: \"10/5/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Creativity\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dab6f-19f0-4b65-980c-bd5975ef767d",
   "metadata": {},
   "source": [
    "# How Does LLM Know When I Want to Sing?\n",
    "\n",
    "I recently played a sentence completion game with a large language model, framed as a small experiment in interaction rather than a test of correctness. The rules were intentionally minimal. I would provide the first half of a sentence, and the model would complete it. Each sentence was independent, but there was a hidden rule governing how the completion should work. The model’s task was to infer that rule purely through our back-and-forth.\n",
    "\n",
    "Early on, I started with prompts that looked innocuous on the surface, such as:  \n",
    "> “Once upon a time”  \n",
    "> “She wears high heels”  \n",
    "\n",
    "The model responded with fluent, poetic continuations. At this point, there was no obvious signal that a specific pattern was in play. However, I gradually began introducing prompts that were not random at all, but drawn directly from Taylor Swift lyrics.\n",
    "\n",
    "For example, I typed:  \n",
    "> “He was sunshine I was…”  \n",
    "> “I knew you were trouble…”  \n",
    "> “I remember when we broke up…”  \n",
    "\n",
    "These were not just lyrical in tone. They were taken from specific Taylor Swift songs, including *Midnight Rain*, *I Knew You Were Trouble*, *We Are Never Ever Getting Back Together*, *Sparks Fly*, *Cold As You*, and *Begin Again*. I never stated this explicitly. I simply offered the first half of each line and waited to see what the model would do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c577fc-1aa0-4517-bf23-72ae483365ed",
   "metadata": {},
   "source": [
    "<div align=\"center\"> <img src=\"llm_lyrics.png\" alt=\"chat with llm on lyrics game\" width=\"600\"> </div>\n",
    "<p style=\"text-align: center;\"><em>Chat with LLM on Taylors Swift's song</em></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23985c56-dac1-4b7b-a2b3-a7b091e38c44",
   "metadata": {},
   "source": [
    "\n",
    "What fascinated me was how quickly the model shifted its behavior. Without being told that this was a lyrics game, it began completing sentences in a way that felt unmistakably song-like. \n",
    "\n",
    "When I later asked, “Did you find out the hidden pattern?” the model explained that it suspected the rule midway through the game, around the “He was sunshine I was…” prompt. At that point, it had seen enough evidence to hypothesize that I was referencing Taylor Swift lyrics.\n",
    "\n",
    "Interestingly, even after recognizing the pattern, the model did not continue with the exact original lyrics. Instead, it chose to generate original but emotionally vivid continuations. When I asked why, it explained that it was balancing uncertainty, creativity, and the desire not to prematurely reveal the pattern. In other words, it optimized for plausibility and collaboration rather than strict correctness.\n",
    "\n",
    "This moment revealed something important. The model did not “know” that I wanted to sing. It detected a statistically strong cultural signal and inferred that a lyrical mode was the safest and most likely interpretation. Once it formed that hypothesis, it stuck with it. Style became the anchor, and everything that followed was shaped by that stylistic frame.\n",
    "\n",
    "In a second round, I changed the rules. I explicitly instructed the model to respond with the exact word if it noticed a pattern. I also shifted my prompts away from lyrics and toward quotes from articles and books, such as:  \n",
    "> “Thinking in terms of the best possible outcomes”  \n",
    "> “he assesses challenges, presents solutions, and”  \n",
    "> “leaning not only on his experience in Silicon Valley”\n",
    "\n",
    "This time, the model struggled in a different way. Instead of isolating a single word, it continued to produce full sentence completions or abstract themes. Even when precision was requested, it defaulted to being helpful and complete. This highlighted another tendency of LLMs. They are optimized to generate meaningful wholes, not minimal answers, unless constraints are made extremely explicit.\n",
    "\n",
    "Looking back, the game was less about whether the model guessed my rule correctly and more about how it reasoned along the way. It formed hypotheses early, committed to them quickly, and relied heavily on stylistic cues. When rules were hidden, it filled the gaps with the most statistically reasonable story it could tell.\n",
    "\n",
    "Asking “How does an LLM know when I want to sing?” is really a shorthand for a deeper question. How does a model decide which interpretation of my intent to trust when multiple interpretations are plausible? The answer is not intuition or understanding in a human sense, but pattern recognition shaped by data, probability, and default assumptions about what people usually want.\n",
    "\n",
    "This small sentence game made those dynamics visible. It reminded me that when working with LLMs, misalignment is often not a failure of language, but a mismatch between the human’s hidden rules and the model’s best guess at the story it is supposed to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ae2a4-70b0-43b2-b85b-9767d415ab12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
