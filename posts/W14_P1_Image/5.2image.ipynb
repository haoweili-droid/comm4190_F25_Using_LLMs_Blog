{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d29df62-c99e-4430-9006-35d42f5af239",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Images and Videos are Now INCLUDED!\"\n",
    "description: \"Interesting finding of gpt 5.2\" \n",
    "author: \"Haowei\"\n",
    "date: \"12/11/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Learning\n",
    "  - Multimedia\n",
    "  - Multimodal\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dab6f-19f0-4b65-980c-bd5975ef767d",
   "metadata": {},
   "source": [
    "# Interesting finding of gpt 5.2: Images and Video included! \n",
    "Last week, gpt 5.2 was announced. With the update, I immediately founded two new features with Chatgpt: 1) chatgpt includes illustrating images while giving text responses, and 2) users can now invite others to join the conversation and form a group chat with Chatgpt.\n",
    "\n",
    "We will focus on the first one today and talk about the collaboration function next time. \n",
    "\n",
    "## When do images show up?\n",
    "I found that chatgpt does not always return images with text, so I’m curious about what can trigger it to include images. In general, I found that it only does when users ask for explanation of an idea or concept that is easily to be understood with visuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ef122-6879-4cec-b0a9-3e574fca4e93",
   "metadata": {},
   "source": [
    "The first question I asked was:\n",
    "> What is venus flytrap? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f898228-cf0d-4c26-bc57-6570565234e0",
   "metadata": {},
   "source": [
    "![ChatGPT answer about Venuflytrap](flytrap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f24db-3068-4601-ad89-aa188c6af39e",
   "metadata": {},
   "source": [
    "It replied with the text and graphics of real venus flytrap and its structure. I can’t imagine how useful it will be for teachers getting a headache of adding visuals to their slides or people having a hard time understanding what Chatgpt is saying with a bunch of texts. Clicking more, I found that it even included a youtube video link to show how venus flytraps work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a397d-0d74-467e-9f5f-72611a00029d",
   "metadata": {},
   "source": [
    "I also asked \n",
    "> “How would you make a sandwich? What are some steps?”\n",
    "\n",
    "and\n",
    "\n",
    "> “How do you Juggle?”.\n",
    "\n",
    "Chatgpt responded with step by step images and even a gif for juggling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047525d-7a3b-45a8-b139-4b2a7e1ef270",
   "metadata": {},
   "source": [
    "![ChatGPT answer about Sandwich and Juggling](sandwich.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7f483-6e24-4609-8f18-1a7cc0b36102",
   "metadata": {},
   "source": [
    "## Cases that visuals were inappropriately added (or not)\n",
    "When I turned to an abstract question “How do you sing a song smoothly”. Chatgpt did not respond with any visuals. That makes sense to me at first because knowing how to sing smoothly is more about understanding, but then I thought visuals can help you to see how your voices are made and where to focus on in your throat. \n",
    "\n",
    "Another interesting case is when I ask “What does a sociologist learn?”, it gave me several flowcharts and slides such as \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da95a7-521d-4fa8-8653-08e5a2f9f64b",
   "metadata": {},
   "source": [
    "![ChatGPT answer about Sociologist](sociology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e6b30-79b9-4028-b809-57a45d70597b",
   "metadata": {},
   "source": [
    "This does not seem to help with my understanding. I would go for text only for this case. \n",
    "\n",
    "These images and videos Chatgpt returned were not generated but searched through popular sites, mostly like knowledge bases such as wikipedia or questionpro, but you can definitely ask it to generate one if you need. \n",
    "\n",
    "This new function makes me think about whether current chatbots are still useful. There are chatbots on the market that are made to help people find visual elements, learn skills, or find information about a specific field. As Artificial General Intelligence is getting more and more powerful, how do we find a market for a LLM application?\n",
    "\n",
    "Rightnow the directions I can think of is 1) Make your knowledge base distinct so that your AI agent provides more professional and accurate information than others, 2) Enlarging the possibility for users to create, make the transition from generating visual to editing and creating faster (eg. Figma make), and 3) be more targeted on multimodal outputs so that you prioritize a certain media type over another for appropriate situations, instead of throwing everything to users to process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13c76a-9746-421c-887c-99d97c0775f6",
   "metadata": {},
   "source": [
    "## Official Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa3b19-bb3f-4f51-baf0-21c0493d569c",
   "metadata": {},
   "source": [
    "With all these thoughts, I asked Chatgpt how it decides to include visuals in its answer to verify my guess. Here’s what it said:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f227bd8-a36d-4dea-8920-746a1a59779a",
   "metadata": {},
   "source": [
    "Good question. I don’t include images automatically. I make a judgment call based on how images would affect understanding or usefulness of the answer.\n",
    "Here’s how I decide, in plain terms.\n",
    "1. Does the topic benefit from visual grounding?\n",
    "If the question is about objects, processes, places, people, or physical actions, images often help readers quickly orient themselves.\n",
    "Examples where images help: plants, animals, cooking, crafts, geography, sports skills\n",
    "Examples where they usually don’t: abstract concepts, definitions, arguments, reflections, ethics\n",
    "So “What is a Venus flytrap?” benefits from images.\n",
    "“How does cognitive load theory work?” usually doesn’t.\n",
    "2. Is the task about how something looks or how something works physically?\n",
    "For step-based or motor tasks (juggling, cooking, assembling something), images can reduce confusion by showing:\n",
    "posture or hand position\n",
    "sequencing\n",
    "spatial relationships\n",
    "That’s why I included images for juggling and sandwich-making.\n",
    "3. Will images reduce cognitive effort rather than add it?\n",
    "I avoid images when they would:\n",
    "distract from the core idea\n",
    "duplicate information already clear in text\n",
    "increase cognitive load instead of reducing it\n",
    "For example, your question “How do you sing a song smoothly” focused on internal control and technique, so text alone was more appropriate.\n",
    "4. Is the user likely exploring or just asking?\n",
    "If the question suggests curiosity, browsing, or learning something new, images help exploration.\n",
    "If the question is precise, conceptual, or reflective, text is usually better.\n",
    "Compare:\n",
    "“What does a sociologist learn?” → mostly conceptual → text-focused\n",
    "“What does a Venus flytrap look like and how does it work?” → exploratory → images help\n",
    "5. I don’t assume images are always helpful\n",
    "I treat images as supporting tools, not default additions. If text already does the job cleanly, I leave images out.\n",
    "In short, I include images when they clarify, ground, or scaffold understanding, and avoid them when they would distract, oversimplify, or add unnecessary load.\n",
    "If you want, I can also explain this decision-making using learning science principles (like cognitive load and multimedia learning), which connects nicely to the kinds of questions you’ve been asking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e2e45-2789-47a4-acfe-e0d0ae9ea4c9",
   "metadata": {},
   "source": [
    "**Do you think it’s telling the truth?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b8d5d-a68a-4760-92b4-c9437bd63dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
