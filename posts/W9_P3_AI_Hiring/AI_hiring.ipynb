{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d29df62-c99e-4430-9006-35d42f5af239",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Should AI be used to evaluate Human?\"\n",
    "description: \"Thoughts After Reading Harvard Business Review Article\" \n",
    "author: \"Haowei\"\n",
    "date: \"12/16/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Ethics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dab6f-19f0-4b65-980c-bd5975ef767d",
   "metadata": {},
   "source": [
    "## Should AI be used to evaluate human?\n",
    "\n",
    "Recently, I have increasingly encountered situations where artificial intelligence is used to evaluate people. One of the most visible examples is hiring. In many recruitment processes, AI systems now filter résumés by scanning for keywords, educational backgrounds, or patterns associated with past “successful” employees. According to Harvard Business Review, nearly 90% of companies use some form of AI in hiring today. While these tools are often promoted as a way to reduce human bias, critics argue that they can be rigid and exclusionary, filtering out nontraditional candidates whose potential does not align neatly with predefined criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f2cb8-4ca2-4728-84ea-32648bde328d",
   "metadata": {},
   "source": [
    "![Harvard Research on Hiring Fairness](HBR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66142dda-bc86-4623-83cc-7dd417fd3e16",
   "metadata": {},
   "source": [
    "\n",
    "The Harvard article complicates this debate by suggesting that the real issue is not whether AI is more or less biased than humans, but that AI quietly locks in a particular definition of fairness. In the case study described, an algorithm designed to promote fairness privileged consistency and standardization, sidelining contextual judgment from managers who understood local needs and individual trajectories. Over time, this algorithmic definition of fairness became dominant, even when it conflicted with human expertise. This resonates with common concerns about résumé screening tools that favor candidates who look similar to existing employees, potentially narrowing rather than diversifying the talent pool.\n",
    "\n",
    "Beyond résumé screening, AI is increasingly used in interviews. Some companies now deploy AI-driven interviewers or digital avatars that adapt questions based on a candidate’s responses. While this may increase efficiency and scalability, it fundamentally alters the nature of an interview. Interviews are traditionally a two-way exchange: candidates are evaluated, but they are also assessing the organization. When AI replaces a human interviewer, the interaction can feel closer to recording a presentation than engaging in a conversation. This not only limits the interviewer’s ability to probe nuance, but also deprives candidates of the chance to build understanding, ask questions, and read interpersonal cues.\n",
    "\n",
    "The implications of AI evaluation extend far beyond hiring. Any context involving assessment can be affected. In education, for example, automated grading systems are increasingly used to evaluate student work. While these tools may perform adequately on closed-ended questions, they struggle with open-ended responses that require interpretation, creativity, or emotional sensitivity. An AI grader that is overly strict, overly lenient, or indifferent in tone can undermine students’ confidence and motivation. As the Harvard article notes, once evaluative thresholds are encoded into a system, they can become rigid boundaries that are difficult to challenge or revise. At the same time, my skepticism toward AI evaluation has been unsettled by a comparison with human judgment. Humans are also subjective, inconsistent, and easily influenced by context, mood, or prior experiences. A hiring manager may be primed by a single word on a résumé or by the candidate they interviewed just before. These forms of bias are rarely scrutinized with the same intensity as algorithmic errors. As the article argues, AI systems do not create fairness on their own; people give them authority, frame them as “objective,” and decide which values they encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda352d9-677d-41a2-b565-809ebe11fd27",
   "metadata": {},
   "source": [
    "This raises a deeper question: why are we often less tolerant of AI mistakes than human ones? If AI is a human-made system trained on historical data, its biases are ultimately reflections of human choices. Yet we frequently hold AI to a higher standard of consistency and accuracy. What, then, is a reasonable tolerance for AI error or hallucination, especially in high-stakes evaluative contexts?\n",
    "\n",
    "These questions are not abstract for me. In projects like KidTalkMirror and other AI persona–based feedback tools, we actively use AI to evaluate human behavior and generate suggestions. Designing these systems forces us to confront the same dilemmas discussed in the Harvard article: whose definition of fairness is embedded in the model, which voices are excluded, and how easily those assumptions can drift over time. Rather than asking whether AI or humans are fairer, the more important task may be to continually interrogate what kind of fairness we are designing for, and who gets to define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be0e62-e718-4505-89a6-ea0bcb410638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
