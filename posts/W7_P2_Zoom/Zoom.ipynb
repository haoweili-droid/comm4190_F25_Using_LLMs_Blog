{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d29df62-c99e-4430-9006-35d42f5af239",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Using Zoom AI Companion for User Studies\"\n",
    "description: \"LLM for Needfinding\" \n",
    "author: \"Haowei\"\n",
    "date: \"10/9/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Creativity\n",
    "  - Product Design\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dab6f-19f0-4b65-980c-bd5975ef767d",
   "metadata": {},
   "source": [
    "# Using Zoom AI Companion for User Studies\n",
    "I recently used Zoom AI Companion during a one-hour user interview. I made a critical mistake: I did not turn on recording. At the time, this felt inconsequential. I assumed that the AI Companion, which was visibly active during the meeting, would still retain a usable transcript or at least allow me to recover the conversation afterward. That assumption turned out to be wrong.\n",
    "\n",
    "After the interview ended, I stayed in the Zoom room and tried to extract the full transcription through the AI Companion chat. What I received instead was a set of fragmented, partial transcripts. Worse, when I tried to prompt the system to reconstruct or summarize the full conversation, it began to hallucinate. The output included statements that were never said, connections that were never made, and smooth but inaccurate narratives that felt plausible yet fundamentally untrustworthy. At that moment, the AI Companion shifted from being a productivity aid to a liability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c577fc-1aa0-4517-bf23-72ae483365ed",
   "metadata": {},
   "source": [
    "![Zoom AI Companion](Zoom_AI_Companion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23985c56-dac1-4b7b-a2b3-a7b091e38c44",
   "metadata": {},
   "source": [
    "This experience highlighted a core limitation of large language model applications in real-world workflows: they are not memory systems. Without recording enabled, the model had no authoritative source of truth to ground its responses. What appeared to be “recall” was in fact inference. From a technical perspective, this is likely tied to token and memory constraints. The model only had access to limited conversational context, possibly cached summaries or short segments, rather than a full transcript. When prompted beyond that boundary, it did what LLMs are designed to do: generate the most statistically plausible continuation. Accuracy was replaced by coherence.\n",
    "\n",
    "For user research, this is a serious problem. Interviews depend on precise wording, tone, pauses, and unexpected turns. Subtle phrasing differences can change the interpretation of user intent. An AI-generated reconstruction that blends partial memory with invented content is worse than no transcript at all, because it introduces false confidence. As a researcher, I would rather acknowledge data loss than unknowingly analyze fabricated data.\n",
    "\n",
    "This also raises broader questions about how AI should be used in user interviews. AI companions are increasingly positioned as silent assistants: transcribing, summarizing, tagging insights in real time. When they work, they reduce cognitive load and allow researchers to stay present in the conversation. But this experience reinforced that AI should augment, not replace, robust data capture practices. Recording, explicit transcription pipelines, and clear data ownership still matter. AI summaries should always be traceable back to raw data, not treated as primary artifacts.\n",
    "\n",
    "Privacy adds another layer of tension. One reason some researchers hesitate to record interviews is concern for participant comfort and data protection. AI companions can feel like a middle ground: present but unobtrusive. However, this incident revealed an uncomfortable trade-off. When privacy-preserving choices limit data persistence, AI systems may compensate by hallucinating rather than refusing. That is not a neutral failure mode. It shifts risk from data exposure to data distortion, which can quietly undermine research integrity.\n",
    "\n",
    "Ultimately, this experience made me more cautious, not less interested, in AI-assisted research tools. LLMs are powerful interfaces for language, but they are bounded by context windows, memory design, and probabilistic generation. When those limits are invisible to users, trust erodes quickly. For AI companions to be truly useful in research settings, they need clearer affordances: explicit signals about what is stored, what is lost, and when the system is guessing rather than recalling. Until then, responsibility still sits firmly with the human researcher to design workflows that do not confuse fluency with fidelity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0a782-db31-4c77-95d0-0f6f00681c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
