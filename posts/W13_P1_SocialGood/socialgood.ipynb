{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d29df62-c99e-4430-9006-35d42f5af239",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Governing AI for Social Good\"\n",
    "description: \"Reflection attending an event\" \n",
    "author: \"Haowei\"\n",
    "date: \"11/20/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Ethics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dab6f-19f0-4b65-980c-bd5975ef767d",
   "metadata": {},
   "source": [
    "# Governing AI for Social Good \n",
    "I attended a talk or seminar event on policy making for AI and social good by the school of Social Policy & Practice at Penn on December 8th. In the event, Dr. Ioana E. Marinescu, who does research at National Bureau of Economic Research, and Dr. Desmond Upton Patton, who also sits on advisory board of companies including TikTok, were invited to share thoughts on social policy regarding AI and practices between industry and academia. The discussion focused on how social policy, institutional design, and lived experience should shape how AI systems are built and governed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f898228-cf0d-4c26-bc57-6570565234e0",
   "metadata": {},
   "source": [
    "![AI for Social Good Event](event.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f24db-3068-4601-ad89-aa188c6af39e",
   "metadata": {},
   "source": [
    "A central theme in Dr. Patton’s remarks was the role of advisory councils and community stakeholders in AI development. He emphasized that decisions about AI systems should not be made only by engineers or executives, but should include voices from the communities most affected by these technologies. This includes people whose daily lives are shaped by AI-driven platforms, not just those who profit from them. Advisory work, as he described it, is fundamentally different from consulting. It is less about optimizing performance metrics and more about surfacing non-negotiable values and harms early, before products are built and deployed at scale.\n",
    "Scale itself emerged as one of the biggest risks. Platforms with billions of users can amplify existing inequalities at unprecedented speed. Dr. Patton used examples from social media to illustrate how AI-generated content can spread misinformation, especially when it targets or reaches children under 18 or mimics authoritative figures such as politicians. These cases raise difficult questions about what kinds of restrictions should exist, and whose voices should shape those restrictions. Parents, educators, and community advocates often understand the social context in ways that technical systems do not. As Dr. Patton noted, AI does not truly “understand” context. It can remix lyrics, images, or speech patterns, but it does not reason about meaning, intent, or consequence in the way humans do. This gap becomes especially visible in classrooms, where clear policies on acceptable AI use are often still missing.\n",
    "\n",
    "Alongside risks, Dr. Patton also spoke about opportunities. He challenged the audience to think about how care, joy, and emotional well-being might be integrated into AI systems, rather than treating efficiency or engagement as the only goals. He proposed using “joy” as an entry point for conversations about AI design, asking what kinds of human experiences systems should protect or enhance. This framing stood out to me because it shifts AI governance from a reactive model, focused only on harm mitigation, to a more generative one that asks what social values we want technology to embody.\n",
    "\n",
    "Dr. Marinescu approached AI governance from an economic and labor perspective. She situated AI within a longer history of technological change, drawing parallels to the computer and the internet, which ultimately increased productivity and economic growth. At the same time, she was clear that these gains are not evenly distributed. New technologies restructure jobs, leading to displacement, role changes, and uncertainty. AI, she argued, disproportionately affects “intelligence jobs,” meaning work that relies heavily on information processing and computation. In contrast, many physical or in-person roles, such as teaching face-to-face, caregiving, or service work, remain deeply human and harder to automate.\n",
    "\n",
    "From this perspective, the core policy challenge is not unique to AI, but to technological change more broadly. The question is how to protect people from the negative effects of restructuring, rather than trying to stop innovation altogether. Dr. Marinescu highlighted policy tools such as unemployment benefits and wage insurance as ways to provide economic security during transitions. She also discussed the idea of slowing down AI deployment through regulation or other instruments, not to halt progress, but to give institutions and workers time to adapt.\n",
    "\n",
    "During the Q&A, several tensions became more explicit. One concern was that regulation, if poorly designed, can actually worsen inequality by favoring large companies that can afford compliance, while making it harder for startups to compete. Another discussion focused on how physical and intelligence-based jobs can be complementary rather than oppositional. For example, digital marketing tools can support a local coffee shop, while the barista role itself remains human-centered. The relative cost of robots versus human labor, as well as issues of safety and mental health, further complicate these trade-offs.\n",
    "\n",
    "What stayed with me most from the conversation was the emphasis on values and voice. Whether through advisory councils, economic policy, or institutional regulation, governing AI for social good requires clarity about what is negotiable and what is not. It also requires acknowledging that AI governance is not just a technical problem, but a social one. The memo, as Dr. Patton suggested, reflects values. Whose values are written into that memo, and whose are left out, may ultimately shape how AI systems affect our lives far more than any single algorithmic breakthrough.\n",
    "\n",
    "In reflecting on this talk, I came away with a stronger sense that governing AI for social good is less about finding a single correct policy and more about building ongoing processes of care, accountability, and adaptation. Both speakers underscored that AI systems do not exist in a vacuum. They are embedded in economic structures, social relationships, and power dynamics that shape who benefits and who bears the costs. Meaningful governance therefore requires listening to community voices early, protecting people through thoughtful social policy, and resisting the urge to treat speed and scale as unquestioned virtues. As AI continues to reshape work, communication, and education, the challenge is not simply to manage risk, but to articulate and defend the human values we want technology to serve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a397d-0d74-467e-9f5f-72611a00029d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
