{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8aa4b931-8783-496f-b5a8-9a23482fc16e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Learning Assessment Using LLMs\"\n",
    "description: \"Can LLMs help a learner to quickly understand their current level of knowledge?\" \n",
    "author: \"Haowei\"\n",
    "date: \"9/12/2025\"\n",
    "categories:\n",
    "  - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49413e3f-5451-48ed-b151-5358cd566cef",
   "metadata": {},
   "source": [
    "Many people use LLM tools such as Chatgpt to help them finish school work or complete a task. Since these tasks are usually temporary and aimed to be finished in the short term, people rarely pay attention to the process of learning, and LLMs become controversial in education as being a shortcut of answers rather than learning aid. \n",
    "\n",
    "According to cognitive psychology,  meaningful learning can be defined with retention and transfer, which means learners can memorize or understand the content in depth and apply it to new scenarios. Learning involves many stages, from preview to processing and understanding the knowledge and to review. To understand how LLMs may help with meaningful learning, I want to start with LLM’s ability to assess a learners’ current level of knowledge in a certain field. In classes or an online course, this is often done by the teacher throwing out some pop quiz questions or seeing a show of hands to get a general idea of where students are in a field of study. \n",
    "\n",
    "I wonder if LLMs can generate suitable quiz questions that accurately assess a students’ level. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3c9b6-65a7-44a2-b3e6-a0accc1f9351",
   "metadata": {},
   "source": [
    "<img src=\"generated_image.png\" alt=\"generated_by_Chatgpt\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f82549-e8e9-417e-b74b-0bd85f1647c5",
   "metadata": {},
   "source": [
    "*illustration generated by Chatgpt(gpt5)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04187e97-8c52-4209-9f0d-67c3015e1ada",
   "metadata": {},
   "source": [
    "For a quick start, I asked Chatgpt if it can generate a quiz for me to assess my knowledge of AI. Of course, I can add a lot more context to help it understand where I am, but the point of using Chatgpt for this task is to quickly help a student understand their current level of knowledge, so I used a short prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74531ee-4218-451d-8647-af86cc167ede",
   "metadata": {},
   "source": [
    "I used the following prompt:\n",
    "\n",
    "> I want to understand where I am in my knowledge with AI. Can you generate quiz questions to help me assess my current level?\n",
    "\n",
    "The first two versions it sent back were either too long or contained a lot of short answer questions, which was quite overwhelming for me as a learner, so I changed my prompt to:\n",
    "\n",
    "> I want to understand where I am in my knowledge with AI. Can you generate short multiple choice quiz questions to help me assess my current level?\n",
    "  \n",
    "These are the eight questions it generated:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d53b36-a4db-4852-9ca2-356e092ee344",
   "metadata": {},
   "source": [
    "![questions generated by gpt on assessing my knowledge in AI](8_initial_questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e1f5e4-e25b-48e0-8800-b70b3fb05e5b",
   "metadata": {},
   "source": [
    "The first eight questions were basic and I got all of them right, so Chatgpt offered me a harder quiz. This time the questions were a lot more detailed and advanced, so I only got 5 out of 8 right. \n",
    "\n",
    "I then asked Chatgpt to make conclusions on my current level and offer recommendations for me to improve my knowledge. Here is my prompt: “Help me interpret the quiz result. What do I understand, what tasks can I complete, and how may I improve?”\n",
    "It sent back a list of items I understood well, tasks I can likely complete, and where I struggled. What brought my attention was its confidence in my technical skills with me just correctly answering some conceptual questions. \n",
    "\n",
    "Specifically, it said:\n",
    "*If someone gave you a dataset + scikit-learn or TensorFlow starter code, you could likely:\n",
    "* Train a simple classifier (logistic regression, decision tree).\n",
    "* Interpret probability outputs correctly (once clarified).\n",
    "* Run a CNN or transformer with prebuilt frameworks.\n",
    "\n",
    "I don’t think I can train a classifier or run a transformer with prebuilt frameworks if someone gives me a dataset and startercode. There seems to be some overestimation of my skillsets. This can easily deceive a learner and make them over confident about themselves. I think this inaccuracy may be because of 1) me guessing some of the hard questions right and gave out fake feedbacks to gpt and 2) Chatgpt’s miss on the validity of quizzes and scope of conclusion that can be drawn from quizzes.\n",
    "\n",
    "Next time anyone says “teachers will be replaced by AI”, tell them that it can’t, even from the first step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fd88d-6e48-4cc9-96d8-88326ac7f0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
